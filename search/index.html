<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta content="IE=edge,chrome=1" http-equiv=X-UA-Compatible><meta content="width=device-width,minimum-scale=1" name=viewport><title>Sergio Carracedo | Search</title>
<meta content="yes" name=mobile-web-app-capable><meta content="black-translucent" name=apple-mobile-web-app-status-bar-style><meta content="Sergio Carracedo's personal page and blog" name=description><meta content="Sergio Carracedo" name=author><link href=https://sergiocarracedo.es//index.xml rel=alternate title="Sergio Carracedo" type=application/rss+xml><link href=/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><link href=/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/site.webmanifest rel=manifest><link color=#5bbad5 href=/safari-pinned-tab.svg rel=mask-icon><meta content="#da532c" name=msapplication-TileColor><meta content="#ffffff" name=theme-color><link crossorigin href="https://fonts.googleapis.com/css2?family=Rozha+One:wght@400;500;600;700&display=swap" rel=preconnect><link crossorigin href="https://fonts.googleapis.com/css2?family=Merriweather:wght@400;500;600;700&display=swap" rel=preconnect><link crossorigin href="https://fonts.googleapis.com/css2?family=Allura:wght@400;500;600;700&display=swap" rel=preconnect><meta property="og:title" content="Search"><meta property="og:description" content="Sergio Carracedo's personal page and blog"><meta property="og:type" content="website"><meta property="og:url" content="https://sergiocarracedo.es/search/"><meta name=twitter:card content="summary_large_image"></meta>
<link href=https://sergiocarracedo.es/search/ rel=canonical><link href=https://sergiocarracedo.es/scss/main.min.c6aeadd0f64f9e58a5f36b759b3c5f69f04adcbaf95c581a64fdd28dfd1410e8.css integrity="sha256-xq6t0PZPnlil82t1mzxfafBK3Lr5XFgaZP3Sjf0UEOg=" rel="stylesheet preconnect"><link as=style href=//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css rel="stylesheet preload" type=text/css></head><body><header class="web-header background-paper-secondary"><h2><a class=back-to-home href=/ title="Back to home"><i class="fas fa-home"></i><div class=back-to-home__content><span class=back-to-home__action>Back to home</span><div class=back-to-home__title><img alt="Sergio Carracedo" class=avatar src=/i/sergiocarracedo_hu_d186183080134713.jpg width=100 height=100><h4>Sergio Carracedo</h4></div></div></a></h2><div class=web-header__page-title><span>Search</span></div><div class=theme-switch><i class="theme-switch__theme fas fa-sun" id=theme-switch-light title="Switch to light theme"></i>
<i class="theme-switch__theme fas fa-moon" id=theme-switch-dark title="Switch to dark theme"></i></div></header><div class=main-content><section class="section wrapper-1200"><header class=section__header><h1 class="section__title page-title" id=search-title>Search</h1><form action=https://sergiocarracedo.es/search class="search-form search-form--dark search-form--l" method=get><label for=search-input hidden>Search</label>
<input class=search-form__input id=search-input name=query placeholder=Search type=text>
<button aria-label="Search in the blog" class=search-form__button>
<i class="fas fa-search"></i></button></form></header><ul id=results><li>Search results would be shown here.</li></ul><script>window.store={"https://sergiocarracedo.es/blog/":{title:"Blog",content:"",url:"https://sergiocarracedo.es/blog/",image:"",tags:[],readingTime:"0 minutes read",date:"Feb 2, 2025"},"https://sergiocarracedo.es/":{title:"Sergio Carracedo",content:"",url:"https://sergiocarracedo.es/",image:"",tags:[],readingTime:"0 minutes read",date:"Feb 2, 2025"},"https://sergiocarracedo.es/corepack/":{title:"Forget about installing yarn, pnpm, etc. Using Corepack",content:`When you face to manage a node.js / javascript project you should set a replicable environment, engine (node) version, package manager, etc.
To manage the node version, you can use nvm which manages the node version in your system (global and per project). You can also use a .nvmrc file to specify the node version you need for that project and use a (shell integration)[https://github.com/nvm-sh/nvm?tab=readme-ov-file#deeper-shell-integration] to automatically switch to the correct version when you enter the project directory.
But what about the package manager? In the past (a long time ago), the only package manager was npm so the bigger problem was to ensure all the developers are using the same version of npm to avoid problems. But now we have yarn and pnpm (my favourite) that are faster and have some features that npm doesn&rsquo;t have. That makes the problem bigger, as you need to ensure all the developers use the same package manager, to avoid issues like having different lock files, different ways the package manager handles the dependencies, etc.
Corepack Corepack is an experimental node.js feature added in v16.9.0 and v14.19.0, so is there in any node version you should use in your projects right now (I guess you are using a node version with security support, if not you should!!!!!).
Corepack is manager that allows you to use yarn, pnpm, or npm &ldquo;without&rdquo; installing them in your system, and the most important, as nvm, each project can use its own package manager version.
It also defines the project package manager and its version in the package.json file, so now package manager is part of the project.
How to use it corepack is included in node.js, so you don&rsquo;t need to install it, you just need to enable it:
corepack enable The first time for a project you need to define the package manager you want to use and the version, for example, to use pnpm version 10.0.0 you should run:
corepack use pnpm@10.0.0 If this version of the package manager is not installed in your system, corepack will download it and use it for the project. It also will update the package.json by adding a line to define the package manager and its version for the project:
{ ... &#34;packageManager&#34;: &#34;pnpm@10.0.0+sha512.b8fef5494bd3fe4cbd4edabd0745df2ee5be3e4b0b8b08fa643aa3e4c6702ccc0f00d68fa8a8c9858a735a0032485a44990ed2810526c875e416f001b17df12b&#34;, ... } Now you must run any package manager command using corepack as proxy. For example corepack pnpm i to install the dependencies.
If you try to run another package manager (via corepack) you will get an error:
‚ùØ corepack yarn i UsageError: This project is configured to use pnpm because /works/test/corepack/package.json has a &#34;packageManager&#34; field } If you call a package manager command without corepack, you will not get an error (except for yarn). You can solve that by adding aliases for the package managers to your shell configuration file (.bashrc, .zshrc, etc.):
alias yarn=&#34;corepack yarn&#34; alias yarnpkg=&#34;corepack yarnpkg&#34; alias pnpm=&#34;corepack pnpm&#34; alias pnpx=&#34;corepack pnpx&#34; alias npm=&#34;corepack npm&#34; alias npx=&#34;corepack npx&#34; I encourage you to use corepack if you are not using it yet, it&rsquo;s a great out-of-the-box tool to avoid problems with the package manager and to ensure all the developers use the same version of it.
`,url:"https://sergiocarracedo.es/corepack/",image:"/corepack/sivani-bandaru-bczrpU9n8f4-unsplash_hu_e1a05ce441adb7bc.jpg",tags:["node","corepack","npm","pnpm","yarn","javascript"],readingTime:"3 minutes read",date:"Jan 26, 2025"},"https://sergiocarracedo.es/input-field/":{title:"Input-field concept to improve reusability and consistency in your form-related UI components",content:`Reusability and consistency are two concepts I am &ldquo;obsessed&rdquo; with. Maybe it&rsquo;s because I know the effort and pain of maintaining a UI component library without those concepts in mind. It&rsquo;s also about the developer experience, and the less time you spend on repetitive tasks, the more time you have to focus on the real problems and deliver value to your users. From the user&rsquo;s point of view, consistency is a key factor in the usability of an application.
The problem When you develop a UI components library, it&rsquo;s very common to have different components to allow users to introduce information in different ways: (I&rsquo;m going to use NextUI components as an example, but you can apply this concept to any UI library)
Text fields Text areas Selects or Dropdowns Date pickers File fields Checkboxes Radio buttons &hellip;. Lets focus on the fields that allow the user to introduce text or a value, like text fields, text areas, selects, date pickers, file fields etc.
Do you see the pattern?
All these kinds of fields have labels, help messages, error messages, and slots for icons, and most of the behavior is the same: focus, blur, change, etc. Set the border color in red if error, set the color and the background to gray if disabled, etc.
There are a lot of common things, implement them in each component is a waste of time, and a source of bugs, as you need to maintain the same behavior in different components. When you need to change something, you need to do it in all the components, and it&rsquo;s also a source of inconsistency, as you can forget to update one of the components.
There are some differences between the components, like the way to introduce the value or to render it, for example, a text field uses an input type=&quot;text&quot; to let the user change the value, and a select component doesn&rsquo;t need an input (not in this example, maybe if its autocomplete, but a list of options, a date picker needs a calendar, etc.
Those are the differences that make the components unique, but the common behavior is the same, with some exceptions or predefined behaviors for the common cases. For example, the dropdown uses the right slot to render the arrow will open the popover with the options, the date picker renders a calendar to identify it, etc.
The solution The solution is to create a helper component that encapsulates the common behavior and the common structure of the fields. A helper component makes no sense use as is, you can think of that like an abstract class in OOP, you can&rsquo;t create an instance of it, but you can extend it and create a new class that inherits the behavior and structure of the abstract class.
Each component can set a preset of properties or slots for the helper component, and the helper component will render the structure and the behavior based on that. For example, the Textfield or Textarea component exposes the slot for left and right icons to the developer, but the dropdown uses the right slot for the arrow and only exposes the left slot for the icon. But the helper component is still the same
The helper component can also expose some properties to allow the developer to customize the behavior, for example, to show a &ldquo;clear&rdquo; button when the field has a value.
The takeaway The takeaway is to think of the similarities to create a helper component that encapsulates the common behavior and structure, and let the components extend it and customize it to create the unique behavior and structure. That applies not only to the input fields, in these examples we can do some similar with the label (it can include the info tooltip), the messages, etc. It is very nice to be able to fix a bug or add a new feature to the fields, and do it just in one place.
`,url:"https://sergiocarracedo.es/input-field/",image:"/input-field/PXL_20250105_233533375.MP_hu_724f8ea309d50066.jpg",tags:["ui"],readingTime:"4 minutes read",date:"Jan 6, 2025"},"https://sergiocarracedo.es/2024-in-a-nutshell/":{title:"2024 in a Nutshell",content:`2024 is over and it is time to look back and remember the milestones and achievements of the year.
Professional This was a year of change, I joined New Relic in June as a Senior Software engineer in the UI tooling team. The challenge is big for me as the goals of the team I joined are very different from the ones in my previous roles: we are focused on providing tools to the frontend teams to improve the developer experience and ensure the quality of the frontend applications. All my previous roles were focused on the development (backend and frontend) of the product itself, but this position is not focused on the product as before. This was a big change for me and I faced many challenges, but pleased with the results and the team, a team full of talented people, great management, and something very important people who like to help and who are easy to work with.
Before joining New Relic, I was working in Nextail as a frontend architect. I strongly believe my team and I achieved all the goals the company set for us, completing the migration of the frontend to a modern reactive and progressive frontend framework, creating a powerful and flexible UI components library to homogenize the user experience, accelerating the development and ensure the quality of the frontend applications, and building the foundations for the future.
Professionally speaking, I&rsquo;m pretty sure 2025 will be a year full of challenges, and maybe changes, but I will still work hard to keep improving my professional skills and continue learning.
Post and Talks This was the third year in the ranking of number of posts written. Only surpassed by 2020 and 2019.
I am very proud of the series of articles I wrote about my experience creating a table component, and about UI components library. This list is not complete yet, but I will continue writing about it in 2025.
I continue publishing articles in DZone
Learnings This year I re-learned (because it is mostly the same learnings I had the last year) important things:
how important is the people you work together how motivation can make a difference both professional and personal. how a good manager makes a difference Personal In February our beloved cat Gauss passed away. We miss him a lot. He was part of our family for 17 years and I am very happy to have shared so many years with him. He taught me a lot of things about cat&rsquo;s behavior and personality, and how important is to trust each other. He loved to sleep on my lap while I was working, go to the vet just with one strap over my shoulder with need a pet carrier because my shoulder was his safe place. I miss him a lot he was very special.
With hardly any time to recover from the loss of Gauss, in April, friends who know how much I love cats, told me about a cat that needed a home. A baby cat someone threw in a trash container, even the umbilical cord. They saved his live and fed him for 1 month and adopted him. We called him Weber in honor of Wilhelm Eduard Weber because Weber was a friend of Carl Friedrich Gauss, and you can see in the picture above, I teach him to trust in my and my shoulder as Gauss did. He is a cat with a lot of energy, very playful, and very affectionate.
Music This is the song I listened to the most in 2024:
I really love it
This is the playlist of the my favourite songs of 2024 (the ones I added as favourites in 2024), the there is a couple of &ldquo;winners&rdquo;
&#x1f389; Happy 2025!! &#x1f389;
`,url:"https://sergiocarracedo.es/2024-in-a-nutshell/",image:"/2024-in-a-nutshell/eyestetix-studio-S7ZWCON3xbQ-unsplash_hu_f1002fd8d2960950.jpg",tags:["lifestyle"],readingTime:"3 minutes read",date:"Dec 29, 2024"},"https://sergiocarracedo.es/music-covers/":{title:"My favourite music covers",content:`A cover version is a new performance or recording by someone other than the original artist typically with another production style or genre.
As is Xmas time, this is a cover of &ldquo;Carol of the Bells&rdquo; with metal style:
Long time ago, in the Napster epoch, I discovered the covers, specifically punk covers of pop songs. It was a revelation for me, and I started to search for more and more covers. With the time I discovered that there are a lot of covers, not only punk covers, but also metal, rock, and other genres covers of pop songs, and also covers of other songs.
Before Spotify, Last.fm was my favourite way of listening music (now last.fm is just the scrobbler but time ago, it was a tag-based music player too). I discover more and more covers, and when the Spotify era started, I started to create playlists with my favourite covers.
Here are some of my favourite covers playlists, most of them are punk and metal covers of pop songs:
And this one is one of my latest discoveries, a playlist of LoFi-style covers of &ldquo;famous songs&rdquo; I use a lot to work:
For me covers world is a rabbit hole, you start with a song you like, and you end with a lot of new songs and artists you never heard before. There are even groups that only do covers, Our Last Night, Northern Kings, etc.
I hope you enjoy the playlists as much as I do.
`,url:"https://sergiocarracedo.es/music-covers/",image:"/music-covers/pexels-stasknop-1319799_hu_23d416a0a9ba913b.jpg",tags:["music"],readingTime:"2 minutes read",date:"Dec 24, 2024"},"https://sergiocarracedo.es/math-max-problem-fast-max/":{title:"Math.max (and min) 'Maximum call stack size exceeded' with large datasets and how to reimplement it to make it much faster",content:`Even though Javascript was not created with the management of large datasets in mind, manipulating them nowadays is very common.
Data aggregation is also a common functionality, calculate the max value in an array, is a simple task, the language provides the Math.max method to do it.
But, try to do this:
const data = Array.from({ length: 1_000_000 }, () =&gt; Math.random() * 1000) console.log(Math.max(...data)) The code is simple: it generates and array with 1.000.000 random items and then tries to get the max value of the array and as the Math.max doesn&rsquo;t accept an array as param, we need to spread the array to pass the values as function&rsquo;s arguments.
But if you run that you will have an error like: RangeError: Maximum call stack size exceeded
This error is caused because when you spread an array in the function&rsquo;s arguments (or use apply) you are doing Math.max(data[0], data[1], .... data[999_999]) and the function&rsquo;s arguments use the call stack which is a limited region of memory to be stored. This limit depends on the engine, so your code can work in a browser and fail in another one that uses a different javascript engine.
Check more details in MDN web docs
The solutions As a teacher told us long time a go: if a problem have only one solution is not a problem, is an exercise. We have multiple solutions to avoid this problem, some of them even improving the original performance.
Chunk the array MDN page proposes to use a hybrid strategy, split the array in chunks, and apply the Math.max function to each one.
function maxUsingChunks(arr: number[]): number { let max = -Infinity; const QUANTUM = 32768; for (let i = 0; i &lt; arr.length; i += QUANTUM) { const subMax = Math.max.apply( null, arr.slice(i, Math.max(i + QUANTUM, arr.length)), ); max = Math.max(subMax, max); } return max; } Using reduce We can loop through the array and compare each value to know if it is higher than the previous max value.
function maxUsingReduce(arr: number[]): number { return arr.reduce((acc, cur) =&gt; Math.max(acc, cur), -Infinity) } Using for Yes, a simple for to loop through the array
function maxUsingFor(arr: number[]): number { const length = arr.length let max = -Infinity for (let i = 0; i &lt; length; i++) { max = Math.max(max, arr[i]) } return max } Performance There are more solutions, but most of them are slight variations of the ones I showed above. Those are all valid and do exactly the same, but which is the best performance?
To measure the performance I&rsquo;m going to use Vitest benchmarking feature, which is perfect for this usage.
We create a .bench.ts file that will include the functions to compare.
// max.bench.ts import { bench } from &#39;vitest&#39; const data = Array.from({ length: 100_000 }, () =&gt; Math.random() * 1000) bench(&#39;Native Math.max&#39;, () =&gt; { Math.max(...data) }) bench(&#39;maxUsingChunks&#39;, () =&gt; { maxUsingChunks(data) }) bench(&#39;maxUsingReduce&#39;, () =&gt; { maxUsingReduce(data) }) bench(&#39;maxUsingFor&#39;, () =&gt; { maxUsingFor(data) }) Then we run the benchmark with vitest bench. Here is the output of the command
‚úì max.bench.ts (4) 2441ms name hz min max mean p75 p99 p995 p999 rme samples ¬∑ Native Math.max 799.83 0.7349 9.2443 1.2503 0.8774 6.3467 7.8521 9.2443 ¬±9.70% 400 ¬∑ maxUsingChunks 291.20 2.3094 12.5102 3.4341 3.8463 12.0433 12.5102 12.5102 ¬±8.72% 146 slowest ¬∑ maxUsingReduce 1,033.83 0.8555 1.3126 0.9673 0.9713 1.2611 1.3066 1.3126 ¬±0.69% 518 ¬∑ maxUsingFor 11,198.84 0.0872 0.1550 0.0893 0.0887 0.1062 0.1166 0.1391 ¬±0.13% 5600 fastest BENCH Summary maxUsingFor - max.bench.ts 10.83x faster than maxUsingReduce 14.00x faster than Native Math.max 38.46x faster than maxUsingChunks ü§Ø The results could seem counterintuitive, a simple for is much faster than any other option, and even the reduce&rsquo;s version is much faster than the native implementation.
Ron Northcutt wrote a nice article that explains in detail why forEach (and another array loop functions like reduce) are slower than a simple for, probably the most relevant reason is the function Overhead (forEach() invokes a callback function for each element in the array, and that is expensive for the engine).
Note: If we try to calculate the max value in a bigger array native Math.max will cause the error Maximum call stack size exceeded, so the native version of Math.max is worse in every aspect
Going further We can try to optimize even more the function with less generic strategies. For example, if we know the theoretical max we can rewrite the max function to stop looping if we reach it and return that value. A real-life use-case that fits this is: you have an array of negative temperatures, and you want to get the max temperature, we can set the theoretical max as 0 (as we are talking about negative temperatures), and if the function finds an item with the value 0, we are sure we find the max and we can return that value without continue looping through the array.
Replacing Math.max If you want to solve the Maximum call stack size exceeded problem, and/or make your apps faster you can just use the maxWithFor function code (changing the function name) and use it in your code. I strongly DON&rsquo;T recommend you to monkey-patch Math.max. It can cause unexpected side effects, and errors, makes the code hard to maintain, etc&hellip;
You can also use a third-party library like fast-max that covers all the aspects we mentioned:
Solves the Maximum call stack size exceeded problem Is fast Implements strategies like the theoretical max Can ignore values Now a question for you: Did you face the call stack size problem in your code? Let me know in the comments.
`,url:"https://sergiocarracedo.es/math-max-problem-fast-max/",image:"/math-max-problem-fast-max/pexels-pixabay-2159_hu_32ee62923c916c0a.jpg",tags:["javascript","performance"],readingTime:"5 minutes read",date:"Dec 17, 2024"},"https://sergiocarracedo.es/creating-a-table-component-iv/":{title:"Even more features. Lessons I Learned Creating a Table Component (4/4)",content:` This post is part of a post series: First part, Second part, Third part) and an extra related post: Writing a query builder to filter data
As I mentioned in the previous chapters of this series, I prefer to provide the maximum number of features in the table. There are several reasons behind this decision, those are the more relevant ones:
Homogeneity: Same solution to the same problem in different places. If you don&rsquo;t provide a solution for the common or repeated needs, you are taking the risk of having different solutions for the same problem. That is very common in large applications where the teams work on different parts of the application, and they can provide different solutions and different user experiences for similar use cases, this causes duplication of work and a bad user experience. Maintainability: In general it&rsquo;s easy to maintain the code, if you solve a bug it will be solved in all the places the table&rsquo;s feature is used, or if a feature changes it will change in all the places, this is very related to the previous point. For example if we decide to use a recycle view to render the table rows for a better performance, we only need to do it once, and all the implementation will benefit from that. Implementation simplicity: Using a table with a lot of use cases and features (mostly based on configurations or props) makes it very simple to implement the table even for any developer, even if is a new-joiner or junior. The trade-offs of this are:
Covering all use cases is not easy: Finding a common solution to similar problems requires effort and time, and it can be hard in some cases. Find a balance: It&rsquo;s not always easy to decide which features should be in the table or not Big component: Including too many features can make the bundle size for the component big. Other features I implemented Table settings persistence As I mentioned in the chapter we allow users to select order of the columns, that is a nice feature, but is not very useful if it needs to set the order every time it loads a page. To solve that we implemented a system to persist and restore the table settings. In our use case, table settings include: column order, column visibility, items per page, and other settings the table implementation can define for the specific use case.
I think the table component shouldn&rsquo;t know anything about how to persist the data, because in that case, we are coupling the table component with the backend, and that limits the reusability of the table. This is an example of the balance I mentioned before.
To solve that and avoid coupling the component to the infra (backend) we use providers, the table component defines an interface for the persistence (read and write), and the page which implements the table can inject that provider via prop to implement the persistence logic. To make the code reusable the provider is a common utility that only needs an ID to identify the table. This provider can store the table settings in the local storage, a backend, etc.
The table should contain the logic to handle the inconsistencies between the settings and the table definition, for example, if a column changes and now is fixed, it should ignore the order in settings for that column, or if we remove or add a new column, the table component contains the logic to set the new column in a default order with the others.
Nested rows Another useful feature in our use case was the nested rows. For example, we have a list of &ldquo;Friends&rdquo; series episodes, and we want to show them grouped by season and even display some aggregated information like count, total views, etc.
There are other sub-features you can include: The groups, each row with children, can be collapsible or not, the header can be a sticky element (is always on top when the children are visible), etc.
The very beginning though is that this is a simple feature, but there are a lot of tricky parts and nontrivial decisions to be made:
How to group the rows when the order changes?. There are multiple ways to do it, and no correct answer, just what makes sense for your use case. You can sort first the parent rows and then sort children inside each row. This will keep the groups You can sort all children together, and then regroup them. This will create repeats in the parent rows as now the children are mixed. Ex: A contains: 3,4,7 and B contains 1,2,5, The ordered rows with the children to display are: B (1,2), A (3,4), B(5), A(7) How to split pages: Should parent row count for pagination or only children or both? How to split pages when the parent rows are collapsible? You can recalculate the pagination when a group of children are expanded or not, or just count items even if they are not expanded. In our use case, we select one of the solutions in each topic, but will not share it with you ;). There are no absolute correct answers here, it depends on what you need.
Async loading of nested rows Another feature related to the nested rows is the async loading. This feature only makes sense when the user can collapse the parent rows (and they are collapsed by default), in this case, the children only are loaded when the user expands a row. This is useful to make the time to show the initial data in the table short and reduces requests to the backend.
This feature requires solving how to load the data, as I mentioned before I think requests should not be part of the table component, so like in the case of the settings persistence, we define a provider interface that will load the data and return it to the table. The table will invoke the provider when the user wants to expand a row and receives the parent row, your implementation will manage that to request the children rows and return it to the table. The table must handle the loading state and, in our case show skeletons for the children rows meanwhile the data is loading.
Expandable rows This feature allows expanding a row letting the implementation define the content to display in the expanded space. This feature can seem similar to nested rows but is not (and is not incompatible). In this case, the content is not part of the table, in the case of nested rows, the children must have the same columns as the parent (as the table), in the expended row, we can to another table (with different columns) or any other content, like a form, a chart, etc, anything.
In our case, as the table component was implemented in Vue, we use a feature I love about this framework, the scoped slots to allow us to define that content, it can be the same for all the expanded rows or by row
Row actions A very common use case (at least for us) is to have actions over the rows, for example, delete the entity the row represents, edit it, execute an action over it, etc. For that, we allow to pass to the table component an action provider that defines the actions for each row (as it is a function the actions can be different or be disabled or not for each row). Those actions are displayed in the table in an internal column) using a dropdown menu component (this is a component is part of our UI component library)
Toolbar The toolbar is a set of icons, visually over the table header, that represent actions for the table.
I implemented build-in actions like:
Settings: To open the table settings panel or modal Fullscreen: To display the table in fullscreen mode Reload: Dispatch an event to let the implementation the user wants to refresh the data Edit: To enter in edit mode, this converts the content of the rows in input fields to change the values directly from the table Print: Prints the table content Download: Exports the table contents to CSV or XLS implementing all the logic on the frontend side. This is an interesting feature. The build-in feature, only exports the data the table contains, if it is in external mode it only can export the current page content, but we can provide a way to overwrite the button behavior and delegate that in an external implementation can get all the data and download it. In any case, this scenario is a good example of how useful are the formatters, as again, we can render the export data in the correct format. Search box: Where is where the user can enter the search text I explained in the first chapter I allow developers to define which build actions want to use, but also allow them to implement their custom actions for a specific use case passing the buttons to render as props and getting an event or callback when the user clicks on that.
Action bar This bar only appears when the user selects more than one row and allows it to execute actions over the selected rows, those actions are defined by the user, for example, delete or edit, or any other action.
Summary Creating a table component is a challenging and interesting experience, that requires (as any other component) paying attention to the details, understanding, identifying, and defining the concepts involved in the process.
My goal with this post series was to give you tips and share the solutions and decisions I made (and why I made them).
My solutions don&rsquo;t are necessarily the correct ones for you, they were the ones that cover my needs, but I expect you to find these posts useful to understand better the different options.
Still have a lot of small things to bear in mind (empty state, items per page selector, no results found state,&hellip;.) when you implement a table component, but I feel you can handle them.
Please let me know if you liked this post, if you miss something, etc. Thanks for reading
`,url:"https://sergiocarracedo.es/creating-a-table-component-iv/",image:"/creating-a-table-component-iv/cover_hu_b0104ca05ece2ce8.jpg",tags:["table","ui"],readingTime:"9 minutes read",date:"Nov 27, 2024"},"https://sergiocarracedo.es/creating-a-table-component-iii/":{title:"Column related features: Lessons I Learned Creating a Table Component (3/4)",content:` This post is part of a post series: First part, Second part, Fourth part and an extra related post: Writing a query builder to filter data
I want to use this last post in the series to discuss col-related features, such as col sorting, hidden cols, excluded cols, fixed cols, etc.
User-defined cols vs internal cols It&rsquo;s important to know that one thing is the cols defined by the user, those are data cols and are defined via the cols definition property, and another thing is the cols in general, which include the user cols, but also cols are defined by other props or features of the table (internal cols), for example the col that renders the checkboxes in the case of a selectable table, the col, that shows an arrow to expand the col, the col that renders the action buttons, etc.
In the next points, I will refer to user-defined cols except if I mention the opposite.
Col sorting This is about the order of the columns in the table, it is very convenient (and common) to render the columns in the same order as they are defined in the definition array, but in some use cases, it can be useful to let the user to reorder the columns to decide which ones are more relevant. I used a property in the col definition object, for example, order.
I always try to avoid defining all the definitions using a default behavior, in this case, the developer can decide to don&rsquo;t set the order in some columns, the criteria to get the final order I decided to apply is to assign a big number to the cols without an explicit order and then order all the columns by the order field value. If more than one col has the same order value, the first in the array has priority and is before the orders on the table.
Talking about the UI there are multiple ways to let the user decide the order, for example dragging the cols, with a col&rsquo;s menu action, or dragging and dropping the cols&rsquo; names in a config section of the table. In any case, we must let the developer block some columns in case it will be necessary, for that we can add a new property to the col&rsquo;s definition: noSortable, which is false by default.
One important thing about that is, makes no sense to let &ldquo;block&rdquo; columns in the middle of the table as you could not drag the blocked col but you can move the cols around it, and that is basically the same as allowing to sort it.
I decided to only block the columns at the ends of the table, and only the ones between them can be sorted don&rsquo;t let move the columns before or after the blocked ones on the left and right respectively, this is also related to Fixed cols.
Regarding the non-user-defined cols, those are not part of the order logic, those cols must be in the position the table requires by design. To achieve that, the table stores internally 2 lists of cols, one the user-defined cols, and all the cols (that included both &ldquo;internal&rdquo; cols and user-defined cols with extra metadata)
Hidden cols One feature we decided to implement was the hidden cols. I want to allow the user to select the cols she/he wants to see in the table to adapt the table to her/his use case.
But as before, we should have a way to control which cols can be hidden by the user, and for that, we must add a property to the col definition (noHiddeable) to set the column as &ldquo;mandatory visible&rdquo;.
To hide a column the user can click in a dropdown menu on the col header, but we must provide a way to list all the columns (including the hidden ones) and make them visible or hide any column, you can do that in a modal, side panel, etc. As an idea, but not a unique solution, I used the same col names&rsquo; list that allows to order of the cols&rsquo; position to show or hide a column.
I recommend you treat hidden cols as visible col for the ordering to be consistent when the user unhides it.
Excluded cols In some cases we need to hide a col from the render, for example, we need the value to define an initial rows&rsquo; sort, but we don&rsquo;t want to show the column. You can think that is the same as a hidden col, but is not. A hidden col is a col we show in the cols list in other words is a col the user can know about, but an excluded col is for internal use. To set a col as excluded we use again a col&rsquo;s definition property (excluded) and you must remember to filter this column in anything related to the visualization: render, col order, etc.
Fixed cols When the table contains a lot of cols, it can be useful to fix some of them, in this case, I decided to not set this in the cols definition, I think is something related to the table itself, so I added a property in the table component to set the number of fixed cols.
The fixed cols must be in the &ldquo;edges&rdquo; of the table, yes, plural, as you can have fixed cols on the left side and the right side, and not just one, you can have multiple fixed cols on each side.
Regarding the property, I think is very convenient to accept a number, that represents the number of fixed cols on the left side, but also accept an object or tuple to define the number of fixed cols on each side. ex.:
&lt;Table fixedCols={1}/&gt; &lt;Table fixedCols={[1,3]}/&gt; To define which cols are the fixed ones, the table first needs to order the cols and then fix the ones that are at the beginning and/or end of the list, always based on the property.
For the internal cols the fixed property should not affect them, but we need to bear in mind that those internal cols can be fixed, for example, the col with the check to select rows is an internal col, that in our implementation is fixed independently of the table prop.
Also, it is important to force some flags or properties in the fixed cols to overwrite the user config, for example, a fixed col is not sortable.
In the next chapter A table component is (or can be) a complex component, and I prefer to provide the maximum number of features in the table itself, making it unnecessary to think about the implementation outside of the table, and making it simple to fix bugs and maintain the table. If you like this approach (vs simple table that only renders data in a tabular way) there are a lot of things to think about, I will talk about Table settings persistence, nested rows, row actions, toolbar, action bar, empty state, async loading, expandable rows in the next post.
`,url:"https://sergiocarracedo.es/creating-a-table-component-iii/",image:"/creating-a-table-component-iii/pexels-pixabay-221392_hu_52fdd0f70a4414c6.jpg",tags:["table","ui"],readingTime:"6 minutes read",date:"Nov 12, 2024"},"https://sergiocarracedo.es/query-builder-filter-data/":{title:"Query builder to filter data in TypeScript",content:`I wrote posts (1, 2, 3 and , 4) about my learnings creating table components, and this post can be the third part as I made a query builder for that component, but it can be used in other uses cases.
A query builder provides a convenient and (usually) simple interface for creating and executing queries (filtering) on a data set.
In TypeScript (or any other language) if you want to filter an array of objects like:
const data = [ { field1: 1, field2: &#34;a&#34;, field3 ....}, { field1: 2, field2: &#34;b&#34;, field3 ....} ... ] We must write a code like this, with the logic to do the filtering:
const filteredData = data.filter(row =&gt; { return row.field1 &gt; 10 &amp;&amp; row.field2 === &#34;b&#34; || .... }) The goal is to define a dynamic way to define the filters without changing the code.
Characterizing a filter A simple filter, is basically, a value source (the attribute name), a value to compare with and the comparator operator.
Using Typescript we can model that like:
NOTE: I&rsquo;m not going to do complex typing to simplify the code, but in production is necessary to type the field to ensure is a key of the data and the value type
type FilterComparationOperator = &#39;eq&#39; | &#39;neq&#39;| &#39;gt&#39; | &#39;gte&#39; | &#39;lt&#39; | &#39;lte&#39; | &#39;contains&#39; | &#39;notContains&#39; | &#39;startsWith&#39; | &#39;endsWith&#39; interface Filter { field: PropertyKey value: unknown operator: FilterComparationOperator caseSensitive?: boolean } First, we model in the type FilterComparationOperator the different ways to compare the field value and the filter value:
equal (eq): non-equal (neq) great than (gt) great than or equal (gte) less than (lt) less than or equal (lte) contains the value (contains) non-contains the value (notContains) starts with the value (startsWith) endsWith (endsWith) You can think more operators if you need it Then we model the filter, with the field we will use to the the value in the data, the value for the filter, the comparator operator, and a couple of flags to refine the filter behavior, if it should be case-sensitive (case-insensitive by default)
With that, we will write a function to compare 2 values using the operator and the filter constraints like if the comparative should be case-sensitive or non-case-sensitive:
function compare&lt;T = unknown&gt;( value: T, operator: FilterComparationOperator, filterValue: T, caseSensitive = false ): boolean { function isString(value: unknown): value is string { return typeof value === &#34;string&#34;; } // If the filter value is a string and the filter is not caseSensitive // converts the values to lowercase for a case insensitive comparison const filterValueComp = isString(filterValue) &amp;&amp; !caseSensitive ? filterValue.toLocaleLowerCase() : filterValue; const valueComp = typeof value === &#34;string&#34; &amp;&amp; !caseSensitive ? String(value).toLocaleLowerCase() : value; // Checks if we can do numeric comparations with the values function isComparable(value: unknown): value is string | number { return typeof value === &#34;string&#34; || typeof value === &#34;number&#34;; } type OrString&lt;TValue&gt; = TValue | string; const comparationFuncs: Record&lt; FilterComparationOperator, (value: OrString&lt;T&gt;, filterValue: OrString&lt;T&gt;) =&gt; boolean &gt; = { // Note the == instead of ===. We want to allow to compare 1 and &#34;1&#34; as the same value eq: (value, filterValue) =&gt; value == filterValue, neq: (value, filterValue) =&gt; value != filterValue, contains: (value, filterValue) =&gt; String(value).includes(String(filterValue)), notContains: (value, filterValue) =&gt; !String(value).includes(String(filterValue)), startsWith: (value, filterValue) =&gt; String(value).startsWith(String(filterValue)), endsWith: (value, filterValue) =&gt; String(value).endsWith(String(filterValue)), gt: (value, filterValue) =&gt; isComparable(value) &amp;&amp; isComparable(filterValue) ? value &gt; filterValue : false, gte: (value, filterValue) =&gt; isComparable(value) &amp;&amp; isComparable(filterValue) ? value &gt;= filterValue : false, lt: (value, filterValue) =&gt; isComparable(value) &amp;&amp; isComparable(filterValue) ? value &lt; filterValue : false, lte: (value, filterValue) =&gt; isComparable(value) &amp;&amp; isComparable(filterValue) ? value &lt;= filterValue : false, }; return comparationFuncs[operator] ? comparationFuncs[operator](valueComp, filterValueComp) : false; } console.log(compare(12, &#34;gt&#34;, 8)); //True console.log(compare(&#34;lorem ipsum dolor est&#34;, &#34;contains&#34;, &#34;DOloR&#34;)); //True console.log(compare(&#34;lorem ipsum dolor est&#34;, &#34;contains&#34;, &#34;DOloR&#34;, true)); //False Having multiple filters and complex relationships between them We want to let the user create complex comparatives like if the (temperature is lower than 20 and temperature is higher than 0) or the temperature is -999 or ((the city name is Vigo and country is Spain) or (city name is Santiago and the country is chile))
Note the parenthesis that groups the logic as is not the same A and B or C (same as (A and B) or C) than A and (B or C)\`. Read more about the order of operations
So we need to define a struct to model these groups and the relationship.
interface GroupFilter { filters?: Filter[] groups?: GroupFilter[] operator: &#39;and&#39; | &#39;or&#39; } This &ldquo;filter group&rdquo; can contain filters and other groups (that can contain more filters and more groups,&hellip;.) and the operator to define the relationships between the filters and groups. For example:
const group: GroupFilter = { operator: &#34;or&#34;, filters: [{ field: &#34;temperature&#34;, value: 20, operator: &#34;lt&#34; }, { field: &#34;temperature&#34;, value: 0, operator: &#34;gte&#34; }], }; defines a filter group that checks if the temperature is lower than 20 OR higher (or equal) than 0.
With this simple nested struct, we can create complex filters like the one in the example:
// if the (temperature is lower than 20 and temperature is higher than 0) or the temperature is -999 or ((the city name is Vigo and country is Spain) or (city name is Santiago and country is Chile)) const filters: GroupFilter = { operator: &#39;or&#39;, filters: [{ field: &#39;temperature&#39;, operator: &#39;eq&#39;, value: -999 }], groups: [ { operator: &#39;and&#39;, filters: [ { field: &#39;temperature&#39;, operator: &#39;lt&#39;, value: 20 }, { field: &#39;temperature&#39;, operator: &#39;gt&#39;, value: 0 }, ] }, { operator: &#39;or&#39;, groups: [ { operator: &#39;and&#39;, filters: [ { field: &#39;city&#39;, operator: &#39;eq&#39;, value: &#39;Vigo&#39; }, { field: &#39;country&#39;, operator: &#39;eq&#39;, value: &#39;Spain&#39; }, ], }, { operator: &#39;and&#39;, filters: [ { field: &#39;city&#39;, operator: &#39;eq&#39;, value: &#39;Santiago&#39; }, { field: &#39;country&#39;, operator: &#39;eq&#39;, value: &#39;Chile&#39; }, ], } ] }, ] } The last piece we need is a function to filter the data rows:
function filter&lt;T extends Record&lt;PropertyKey, any&gt;&gt;( data: T[], filterGroup: GroupFilter ): T[] { // If the no rows, we don&#39;t need to continue if (data.length === 0) { return data; } // Check if a row fulfill the constraints const filterRow = (row: T, filterGroup: GroupFilter | undefined): boolean =&gt; { // Exit condition for this recursive function if (!filterGroup) { return true; } // Checks if a col in the row matches a filter const filterColFunc = (filter: Filter): boolean =&gt; { // If the row has no a field with the field name continue if (!row[filter.field]) { return true; } return compare( row[filter.field], filter.operator, filter.value, filter.caseSensitive ); }; const groups = filterGroup.groups || []; const filters = filterGroup.filters || []; if (filterGroup.operator === &#34;and&#34;) { // For the and operator, we should return true if all of the filters and groups are fulfilled. If no filters or no groups we consider it a match return ( (filters.length === 0 || filters.every(filterColFunc)) &amp;&amp; (groups.length === 0 || // For the groups we apply again the filterRow function to get the result for the group groups.every((group: GroupFilter) =&gt; filterRow(row, group))) ); } else { // For or operator, we should return true if any of the filters or groups fulfill or they are empty return ( filters.some(filterColFunc) || groups.some((group: GroupFilter) =&gt; filterRow(row, group)) || (filters.length === 0 &amp;&amp; groups.length === 0) ); } }; // Loops the rows and filter them return [...data].filter((row) =&gt; filterRow(row, filterGroup)); } Now we can execute our filter just by doing:
const filteredData = filter(data, filters) Putting all together Run it in TS playground
Last thoughts The code I show you here is just a base, there are improvements it needs to be production-ready, for example the typings, the comparison between numbers and strings (now 1 !== &ldquo;1&rdquo; and maybe you want to consider that is the same for filtering porpoises), the performance, etc.
But my goal is to show you how simple is to create a dynamic filter that allows the user to define how to filter the data, for example, the user can generate the filter groups using a query builder UI or just a form to select a couple of value, but the developer can define the operators depending on the field without the need to write it in the code.
Read the code carefully and check how powerful are the recursive functions, they can seem complicated in the beginning, but when you start to understand them, they make the code simpler and more flexible.
`,url:"https://sergiocarracedo.es/query-builder-filter-data/",image:"/query-builder-filter-data/pexels-igor-starkov-233202-1117452_hu_c8fa54626f759a94.jpg",tags:["typescript","ui","filtering"],readingTime:"7 minutes read",date:"Nov 4, 2024"},"https://sergiocarracedo.es/creating-a-table-component-ii/":{title:"Table pagination, sorting, filtering and row selection. Lessons I learned creating a table component (2/4)",content:` This post is part of a post series: First part), Third part, Fourth part, and an extra related post: Writing a query builder to filter data
In the first chapter I talked about the data and cols definition. How to manipulate the data, transform it, and format it.
This second chapter will focus on the pagination, sorting, and filtering features and the row selection.
Those features can be or cannot be the responsibility of the table, maybe you can delegate them in your code, and in some true way, but I like to provide a homogeneous and complete developer experience minimizing the repetitive tasks and putting them all together where they are needed.
Internal pagination (or filtering or sorting). Let&rsquo;s start with the simplest case: the table gets, via props, all the rows.
In this case, the table can do the filtering, sorting, and pagination. The order of the words is very relevant, as it is the order we should follow: first filter the rows, then sort them, and finally, paginate them.
We can not paginate first as filtering will remove items from the page, and we need to sort the full list to return valid results.
Sorting Typically, the table will render a kind of button/arrow in the header to allow the user to select the column to sort by and the direction of the sorting. But not all the columns must be sortable, that is something you can define in the col definition structure. (You can consider if the attribute is not present, the column is sortable by default to avoid being very verbose)
const cols = [ { id: &#39;identifier&#39;, value: &#39;id&#39;, sortable: false }, { id: &#39;fullName&#39;, value: (row) =&gt; \`\${row.firstName} \${row.lastName}\`, format: (value) =&gt; value.toUpperCase() }, ... ] Adding that to the col&rsquo;s definition, we are letting the developer use the table to define the column&rsquo;s behavior, making it flexible.
We need a way to communicate to the developer the sort and sort direction the user selected outside the component, we can use custom events.
Internal filtering You can implement the filtering inside the table, the basic filtering is a search box that finds elements that partially fit the user&rsquo;s texts in any row of any column, but as we did before we can define a col&rsquo;s definition attribute to disable the search.
What about complex filtering Search is nice, but probably you will need more complex filtering, for example, values of a col temperature higher 0 and lower 10, or even filter using multiple criteria in the same or different columns and different operators. You can achieve that using a filtering definition system (I will talk about this in the next chapter)
Pagination This is the last step, we split the data into chunks of certain elements (page size) and render only the one that corresponds with the selected page.
The table should provide a pagination component to let the user select the page navigate between pages, and maybe select the page size (items per page). All those values should emitted as events outside the table to let the developer know them.
As you know the total rows you can also calculate the number of pages
Some tips about the pagination Define with your team which is the first page: 0 (like the arrays) or 1 (more natural). This is very important to avoid misleading and errors. You should reset the active page to the first one when the data or filters change, but not with sorting When the page size (items per page) changes you can reset the active page or calculate the new page will show the same first item When the table provides these features internally the code will be something like this (pseudocode)
const filteredRows = props.rows.filter(item =&gt; /*[filter by criteria]*/) const sortedRows = filteredRows.sort((a,b) =&gt; /*[filter by criteria]*/) const totalPages = Math.ceil(sortedRows.length / itemsPerPage) const rowsToRender = sortedRows.slice((page - 1) * itemsPerPage, page * itemsPerPage) Remember the array operation we are doing returns shallow copies.
Internal pagination (or filtering or sorting). When the pagination, sorting, or filtering occurs outside the table, for example in the backend. In this case, we can NOT provide the features in the table, if the table haven&rsquo;t all the rows the sort will be incorrect, and the same with filtering.
In this case, we should provide an &rsquo;external&rsquo; mode property to let the table the data to render is the one provided in the property it doesn&rsquo;t need to do anything else than render it.
We still need the pagination component, the sorting buttons, and the filtering ui, but those only must emit the changes in the values outside the table and the code in charge of retrieving the data from the backend will use them to do the correct request and get the correct page, filtering, and sorting.
This solution of internal and external modes, lets me create a component that covers multiple use cases, making developers&rsquo; lives easier, Creating a table with pagination, sorting, and filtering is easier as do:
// mypage.tsx export default () =&gt; { const data = [ {id: 1, firstName: &#39;Sergio&#39;, lastName:&#39;Carracedo&#39;, country: &#39;Spain&#39;}, {id: 2, firstName: &#39;Manolito&#39;, lastName:&#39;Gafotas&#39;, country: &#39;Andorra&#39;}, ... ] const cols = [ { id: &#39;identifier&#39;, value: &#39;id&#39;, sortable: false }, { id: &#39;fullName&#39;, value: (row) =&gt; \`\${row.firstName} \${row.lastName}\`, format: (value) =&gt; value.toUpperCase() }, ... ] return (&lt;MyTable rows={data} cols={cols} /&gt;) } For an external mode we only need to get the changes in the page, filter (search), and sort:
// mypage.tsx export default () =&gt; { const [page, setPage] = useState(1) const [search, setSearch] = useState(&#39;&#39;) const [sort, setSort] = useState({ col: &#39;id&#39;, desc: false}) const [data, setData] = useState([]) const cols = [ { id: &#39;identifier&#39;, value: &#39;id&#39;, sortable: false }, { id: &#39;fullName&#39;, value: (row) =&gt; \`\${row.firstName} \${row.lastName}\`, format: (value) =&gt; value.toUpperCase() }, ... ] useEffect(() =&gt; { // get data from API setData(....) }, [page, search, order]) return (&lt;MyTable rows={data} cols={cols} mode=&#34;external&#34; onPageChange={(e) =&gt; setPage(e.value)} onSearchChange={(e) =&gt; setSearch(e.value)} onSortChange={(e) =&gt; setSort(e.value)}/&gt;) } Row selection You could want to allow users to select a row or multiple rows (this behavior is configurable via prop), for example clicking in a checkbox in the first col, and/or clicking in a checkbox in the header that will select all the rows when the selection is multiple.
Row value First, you need to provide a way to assign a unique value to each row to let the table know which one is selected, even when the data is not in memory (external pagination).
If the user selects a row, changes the page, and later goes back to the page with the selected item, the table must show the item selected.
I used the same strategy as for the col value provider, but in this case is a table property. The row value provider can be:
a string that identifies the col by id and uses the col&rsquo;s row value to provide the row id a function that returns a value The row value must be unique for each row, even for the ones that are not in memory, for example, using a value based on the row index, can generate issues when the pagination occurs in the backend, If you are not careful about how you calculate the index, you can have values repeated in different pages.
The &lsquo;Select all checkbox&rsquo; This one is tricky, for an &lsquo;internal&rsquo; table mode, when you select it, the table must check all the rows (even those that are not rendered). Typically you will get the selected row values in an array.
But for the &rsquo;external&rsquo; table mode, that is not enough as you don&rsquo;t know all the row values available, as they are in the backend. In this case, you must continue checking all the rows, but also send outside the table a value allSelected = true, which lets the developer get it from the table and send it to the backend to let it know you want to apply an operation over all items, the backend must know how to do that.
In the case the user deselects one row, the global check must acquire an intermediate state, and let the outside of the table know the non-selected rows.
Summarizing I strongly believe that a table component must provide those functionalities and the developer can decide which ones or how some behaviors should be via configuration props, as you can see above using the table component is very straightforward, you pass the data, the columns definitions and some props to configure the table features and behaviors and you will get a table with the expected behavior. One advantage of this approach is it works as a black box, and any change or improvement will be applied in any table use after updating the component, for example, if you add a recycle view to improve the performance, all the table usage will be benefited of it without any change.
`,url:"https://sergiocarracedo.es/creating-a-table-component-ii/",image:"/creating-a-table-component-ii/pexels-pixabay-159510_hu_2c99051ed4bfce3a.jpg",tags:["table","ui"],readingTime:"7 minutes read",date:"Oct 29, 2024"},"https://sergiocarracedo.es/creating-a-table-component-i/":{title:"Lessons I learned creating a table component (1/4)",content:` This post is part of a post series: Second part), Third part, Fourth part, and an extra related post: Writing a query builder to filter data
I love creating UI components because they are the fundamental bricks supporting a UI application. When the components are good enough, both User and Developer experience increases, making applications consistent, easy to implement, and easy to scale.
If the UI application shows data in lists, sooner or later, you will need a table. Tables are UI components that can become very complex depending on their features. The goal of this blog post is not to tell you how to program features but to help you understand some typical needs and features tables should implement to give you useful ideas for your custom UI table component&rsquo;s implementation.
I created a custom UI table component for 2 different projects, and I learn a lot from those experiences.
What is a table This a fundamental question, but it is also important because we will base the component&rsquo;s implementation on that.
A table is a set of normalized data (the rows) organized in columns (cols). The rows contain cells, that represent the cross between a row element and a column.
&ldquo;Normalized&rdquo; is important as in a table the rows share the same structure, that structure is defined by the cols. In other words, if our table cols are: id, name, and surname, the rows (data) should provide those values (or at least most of them)
Table component&rsquo;s architecture I think a component should work as a black box, that gets inputs and returns the DOM to render the components so the data and structure of data should be passed to the table component as properties and table must know how to render it based on that (and in extra config props we will discuss later), and the developer should not know the component internals, just how pass the data and config.
This approach has a lot of advantages, for example, consistency, if you implement a change in the table component, you can do it without the need to refactor all the usages, as the input is the same. For us, that means that a table component should render the rows not delegate it to a child component in the table usage, like in the example bellow.
// ‚ùå‚ùå‚ùå Don¬¥t do that &lt;MyTable rows={rows}&gt; {rows.map(row =&gt; &lt;MyTableRow row={row} /&gt;)} &lt;/MyTable&gt; Believe me, I saw that before, and this is a direct attack to the component responsibility.
// ‚úÖ‚úÖ‚úÖ Do that &lt;MyTable rows={rows} cols={cols} config={config} /&gt; The data The first table&rsquo;s input (property) to model is the data. You will get that from an API or generate it on your application (I will tell you more about data pagination, sorting, search, etc. later)
The data input is typically an array, as it represents a list of rows, for example:
const data = [ [1, &#39;Sergio&#39;, &#39;Carracedo&#39;, &#39;Spain&#39;], [2, &#39;Manolito&#39;, &#39;Gafotas&#39;, &#39;Andorra&#39;], ... ] But it also can be an array of objects:
const data = [ {id: 1, firstName: &#39;Sergio&#39;, lastName:&#39;Carracedo&#39;, country: &#39;Spain&#39;}, {id: 2, firstName: &#39;Manolito&#39;, lastName:&#39;Gafotas&#39;, country: &#39;Andorra&#39;}, ... ] Even with nested values (we should bear in mind this cases if we want to create a flexible component):
const data = [ {id: 1, names: { firstName: &#39;Sergio&#39;, lastName:&#39;Carracedo&#39; }, country: &#39;Spain&#39;}, {id: 2, names: { firstName: &#39;Manolito&#39;, lastName:&#39;Gafotas&#39; } , country: &#39;Andorra&#39;}, ... ] There are more cases, but the point I want to highlight is that the data can came in different ways, and the way to get the cell&rsquo;s value can be different
The cols&rsquo; definition The cols are not only the title that we will show in the table header, the cols define a lot related to the data, and there are a lot of behaviors we can model with the cols&rsquo; definition. Let me give you the ones I think are more relevant and interesting in a table for each col.
id We need a way to refer us to a column, so we need an ID for it, that can be just a number or a string.
title This will be the value we will render in the header
value This will tell our component from where it should get the data to render the col&rsquo;s cell for each row. This field in interesting. Let me tell you more about it, there are more that you can think.
The simplest provider will depend on the data (rows), for the first case I mentioned above (Array of arrays), the provider can be just the index of the row, for example, 1 should get 'Carracedo' and 'Gafotas', and those are the values in the index 1 of each row.
If the data is like the second example (Array of objects), the provider can be the key name to the value, for example, firstName, to get the same values. But to cover the third example (Array of objects with nested values), we can allow passing an string with dot notation, for example: names.firstName
But there are more providers you can implement, you can allow to pass a function as a value provider, this function gets the row, and the row index as arguments and must return the value for the cell.
It&rsquo;s important to notice that there is no strong correlation between the cols in the table and the cols in the data. The table can show a col that is a derivative value from the original data, for example, the fullName, or the age (if birthday is a field in the data source). You can use that value provider function to create derived values on-fly. You could do it before to pass the data to the table, but forces you to implement a loop to create a new structure with the derivative values. I guess the function value provider is simpler and elegant. Let&rsquo;s see it in an example:
const cols = [ { id: &#39;identifier&#39;, value: &#39;id&#39; }, { id: &#39;fullName&#39;, value: (row) =&gt; \`\${row.firstName} \${row.lastName}\` }, ... ] In the example for the col identifier the table will get the value from the property id and render it in each row (that is a direct value), but for the col fullName the table will call the function in each row to render the cell, and will use that derivative value.
You can use it for a lot of cases:
Create the year sales average col from the value in the other 12 cols that represents the sales in each month If the name contains some letter, you can render the treatment etc I have not mentioned examples on purpose like converting the name to uppercase, because that is not the goal of this field. Remember it&rsquo;s just a data provider, how the data is formatted should be done in another way.
formatter Sometimes the data you will render in a table needs to be transformed (formatted) before rendering it, there are a lot of common cases:
Date-time: Your data provides the datetime as an integer and you want to show it as a string, for example, &lsquo;YYYY-MM-DD&rsquo; Numbers: You want to show them with a specific number of decimal places, thousands separators, or abbreviations Not available values: Show N/A if the value is undefined &hellip; As I mentioned, the value must be just a definition of how to get the data, the pure data, if you need to format it before showing it this field is your answer.
Why we should have 2 fields in the col definition if we could do it in the value field using the function provider you mention?
True, you can do the transform and/or the format with the value provider function, but in this case, you are losing useful information, for example, to sort the cols or to find values.
Imagine your table data came from an API has no pagination, or sorting, the table can do the sorting and pagination (I will write about sorting, searching, pagination in a next post).
You have a date-time col as unix epoc (integer value) and you want to display it in &lsquo;MM-DD-YYYY&rsquo; format, if you do the transformation in the value provider the table will sort the col using the formatted values and it will order then for example: [&lsquo;12-01-2024&rsquo;, &lsquo;12-01-2010&rsquo;, &lsquo;11-11-2025&rsquo;] as [&lsquo;11-11-2025&rsquo;, &lsquo;12-01-2010&rsquo;, &lsquo;12-01-2024&rsquo;], that is not the expected behavior of sorting.
We must sort the values before formatting them to allow us to sort or/and search them in the table
render The last thing to bear in mind about how the data is displayed in the table is the render. Not always we want to show the data as a text, maybe we need to render an image, a chip, a progress bar, etc&hellip;
How to do that will depend on the UI framework you are using (assuming you are using one):
In Vue you can use named slots to change the way a cell content is rendered In React you can pass a component in the render property of the col In any case, this &ldquo;render function&rdquo; will get the value after formatting it and doing the render.
This is the flow of the data in the table component:
Defining this architecture is vital for other functionalities the table can implement like row sorting, pagination, col sorting, col visibility, col resize, fixed cols, search, etc.
In the next post, we will understand the different ways the data can be sorted, and paginated (spoiler: external or internal) and the implications it has in other features.
If you like this post, please leave a comment to let me know your interest.
Lessons I learned creating a table component (part II)
`,url:"https://sergiocarracedo.es/creating-a-table-component-i/",image:"/creating-a-table-component-i/pexels-pixabay-210607_hu_aa29bd02632e015.jpg",tags:["table","ui"],readingTime:"8 minutes read",date:"Oct 19, 2024"},"https://sergiocarracedo.es/branded-types/":{title:"Branded types in TypeScript",content:`When you model entities with typescript, it is very common to get an interface like
interface User { id: number username: string ... } interface Order { id: number userId: number title: string year: number month: number day: number amount: { currency: &#39;EUR&#39; | &#39;USD&#39;, value: number } ... } The problem The properties&rsquo; types have no semantic meaning. In terms of types User.id, Order.id, Order.year, etc. are the same: a number
Following the previous example we can have a set of functions that do actions over the entities, for example:
function getOrdersFiltered(userId: number, year: number, month: number, day: number, amount: number) { // ...} function deleteOrder(id: number) { // ... } Those functions will accept any number in any arg no matter the semantic meaning of the number, for example:
const id = getUserId() deleteOrder(id) Obviously, that is a big mistake, and it could seem easy to avoid reading the code, but the code is not always as simple as the example.
The same happens with getOrdersFiltered, we can swap the values of day and month, and we will not get any warning or error, the errors will happen in occur (or not if the day is lower than 12), but is obvious that the result will not be the expected.
The solution Object calisthenics&rsquo; rules provide a solution for that: Wrap all primitives and Strings (Related Primitive obsession anti-pattern)
The rule is to wrap the primitives in an object that represents a semantic meaning (DDD describes that as ValueObjects)
But with TypeScript we don&rsquo;t need to use classes or objects for that, we can use the type system to ensure a number that represents something different from a year, can&rsquo;t be used instead a year.
Branded types This pattern uses the extensibility of types to add a property that ensures the semantic meaning:
type Year = number &amp; { __brand: &#39;year&#39; } That simple line creates a new type that can work as number but is not a number it&rsquo;s a year.
const year = 2012 as Year function age(year: Year): number { //... } age(2012) // ‚ùå IDE will show an error as 2012 is not a Year age(year) // ‚úÖ Generalizing the solution To avoid writting a type per branded type we can create a utility type like:
declare const __brand: unique symbol export type Branded&lt;T, B&gt; = T &amp; { [__brand]: B } That uses a unique symbol as the brand property name to avoid conflicts with your properties and gets the original type and the brand as generic params
With this, we can refactor our models and functions like:
type UserId = Branded&lt;number, &#39;UserId&#39;&gt; type OrderId = Branded&lt;number, &#39;OrderId&#39;&gt; type Year = Branded&lt;number, &#39;Year&#39;&gt; type Month = Branded&lt;number, &#39;Month&#39;&gt; type Day = Branded&lt;number, &#39;Day&#39;&gt; type Amount = Branded&lt;{ currency: &#39;EUR&#39; | &#39;USD&#39;, value: number}, &#39;Amount&#39;&gt; interface User { id: UserId username: string ... } interface Order { id: OrderId userId: UserId title: string year: Year month: Month day: Day amount: Amount ... } function getOrdersFiltered(userId: UserId, year: Year, month: Month, day: Day, amount: Amount) { // ...} function deleteOrder(id: OrderId) { // ... } Now, in this example the IDE will show an error as id is an UserId and deleteOrder expects an OrderId
const id = getUserId() deleteOrder(id) // ‚ùå IDE will show an error as id is UserID and deleteOrder expects OrderId &ldquo;Trade off&rdquo; As a small trade-off you will need to use X as Brand, for example const year = 2012 as Year when you create a new value from a primitive, but this is the equivalent to a new Year(2012) if you use value objects. You can provide a function that works as a kind of &ldquo;constructor&rdquo;
function year(year: number): Year { return year as Year } Validation Branded types are also useful to ensure the data is valid as you can have specific types for validated data and you can trust the user was validated just by using types
type User = { id: UserId, email: Email} type ValidUser = Readonly&lt;Brand&lt;User, &#39;ValidUser&#39;&gt;&gt; function validateUser(user: User): ValidUser { // Checks if user is in the database if (!/* logic to check the user is in database */) { throw new InvalidUser() } return user as ValidUser } // We can not pass just a User, needs to be a ValidUser function doSomethingWithAValidUser(user: ValidUser) { } Readonly is not mandatory, but to be sure your code will no change the data after validating it, it&rsquo;s very recommendable
Recap Branded types are a simple solution that:
Improves the code readability: Makes clearer which value should be used in each argument. Reliability: Helps to avoid mistakes in the code that can be difficult to detect, now the IDE (and the type checking) help us to detect if the value is in the correct place Data validation: You can use branded types to ensure the data is valid You can think of Branded types like a kind of version of ValueObjects but without using classes, just types and functions.
Enjoy the power of typings
`,url:"https://sergiocarracedo.es/branded-types/",image:"/branded-types/pexels-googledeepmind-17485819_hu_8fe5354fa250c65f.jpg",tags:["typescript"],readingTime:"4 minutes read",date:"Oct 7, 2024"},"https://sergiocarracedo.es/no-more-js-semicolons/":{title:"Are you using semicolons in JS/TS? Maybe is time to remove them",content:`I have been using standardjs (and standardts) for 5 years as lint ruleset in my projects, and I am very happy with that. I tried before Google config lint and eslint-config-airbnb and one of the main differences I found between those rulesets is relative to semicolons; Google and Airbnb require the use of semicolons at the of the lines, but standardjs don&rsquo;t.
I worked a long time ago with PHP that requires them, but I adapted very fast to not use them in JavaScript and Go. Now I&rsquo;m working on a project with a ruleset that includes the semicolons rule, and I found it a little bit uncomfortable in some situations
I guess this is a discussion like tabs vs spaces but I want to share my thoughts and give you references to opinions in the community about it.
Background Before entering subjective (opinions) things let&rsquo;s talk about if JavaScript need semicolons and about the ASI (Automatic semicolon insertion)
ASI JS (ECMAScript) allows us to not terminate statements and declarations with a semicolons, as the interpreter will &ldquo;insert&rdquo; a semicolon automatically, for example &ldquo;when the offending token that is not allowed by any production of the grammar is separated from the previous token by at least one line (LineTerminator)&rdquo;,
Simplifying, that means that if the next token has no meaning for the previous one, the interpreter adds an automatic semicolon. For example
myFunction() // an automatic semicolon will be inserted here as otherFunction() has no meaning for myFunction() otherFunction() myFunction() // No semicolon will be inserted here as \`.value\` is valid in for the language grammar (accessing to the myFunction() result property .value myFunction() // No semicolon will be inserted here as \`[\` is valid in for the language grammar [1,2,3].forEach(...) The last example is interesting as the interpreter doesn&rsquo;t add the automatic semicolon, but will throw a syntax error as the token , is not correct there (we will swe how to solve it later)
(You can check the full list at https://tc39.es/ecma262/#sec-automatic-semicolon-insertion)
Note that the TC39 specification says &ldquo;Most ECMAScript statements and declarations must be terminated with a semicolon. Such semicolons may always appear explicitly in the source text. For convenience, however, such semicolons may be omitted from the source text in certain situations. These situations are described by saying that semicolons are automatically inserted into the source code token stream in those situations.&rdquo;
Community opinions As with tabs vs spaces topic, there are multiple points of view and opinions in the community let&rsquo;s review them
In favor of not using In the standardjs rules&rsquo; list page they provide multiple links about the motivation to don&rsquo;t use them. They also provide an extra rule to avoid the issues with the tokens like [, ( and \`
I recommend you to read the Open letter to Javascript leaders regarding semicolons by Isaac Z. Schlueter.
His voice is very relevant in the community as he is the creator of NPM, and in the letter (written 14 years ago), he explains:
&ldquo;Yes, it‚Äôs quite safe, and perfectly valid JS that every browser understands. Closure compiler, yuicompressor, packer, and jsmin all can properly minify it. There is no performance impact anywhere.&rdquo; &ndash; Isaac Z. Schlueter
Against of not using The most important voice is the TC39 specification, which indicates &ldquo;statements and declarations must be terminated with a semicolon.&rdquo;, we can still fulfill this requirement without writing semicolons, please continue reading my thoughts to see how Some cases like when the next line starts with [, ( and \` need it (but it can be solved with a lint rule like the one standardjs provides) or when you try to use return in 2 lines can be not obvious you are doing it wrong without semicolons. https://medium.com/free-code-camp/codebyte-why-are-explicit-semicolons-important-in-javascript-49550bea0b82 The use of semicolons is a widely-accepted practice in the JavaScript community and you will find a lot of code following this My thoughts Probably this is the less important part of this post, but I want to share it with you in any case
Semicolons represents a visual overload I believe semicolons add noise to the code and are not really needed (except in a couple of cases), for example, when you use an anonymous array you need to explicitly say the interpreter the [ doesn&rsquo;t try to access the previous line result:
someFunction() ;[1, 2].forEach(x =&gt; console.log(x)) Manual editions When you are editing a multiline sentence and need to add a new line you need to manually remove the semicolon. Check this example:
[&#39;z&#39;, &#39;d&#39;, &#39;C&#39;, 1, 9, &#39;a&#39;] .filter((char) =&gt; !isNaN(char)); Now you want to map the result to uppercase, adding an extra line, intuitively you add a new line after the last one.
[&#39;z&#39;, &#39;d&#39;, &#39;C&#39;, 1, 9, &#39;a&#39;] .filter(char =&gt; !isNaN(char)); .map(char =&gt; char.toLocaleUpperCase()) This will fail as the semicolon in the second line breaks the code, you must remove it and add it in the last line.
This is something prettier or the lint --fix can not handle like in a coma-separated list and you need to take care of it.
Semicolons are mandatory As the TC39 specification mentions &ldquo;statements and declarations must be terminated with a semicolon.&rdquo;, but probably you are not writing code that will be passed to the interpreter directly, probably you are using Typescript and or a bundler, or a minifier. All of those will add the implicit semicolons for you.
Check yourself, open the Typescript playground and you can check the example code don&rsquo;t include the semicolons, but the transpiled one does.
Conclusion I think there are good arguments in both positions, and I guess the most conventional and extended position is to use semicolons in JS, but in TS seems very extended to don&rsquo;t use them. I think both positions are valid, but maybe you can spend a few minutes to think about that and what do you prefer.
I feel very comfortable not using them, but in any case I have not big issues working with or without semicolons, it&rsquo;s not a blocking or an annoying thing.
I don¬¥t want to create a flame, flames are unuseful, just to know the different opinions.
What do you use and/or prefer? Please let me know in the comments
`,url:"https://sergiocarracedo.es/no-more-js-semicolons/",image:"/no-more-js-semicolons/pexels-karolina-grabowska-4021799_hu_fe3ee1a194441fb8.jpg",tags:["javascript","typescript","opinion"],readingTime:"5 minutes read",date:"Aug 25, 2024"},"https://sergiocarracedo.es/circular-dependencies/":{title:"Detecting circular dependencies in Javascript projects",content:`Circular imports (or circular dependencies, or cycle dependencies, but not the same as circular references) are easy to have in your code base, and more when the code grows. It can impact in the bundle generation or cause issues (for example in HMR) and you should avoid them because they are a symptom of an incorrect architecture or code organization and is a big code smell.
What are circular dependencies Circular dependencies occur when a package (A) depends on another package (B) and B also depends on A. Circular dependencies can be more complex: A depends on B, B on C, C on D and D on A
Example In a Javascript/Typescript project you have several files to organize the code and some of them depend on the const, types, functions, classes, etc. in other files, which is good as it allows us to reuse code and make it more understandable.
Let&rsquo;s see a simple example with 3 files:
// log.ts import { verbose } from &#39;./config&#39; export function log (...messages: string[]) { if (verbose) { console.log(...messages) } } // config.ts import { log } from &#39;./log&#39; export const verbose = true // Try to get config.json file export let globalConfig = {} export async function getConfig() { try { globalConfig = (await fetch(&#39;http://myserver.com/config.json&#39;)).json() } catch (e) { log(e) } } // main.ts import { getConfig, globalConfig } from &#39;./config&#39; getConfig() With this code main.ts imports config.ts and it imports log.ts and finally it imports again config.ts creating the circular dependencies
Another typical circular dependency cause is when you have a folder (for example a component) with multiple files (ex. the component, the types, subcomponents, etc) and you have an index file that re-export the component and the types, and your component instead of import the types from the types file do it from the index.ts file.
Circular dependency issues In this situation, you will not get any error, and probably nothing special happens and the code will work.
Bundlers like Vite, Webpack, etc, Typescript&rsquo;s transpiler, and other tools can handle circular dependencies to avoid infinite recursion generating the dependency tree.
It doesn&rsquo;t mean everything is solved. There are a lot of issues related to circular dependencies:
Unexpected side effects: The program can fail randomly depending on which was the first file that imports the package Webpack and Vite HMR issues: For example, before Vite 5 any change that involves circular dependencies triggered a full page reload, now it is improved but still having issues Memory leaks: Example Build: It can be slower as the bundler tools need to handle them Code is hard to understand Is a code smell / anti-pattern: Having circular dependencies means your code and architecture is not well-defined Circular dependencies can happen in other languages, it&rsquo;s not a Javascript/Typescript problem, but for example Golang or Cargo (Rust) don&rsquo;t allow cycle dependencies
Finding circular dependencies In the example above detecting a circular dependency is easy as there is just a few files, but in a real project it can be hard as the dependency chain can be big:
A -&gt; B -&gt; C -&gt; D -&gt; E -&gt; F -&gt; A We need a tool to detect them, and fortunately, there are a lot of tools to detect the circular dependencies
dpdm - https://github.com/acrazing/dpdm This is my favorite one. This tool only needs an entrypoint or the files to scan, and it will start to analyze the code in order to find circular dependencies.
To check the example above we can run: dpdm ./src/main.ts
The output will be:
It detects the circular dependency and gives information about the dependency tree. main.ts imports config.ts, config.ts imports log.ts and (this is not written, but it is implicit) log.ts imports again main.ts
dpdm is very fast even with large code bases and supports CommonJS and ESM, and Javascript and Typescript
We can run the tool in a pre-commit hook and/or in the CI to ensure the code doesn&rsquo;t include circular dependencies.
Madge - https://github.com/pahen/madge Madge is a tool for generating a visual graph of the module dependencies, and can also find circular dependencies.
Running it with the default flag will return the dependency tree, and if we add the --image flag: madge src/main.ts --image deptree.svg we will get a SVG file that represents the dependency tree
Using madge src/main.ts --circular we get the circular dependencies in a similar output to the previous tool
Bundler plugins There are plugins available for bundlers like Webpack and Vite to detect the circular dependencies on build time:
https://www.npmjs.com/package/vite-plugin-circular-dependency https://www.npmjs.com/package/circular-dependency-plugin Eslint The eslint-plugin-import includes the rule import/no-cycle to ban the cycle imports
Solving circular dependencies After detecting a circular dependency, there are multiple strategies to solve it, This is something I will write about in a future post, but for the example, it is as easy as moving the verbose constant to it own file (for example config-log.ts or just to the log file.
Did you have issues in the past with circular dependencies? Let me know in the comments
`,url:"https://sergiocarracedo.es/circular-dependencies/",image:"/circular-dependencies/pexels-miriam-alonso-7585854_hu_e453b349777e89ab.jpg",tags:["javascript","typescript","clean code"],readingTime:"4 minutes read",date:"Aug 19, 2024"},"https://sergiocarracedo.es/axios-interceptors-validate-headers/":{title:"Axios interceptors to validate allowed headers",content:`Today I want to share a small tip working with CORS and Axios.
It&rsquo;s very typical in the backend to only allow certain headers, even more so when it defines the CORS-related Access-Control-Request-Headers header used in response to a preflight request. In those cases, if the frontend make a request with an unexpected header the request will be rejected.
This kind of issue mostly happens in the production environments because the browser ignores CORS headers when it fetches data from localhost. That is a problem as nobody will probably detect an issue sending unexpected headers before deploying the code to production.
Axios interceptors Axios is a widely used library to manage HTTP requests that have an interesting feature: the interceptors are like middlewares where you can execute code before the request (request interceptors) and/or after getting the response (response interceptors)
We will focus on the request interceptors, which are executed before the request to the server is made, this is the perfect place to put our code:
import type { InternalAxiosRequestConfig } from &#39;axios&#39; const allowedCorsHeaders = [&#39;Authorization&#39;,&#39;Origin&#39;,&#39;X-Requested-With&#39;, &#39;Content-Type&#39;,&#39;Accept&#39;].map((header) =&gt; header.toLowerCase()) function validateHeadersInterceptor(config: InternalAxiosRequestConfig): InternalAxiosRequestConfig =&gt; { // Get the headers we are sending in the requests const headers = Object.keys(config.headers).map((header) =&gt; header.toLowerCase()) // Get the headers to send are not in the allowed list const invalidHeaders = headers.filter((header) =&gt; !allowedCorsHeaders.includes(header)) if (invalidHeaders.length &gt; 0) { const msg = \`Header(s): \${invalidHeaders.join(&#39;, &#39;)} are not allowed.\` throw new Error(msg) } return config } The validateHeadersInterceptor function gets the config (the current request config) and checks if there are some unexpected headers, if there are thrown an error let the developer know she/he is using an unexpected header.
Using the interceptor is easier as do the:
axios.interceptors.request.use(validateHeadersInterceptor) The interceptor&rsquo;s order You should know that when you have multiple interceptors, the execution order is the inverse of the &ldquo;use&rdquo; order
For example:
axios.interceptors.request.use(interceptor1) axios.interceptors.request.use(interceptor2) axios.interceptors.request.use(validateHeadersInterceptor) axios.interceptors.request.use(interceptor3) the execution order will be interceptor3, validateHeadersInterceptor, interceptor2 and interceptor1. In our validateHeadersInterceptor case we want to check even if another interceptor adds an invalid header. In the example if interceptor1 or interceptor2 add a header to the request we will not notice it as the validator was executed before.
We should ensure our interceptor is &ldquo;used&rdquo; first, before any other interceptor.
`,url:"https://sergiocarracedo.es/axios-interceptors-validate-headers/",image:"/axios-interceptors-validate-headers/nathan-dumlao-eksqjXTLpak-unsplash_hu_5fef1b42956b36f8.jpg",tags:["axios","request","validation"],readingTime:"2 minutes read",date:"Jul 1, 2024"},"https://sergiocarracedo.es/monsgeek-m5-custom-keybord-review/":{title:"Monsgeek M5 custom keyboard review",content:`5 years ago I bought my first mechanical keyboard, a cheap and simply one, the Krom Kernel, a 100% size keyboard with Outemu Red switches with spanish layout. During this time I change the space bar keycap twice because the plastic was not very good and the part that connects with the switch was not very reliable and it broke causing a bad experience pressing it. But was something I could fix, replacing all the keycaps with a better quality ones.
After 5 years, some switches start to fail randonly, so I thought it&rsquo;s time to think in a new keyboard.
My requiriments, wish list:
High quality 100% or TLK Silent (at least more than the previous keyboard) switches Hot-Swappable switches (to change the switched if I want to change the switch type or replace a broken one) Wired: I don&rsquo;t want to thing in charge battery or having connection issues (is very frustating when a key is not send to the computed randomly) The layout I want to have the possibility of changing the keycaps in the future without replacing the keyboard, but for the Spanish layout the options are limited, so the first drama was to move to an US ANSI layout.
I was not sure I could get used to the new layout, but with the F√©lix G√≥mez&rsquo;s advice I configured my previous keyboard with the US international (with dead keys) layout and I tried, and trained, to get used to it, I was not easy because the muscular memory after 30 years using a spanish layout is strong, but with pacience I could get used to it.
US international layout with dead keys This layout is the most used one in the US, and it&rsquo;s very similar to the US international layout, but with the addition of the dead keys that allows you to write accents and special characters like the √ë in a natural way.
For example to write √° you need to press the ' (next to the enter), then a, like in an Spanish layout keyboard. To write the √± you need to press the ~ key and then the n key.
Source: https://en.wikipedia.org/wiki/British_and_American_keyboards#/media/File:KB_US-International.svg
This has a negative counterpart: to write the characters in the dead keys you need to use the space, for example to write ' you must type the ' and then the space. This is not direct as just press a key but you can get used to it.
One of the advantages of this layout is &ldquo;international&rdquo; so you can write a lot of characters specific of another languages easily.
The size I used 100% keyboards all my life, and when I work in a laptop I miss the numeric keypad a lot, also the position of the PageUp, PageDown, Home, and End keys are not very comfortable in the laptop keyboards I used. But I was open to explore TLK keyboards, this discards the 75%, 65%, compact ones, etc.
The choosen one I saw a lot of different keyboards, I really spend a lot of time on that. But casually I saw a youtube video reviewing a Monsgeek keyboard, and after check its specs and the different size&rsquo;s variants I decided to buy the M5, a 100% keyboard DIY kit.
This is not a complete keyboard, it is a kit to build your own keyboard. I never build a custom keyboard, but I was a challege I want to do.
The kit includes a robust alumium case. In my case (M5, the biggest size), its height is arround 2Kg, it&rsquo;s really heavy, but I like it because the keyboard can not slide on the table when you hit it with your hands.
Source: Monsgeek
The kit includes a gasked mount RGB backlit and different foam and plastic layers to improve the keyboard&rsquo;s feel. and reduce the switches&rsquo; echoes in the case to get a more pure sound. It uses a QMK firmware with VIA support that removes the requirement of a separate software to configure the keyboard, but you can use the VIA configurator (a web page) to configure it.
The switches Source: Gatheron
Following the F√©lix&rsquo;s advice I deciced to use the Gateron G Pro 3.0 Prelubed switches. Those are linear switches with a nice feeling, low sound level, and oriented to office and gaming.
The feeling is very good much better than the Outemu Red switches I used in the previous keyboard.
The keycaps My first idea was to find a keycap set similar to Amstrad CPC464, but I didn&rsquo;t find anything similar (only custom keycaps set but theit price is too expensive).
I was looking several keycaps set, but finally I chose this one https://www.aliexpress.com/item/1005005975658627.html
I like the colors&rsquo; combination, the typography and the shape (QXA profile), very modern and minimalist. I&rsquo;m very happy with the result.
Summary I&rsquo;m very happy with the result, I really enjoyed the proccess of mounting the key, the switched, stabilizers, the keycaps. It was easier than I thougth.
The keyboard is very solid, the case is very robust, the switches are amazing I love the sound (the sound is similiar to the one in this video) and the touch, and the set created with the keycaps is beautiful.
It was more expensive than I expect, but I think it&rsquo;s a good value. I hope this keyboard will be a good companion for me for long time, maybe I can change the keycaps when I get bored of the current ones. Or I can try another types of switches, maybe I can found ones feel even more comfortable.
`,url:"https://sergiocarracedo.es/monsgeek-m5-custom-keybord-review/",image:"/monsgeek-m5-custom-keybord-review/cover_hu_ab8d1046378038a7.jpg",tags:["keyboard","hardware"],readingTime:"5 minutes read",date:"Jun 23, 2024"},"https://sergiocarracedo.es/a-real-world-on-a-meeting-light-sign/":{title:'A real-world "on a meeting" light sign',content:`Me and my wife work remotely and we do it in the same room, but there is a problem: the meetings, and obviously, working remotely we have, let&rsquo;s say, more than we would like, and is not always easy to know when the other is in a meeting.
I decided to find a solution to make it visible when someone is in a meeting simply, only with a sight.
The solution: Put a light sign on the working room&rsquo;s wall and turn it on when I am in a meet and off when I am not, simple. :)
The light sign I found a cheap neon-like sign on an online store and I order it. This light sign is powered via USB and controlled manually with a switch button, so that I could control it automatically. The neon-like is just a shaped LED strip, so I ordered a wifi led controler (and a power supply), to replace the light sign&rsquo;s controller, and now we can control the light via wifi using Tuya&rsquo;s app.
We solved the hardware side, now we must solve how to detect when I am in a meet
Detecting when I am in a meet I guess there are a lot of ways to detect when you are in meeting, but I decided to use the webcam, then the solution works with any meet application: Zoom, Google meet, discord, slack, etc
Doing a simple search I found that on Linux (if the kernel uses modules, which is common) you can execute:
$ lsmod | grep uvcvideo And get something like:
uvcvideo 139264 0 videobuf2_vmalloc 20480 1 uvcvideo uvc 12288 1 uvcvideo The uvcvideo module is the one we are interested on (the first line), in that line, the first number means the module size in bytes (we can ignore it) and the second means the number of instances of the module in use, in our case: 0 means webcam not in use 1 implies webcam in use.
Then we can create an script or a program to check this value and turn on or off the wifi controller, but before seeing the code:
Using Home Assistant to control the light I am a big fan of Home Assistant, as is a very flexible way of managing home automation using devices of multiple brands. This is the reason why I decided to use it, instead of fighting with the wifi controller maker&rsquo;s API (if available), I will delegate that to Home Assistant, as you can find integrations for a lot of makers standardizing the way we control the light
The next thing I did was to create a virtual switch in Home Assistant, this will allow me to store the webcam&rsquo;s usage status in Home Assistant and then trigger a scene to turn on or off the light.
Go to Settings &gt; Devices &amp; services &gt; Helpers tab &gt; Click on Create helper &gt; Select toogle
Doing that instead of changing the light status directly, allows us to be more flexible and create better automation, for example to also turn on another light or pause the vacuum cleaner when you are in a meetings to reduce ambient noise.
After that we need to create an access token for the Home assistant&rsquo;s API
Click over your username &gt; Security tab &gt; Click on Create token
Those are the automation I create:
alias: Turn On meeting light description: &#34;&#34; trigger: - platform: state entity_id: - input_boolean.on_a_meet to: &#34;on&#34; for: hours: 0 minutes: 0 seconds: 6 # I added a minimum time to avoid false positives condition: [ ] action: - type: turn_on device_id: #the light device id entity_id: #the light entity id domain: light brightness_pct: 11 mode: single alias: Turn Off meet light description: &#34;&#34; trigger: - platform: state entity_id: - input_boolean.on_a_meet to: &#34;off&#34; condition: [ ] action: - type: turn_off device_id: #the light device id entity_id: #the light entity id domain: light mode: single With that, we can create a program to check the webcam&rsquo;s status and send it to Home Assistant
The observer I created a program written in Go to check the uvcvideo module status and send the changes to Home Assistant
package main import ( &#34;bufio&#34; &#34;bytes&#34; &#34;context&#34; &#34;encoding/json&#34; &#34;errors&#34; &#34;fmt&#34; &#34;log&#34; &#34;log/syslog&#34; &#34;net/http&#34; &#34;os&#34; &#34;strconv&#34; &#34;strings&#34; &#34;time&#34; ) var prevState *bool // nil = unknown, true = active, false = inactive func getEnv(key, fallback string) string { if value, ok := os.LookupEnv(key); ok { return value } return fallback } func logError(e error) { if e != nil { fmt.Println(e.Error()) log.Default().Println(e.Error()) } } func updateHassStatus(ctx context.Context, status bool) (err error) { host := getEnv(&#34;ON_A_MEET_HASS_SERVER&#34;, &#34;http://192.168.0.104:8123&#34;) token := os.Getenv(&#34;ON_A_MEET_HASS_TOKEN&#34;) entityID := getEnv(&#34;ON_A_MEET_ENTITY_ID&#34;, &#34;input_boolean.on_a_meet&#34;) posturl := fmt.Sprintf(&#34;%s/%s/%s&#34;, host, &#34;api/states&#34;, entityID) state := &#34;on&#34; if !status { state = &#34;off&#34; } type Body struct { State string \`json:&#34;state&#34;\` } body := Body{State: state} bodyBytes, err := json.Marshal(body) if err != nil { return } r, err := http.NewRequest(&#34;POST&#34;, posturl, bytes.NewBuffer(bodyBytes)) r.Header.Add(&#34;Content-Type&#34;, &#34;application/json&#34;) r.Header.Add(&#34;Authorization&#34;, fmt.Sprintf(&#34;Bearer %s&#34;, token)) client := &amp;http.Client{} res, err := client.Do(r) if err != nil { return } defer res.Body.Close() return } type ModuleMeta struct { Name string Size int64 UsedBy []string InUse bool } func getModuleMeta(name string) (meta ModuleMeta, err error) { file, err := os.Open(&#34;/proc/modules&#34;) if err != nil { return } defer file.Close() scanner := bufio.NewScanner(file) for scanner.Scan() { s := strings.Split(scanner.Text(), &#34; &#34;) if s[0] == name { size, err := strconv.ParseInt(s[1], 10, 64) if err != nil { return ModuleMeta{}, err } return ModuleMeta{ Name: s[0], Size: size, UsedBy: strings.Split(s[3], &#34;,&#34;), InUse: s[2] != &#34;0&#34;, }, nil } } err = errors.New(&#34;module not found&#34;) return } func loopCheckState(ctx context.Context) (err error) { meta, err := getModuleMeta(&#34;uvcvideo&#34;) if err != nil { return } if prevState == nil || *prevState != meta.InUse { log.Default().Printf(&#34;Module %s status changed to: %t\\n&#34;, meta.Name, meta.InUse) prevState = &amp;meta.InUse err = updateHassStatus(ctx, meta.InUse) } return } func main() { logWriter, err := syslog.New(syslog.LOG_SYSLOG, &#34;on-a-meet&#34;) if err != nil { log.Fatalln(&#34;Unable to set logfile:&#34;, err.Error()) } log.SetOutput(logWriter) ctx := context.TODO() log.Default().Printf(&#34;Starting on-a-meet script&#34;) for { err = loopCheckState(ctx) if err != nil { logError(err) } time.Sleep(time.Duration(1 * time.Second)) } } A very simple program, only mentions that instead of using lsusb I read /proc/modules to get the same data
Running the program as a user service To run the program when the computer starts up, the best way is to convert it into a service.
You only need to create a file, for example on-a-meet.service with the following content
[Unit] Description=&#34;On A Meet Service&#34; [Service] Type=simple ExecStart= Path/to/your/compiled/script Restart=on-failure StandardOutput=file:%h/log_file [Install] WantedBy=default.target Then
copy it /etc/systemd/user (as root user)
Run systemctl --user edit on-a-meet.service and add the following (with the correct values for your case)
[Service] Environment=&#34;ON_A_MEET_HASS_SERVER=&#34; Environment=&#34;ON_A_MEET_ENTITY_ID=&#34; Environment=&#34;ON_A_MEET_HASS_TOKEN=&#34; To set the environment variables the service will need
Run systemctl --user daemon-reload (as your user)
Run systemctl --user start on-a-meet.service
Run systemctl --global enable on-a-meet.service (as root)
And that&rsquo;s all!!! After that, you will get a visual notification when your camera is active
See it in action The next steps I would like to turn on the light sign when at least one computer in the room is using the webcam, I think just adding the script to the other computers and tuning up a bit the Home Assistant&rsquo;s scenes would be easy Add OSX support, unfortunately, I will use a Mac because the work, then I will need to find out how to check when the camera is active on OSX Understand why the module says is in use for a second when the camera is not in use, causing false positives Comments, ideas, or feedback is welcomed!!
`,url:"https://sergiocarracedo.es/a-real-world-on-a-meeting-light-sign/",image:"/a-real-world-on-a-meeting-light-sign/IMG_20240518_172824_hu_a6aa21277c6218f3.jpg",tags:["hardware","work-setup","golang"],readingTime:"6 minutes read",date:"Jun 16, 2024"},"https://sergiocarracedo.es/hexagonal-architecture-frontend/":{title:"A real case: why hexagonal architecture, decoupling, and Dependency injection can be very useful in the frontend",content:`Hexagonal architecture is a software design pattern based on the separation of responsibilities. The goal is to decouple the business logic (domain) and the application from other external interfaces.
Simplifying in hexagonal architecture we communicate the core of the app (domain + application) with the external elements using ports and adapters. A port lives in the core, it is the interface any external code must use to interact with the core (or the core with the external code), the adapter is the external piece of code that follows the port interface and execute the tasks, get the data, etc.
You can imagine the port is a space reserved only for an exact type of vessel. The vessel only can enter the port and dock if the load/unload doors are of an expected size and are in the correct position. Multiple vessels can fit in a port and vessels can be replaced, but ports are unique and can not be moved.
A key concept is that the core doesn&rsquo;t know anything about how are the external internals. The port defines the vessel doors positions but doesn&rsquo;t care about how the load is stored in the vessel.
In this case, we will also use the repository pattern (that fits very well with hexagonal as defines a centralized and abstract way of accessing data and it is a very common pattern), and the dependency injection principle that allows us to create decoupled (or loosely coupled) software. Simplifying again, it allows us to replace an adapter with another one that follows the same port interface.
Let&rsquo;s see it in action with a small (and typical) example:
Your domain (core) needs to get a list of users with a name, so you define the port that is a repository. The port defined a method to do that: getUsersByName(name: string): User[] In English, defines that the adapter must provide a method called \`getUsersByName&rsquo; that gets a name and should return the list of the users that match that name.
If you want to go further in those patterns, there is a lot of documentation on the internet. I want to focus the post on a real case
A real case The initial context We have a single web application (frontend) that works for different clients (tenants), that application uses a backend that provides the menu data. The backend returns something like this:
{ &#34;title&#34;: &#34;Main Menu&#34;, &#34;id&#34;: &#34;main&#34;, &#34;is_staff&#34;: false, &#34;items&#34;: [ { &#34;title&#34;: &#34;Home&#34;, &#34;icon&#34;: &#34;&#34;, &#34;url&#34;: &#34;/&#34;, &#34;is_staff&#34;: false }, { &#34;title&#34;: &#34;Dashboards&#34;, &#34;icon&#34;: &#34;dashboards&#34;, &#34;id&#34;: &#34;dashboards&#34;, &#34;is_staff&#34;: false, &#34;items&#34;: [ { &#34;title&#34;: &#34;Home&#34;, &#34;icon&#34;: &#34;dashboards-home&#34;, &#34;url&#34;: &#34;/dashboards&#34;, &#34;is_staff&#34;: false }, { &#34;title&#34;: &#34;Config üö´&#34;, &#34;icon&#34;: &#34;dashboards-config&#34;, &#34;url&#34;: &#34;/dashboards-config&#34;, &#34;is_staff&#34;: true }, { &#34;title&#34;: &#34;Advanced Reports&#34;, &#34;icon&#34;: &#34;&#34;, &#34;id&#34;: &#34;advanced_reports&#34;, &#34;is_staff&#34;: false, &#34;items&#34;: [ { &#34;title&#34;: &#34;Sales Analysis&#34;, &#34;icon&#34;: &#34;&#34;, &#34;url&#34;: &#34;/sales_analysis&#34;, &#34;is_staff&#34;: false }, ... ] } ] } ] } The front end implements partially the repository pattern as it just returns the data the backend provides without more manipulation than remove the first level in the tree (the main menu item). The view executes the repository call using a service, that again, just returns the same information that it gets from the repository.
The issues This &ldquo;architecture&rdquo; works, but have some issues can create serious problems in the future:
The data structure is coupled to the backend data: All the data flows from the backend to the view using the same interfaces, if the backend changes just the name of a property we need to follow the data flow in our code until the view changes it in all the places. The title string includes an emoji to allow users to visualize when a menu item is only for staff users: That information is also provided in the is_staff property, if we want to expose a menu item to the regular users we need to change it in 2 places, and that is never a good idea. Visuals are defined in the backend: The name of the icon to use is defined in the backend. Unless the icon would be an app (backend + frontend) global concept it is not a good idea to pass that value front the backend. No domain: there is no domain, or at least no explicit one. Logic is applied in the view (that it is not bad per se, but if the logic is related to the business rules, it must live in the domain) The problem Because of different reasons the company decided to create a new version of the backend. This new backend (called v2) will not be retro-compatible with the legacy one, but it will represent semantically the same entities.
The menu endpoint will return the same menu (it will provide more features) but the new endpoint response structure is completely different:
[ { &#34;menuStateId&#34;: 3, &#34;menuPosition&#34;: 1, &#34;menuName&#34;: &#34;Dashboards&#34;, &#34;menuItemId&#34;: 9, &#34;menuItemTitle&#34;: &#34;Home&#34;, &#34;menuItemPosition&#34;: 1, &#34;menuItemLink&#34;: &#34;/dashboards&#34;, &#34;menuItemStateId&#34;: 3, &#34;menuInternalName&#34;: &#34;dashboard&#34;, &#34;menuId&#34;: 12, &#34;menuParentId&#34;: 1, &#34;menuItemInternalName&#34;: &#34;dashboard.home&#34; }, { &#34;menuStateId&#34;: 3, &#34;menuPosition&#34;: 1, &#34;menuName&#34;: &#34;Dashboards&#34;, &#34;menuItemId&#34;: 9, &#34;menuItemTitle&#34;: &#34;Sales analysis&#34;, &#34;menuItemPosition&#34;: 1, &#34;menuItemLink&#34;: &#34;/sales_analysis&#34;, &#34;menuItemStateId&#34;: 3, &#34;menuInternalName&#34;: &#34;dashboard&#34;, &#34;menuId&#34;: 12, &#34;menuParentId&#34;: 1, &#34;menuItemInternalName&#34;: &#34;dashboard.sales_analysis&#34; }, { &#34;menuStateId&#34;: 3, &#34;menuPosition&#34;: 1, &#34;menuName&#34;: &#34;Dashboards&#34;, &#34;menuItemId&#34;: 9, &#34;menuItemTitle&#34;: &#34;Config&#34;, &#34;menuItemPosition&#34;: 1, &#34;menuItemLink&#34;: &#34;/dashboards-config&#34;, &#34;menuItemStateId&#34;: 1, &#34;menuInternalName&#34;: &#34;dashboard&#34;, &#34;menuId&#34;: 12, &#34;menuParentId&#34;: 1, &#34;menuItemInternalName&#34;: &#34;dashboard.sales_analysis&#34; }, { &#34;menuStateId&#34;: 3, &#34;menuPosition&#34;: 1, &#34;menuName&#34;: &#34;Dashboards&#34;, &#34;menuItemId&#34;: 9, &#34;menuItemTitle&#34;: &#34;Sales analysis&#34;, &#34;menuItemPosition&#34;: 1, &#34;menuItemLink&#34;: &#34;/sales_analysis&#34;, &#34;menuItemStateId&#34;: 3, &#34;menuInternalName&#34;: &#34;dashboard&#34;, &#34;menuId&#34;: 12, &#34;menuParentId&#34;: 1, &#34;menuItemInternalName&#34;: &#34;dashboard.sales_analysis&#34; }, { &#34;menuStateId&#34;: 3, &#34;menuPosition&#34;: 1, &#34;menuName&#34;: &#34;Main&#34;, &#34;menuItemId&#34;: 11, &#34;menuItemTitle&#34;: &#34;Home&#34;, &#34;menuItemPosition&#34;: 5, &#34;menuItemLink&#34;: &#34;/&#34;, &#34;menuItemStateId&#34;: 3, &#34;menuInternalName&#34;: &#34;home&#34;, &#34;menuId&#34;: 1, &#34;menuParentId&#34;: 1, &#34;menuItemInternalName&#34;: &#34;home&#34; }, ... ] The new backend endpoint returns the menu items and its parent menu data in the same row. The structure is flat (no nested items). Another difference is the is_staff, is still there, but it‚Äôs a specific value for the menuItemStateId property. No icon name, but now we have an internalId as a semantic unique id.
Things can become harder The new backend will not replace the legacy one, at least not in the next months, clients will be migrated slowly to the new backend. So some clients will use the legacy backend and another will use the new one. That means we will have both backends working at the same time for months.
As the data returned by both backends is very different seems tough to use the same frontend code to render the menu for all the clients, right? (not really as we will see later)
A possible solution is to create different menu-related components, code, etc. depending on the backend version adapting our application to them, this can work, but means we will need to duplicate a lot of code, for example, the views, the services, etc making the maintenance harder.
Decoupling us from the backend Let&rsquo;s forget for a while how is the data the backend(s) returns, and think in what we want to represent from the point of view of our application.
We want to represent a menu that can have items with children items (and no link) and items with links a no children. Then let&rsquo;s create a model, models in our case, in our domain as entities which will represent exactly that:
type State = &#39;disabled&#39; | &#39;only_for_staff&#39; | &#39;open&#39; class Menu { readonly id: number = 0 readonly internalName: string = &#39;&#39; readonly title: string = &#39;&#39; readonly icon: string = &#39;&#39; readonly image: URL | undefined readonly state: State = &#39;open&#39; readonly description: string = &#39;&#39; readonly position: number = 0 readonly children: (Menu | MenuItem)[] = [] constructor(values: MenuDto) { this.id = values.id this.internalName = values.internalName //... this.children = values.children } get onlyForStaff(): boolean { return this.state === &#39;only_for_staff&#39; } } class MenuItem { readonly id: number = 0 readonly internalName: string = &#39;&#39; readonly title: string = &#39;&#39; readonly icon: string = &#39;&#39; readonly url: string = &#39;&#39; readonly state: State = &#39;open&#39; readonly position: number = 0 readonly menuId: number = 0 private constructor(values: MenuItemDto) { // hydrate the entity this.id = values.id //.... } get isStaff(): boolean { return this.state === &#39;hidden&#39; } public get external(): boolean { return (this.url.includes(&#39;http://&#39;) } } This is a simplified version of the entities, but you can see the idea. We have a Menu entity that can have children that can be Menu or MenuItem entities. The MenuItem entity has a url property that can be used to know if the item is a link or not.
We modeled the domain and our application layer (and views) can access to it.
The key is: we modeled our menu independently of our backends&rsquo; data structures, we can use any backend that represents that entity to get the data independently of the structure.
The port We should create the port that will allow us to get the menu&rsquo;s data from the backend(s).
interface MenuRepo { getMainMenu(states: State[]): Promise&lt;(MenuItem | Menu)[]&gt; } The port defines how the repository should look like. In this case, we want a method that will return the main menu, filtered by state ('disabled' | 'only_for_staff' | 'open').
The adapter. The repositories will do the magic We need to create the adapters that will get the data from the backend and transform it to our domain entities. We need an adapter, also called repository implementation for each backend (we could have even more for mocked data, stubs for testing, etc).
Remember, the repository implementation (adapter) is the one that knows the &ldquo;external to the core&rdquo; internals:
How to get the data at the infrastructure level: REST, Graphql, local storage, etc How to request the data: for example for an XHR request: headers, query params, URL, etc The returned data structure and how to transform it to the domain entities How to handle errors, retries, etc How to cache the data But again, the domain NEVER should not know about that.
For example, the domain must not know that to get items available only for staff users we need to pass the menuItemStateId param with the value1.
menuItemStateId is an implementation detail. it only makes sense in repository implementation, not in the domain, the domain should know about the onlyForStaff meaning and the adapter should know how to get that information from the backend.
In this case (for backend v2) we need to pass a query param called menuItemStateId with the value 1 to get the staff-only items, but that is different for the legacy backend. Or for another backend that can use a different value for that filter, but the argument that represents what we want is still the same: onlyForStaff.
From the point of view of the layers on the right side the port&rsquo;s line in the workflow (Image above) do not matter how the data is retrieved, the only thing that matters is the data is returned as a domain entity. That is our contract.
// menu.legacy.repo.ts type Response = { // This type defines the shape of the data the backend returns. I do not include it here to put the focus on the data transformation into entities } class LegacyMenuRepo implements MenuRepo { async getMainMenu(states: State[]): Promise&lt;(MenuItem | Menu)[]&gt; { // 1Ô∏è‚É£ const data = await fetch&lt;Response&gt;(&#39;tenant.company.com/get_menu&#39;) const backendMenu = await data.json() return backendMenu.map(item =&gt; responseToEntity(backendMenu)) } private responseToEntity(response: Response): (MenuItem | Menu) { // transform the response to the domain entities if (&#39;items&#39; in response) { return new Menu({ id: response.id, internalName: response.id, title: response.title, icon: mapIcon(response.id), // 2Ô∏è‚É£ image: mapImage(response.image), // 2Ô∏è‚É£ state: mapState(response.state), // 3Ô∏è‚É£ children: response.items.map(item =&gt; responseToEntity(item)) }) } else { return new MenuItem({ id: response.id, internalName: response.id, title: response.title, icon: mapIcon(response.id), // 2Ô∏è‚É£ url: response.url, state: mapState(response.state), // 3Ô∏è‚É£ menuId: response.menuId }) } } } Things to put focus on:
1Ô∏è‚É£: the method that receives the states&rsquo; argument is not used in the code: This is because the backend does not accept any filter, the legacy backend does the filtering using the backend context. But it ensures will only return the items the user can have access to. 2Ô∏è‚É£: Those map functions are in charge of providing the correct icon and image, now the backend does not provide that information, so our repository implementation should provide it. Remember the repository implementation (adapter) is the one that knows all the external internals and for the images the adapter knows that if the id is &ldquo;x&rdquo; should return the image &ldquo;y&rdquo; and the icon &ldquo;z&rdquo; 3Ô∏è‚É£: The mapState function behavior is similar to 2Ô∏è‚É£ but in this case, the backend returns a number that represents the state, the adapter should know how to map that number to the domain state, and that function can be reversed to know with the state should be sent to the backend. We need to implement the adapter for the &ldquo;new&rdquo; backend (v2):
// menu.v2.repo.ts type Response = { // This type defines the shape of the data the backend v2 returns. I do not include it here to put the focus on the data transformation into entities } const stateMappings: Record&lt;number, State&gt; = { 0: &#39;disabled&#39;, 1: &#39;only-for-staff&#39;, 2: &#39;open&#39; } const stateMappingsReverse: Record&lt;number, State&gt; = { &#39;disabled&#39;: 0, &#39;only-for-staff&#39;: 1, &#39;open&#39;: 2 } class V2MenuRepo implements MenuRepo { async getMainMenu(states: State[]): Promise&lt;(MenuItem | Menu)[]&gt; { const data = await fetch&lt;Response&gt;(&#39;menu.company.com/company/get&#39;, { // 1Ô∏è‚É£ params: { menuItemStateId: states.map(state =&gt; stateMappingsReverse[state]) // 2Ô∏è‚É£ } }) const backendMenu = await data.json() return backendMenu.map(item =&gt; responseToEntity(backendMenu)) } private responseToEntity(response: Response): (MenuItem | Menu) { //Here the transformations from flat to nested are more complex (require more code lines) so I&#39;m going to ignore it in the example. Let&#39;s imagine it is done after this line // transform the response to the domain entities if (&#39;items&#39; in response) { return new Menu({ id: response.id, internalName: response.internalName, title: response.title, icon: mapIcon(response.internalName), // 3Ô∏è‚É£ image: mapImage(response.internalName), // 3Ô∏è‚É£ state: stateMappings[response.menuStateId], // 4Ô∏è‚É£ children: response.items.map(item =&gt; responseToEntity(item)) }) } else { return new MenuItem({ id: response.id, internalName: response.id, title: response.title, icon: mapIcon(response.internalName), // 3Ô∏è‚É£ url: response.url, state: stateMappings[response.menuItemStateId], // 4Ô∏è‚É£ menuId: response.menuId }) } } } Things to put the focus in the backend v2 repo implementation:
1Ô∏è‚É£: The endpoint (even the domain) is different from the other repo. That is expected as it is a different backend. 2Ô∏è‚É£: Get need to convert the meaning of the filters to the backend meaning. The adapter knows that the backend expects a query param called menuItemStateId with the values 0, 1, or 2 to get the items with the state disabled, only-for-staff, or open 3Ô∏è‚É£: We have mapping functions for the icons and images, but this function is different from the legacy one. 4Ô∏è‚É£: we convert the menuItemStateId and menuStateId to the domain state using the mappings. After the changes, the architecture looks like this:
Now we have 2 different adapters (one per backend) for the same port. Those adapters follow the contract and convert the backend data to the domain entities
The rest of the flow is the same: the domain does not know how the data is retrieved, only knows how to use it. This gives us a lot of flexibility, we can change the backend without changing the domain the application, or the views.
The dependency injection The last piece of the &ldquo;puzzle&rdquo; is the dependency injection, which allows us, to replace a repository implementation with another one that follows the same port interface, but instead of importing it from the code that will call the repository, we inject it from outside allows us to change it easily.
Let&rsquo;s suppose we have a usecase (or application service) that will use the repository to get the menu:
class GetMainMenuUseCase { constructor(private menuRepo: MenuRepo) {} async execute(states: State[]): Promise&lt;(MenuItemDto | MenuDto)[]&gt; { return this.menuRepo.getMainMenu(states).map(entity =&gt; entity.toDto()) } } We can use a factory to create the repository implementation:
function createMenuRepo(clientId: string): MenuRepo { if ([&#39;client123&#39;, &#39;client34&#39;].includes(clientId)) { return new V2MenuRepo() } else { return new LegacyMenuRepo() } } const useCase = new GetMainMenuUseCase(createMenuRepo(&#39;client123&#39;)) usecase.execute([&#39;open&#39;, &#39;only-for-staff&#39;]) Summarizing The hexagonal architecture, the repository pattern, and the dependency injection are very powerful tools that allow us to create decoupled software that works in independent pieces loosely coupled that can be easily changed and make the maintenance simpler.
Those pieces should define a contract for the actions (execute a method) and for the returned data, should not uses in other places, for example is a bad practice to pass the filters directly to the repository implementation and use them as is in the http request because you ara coupling your application code to the backend as we see in the example when we map the filter values
As you can see in the example, we can change the backend at any moment, it&rsquo;s just to change the repo implementation we inject into the use case without changing anything else.
This will work only if all the different backend returns the same business concepts, if not we are talking about different domain models and we need to create different ports and adapters for each one.
Achieve that can require time and knowledge of the domain and the business rules, but the benefits are worth it.
`,url:"https://sergiocarracedo.es/hexagonal-architecture-frontend/",image:"/hexagonal-architecture-frontend/shaah-shahidh--subrrYxv8A-unsplash_hu_209d35f9cc0327dd.jpg",tags:["typescript","architecture","clean code","hexagonal"],readingTime:"14 minutes read",date:"May 20, 2024"},"https://sergiocarracedo.es/ui-components-library-ii-anatomy-interfaces/":{title:"UI components library (Chapter II): Components anatomy and interfaces",content:` This is the second post of a series about why and how to create a UI components library. I&rsquo;m going to focus on the code examples in Vue.js, but the concepts are valid for any other framework like React, Angular, LitElements, etc.
Chapter I: Introduction
Before creating a components library, it&rsquo;s important to understand the anatomy of a component, its interfaces, and how to create a good &ldquo;API&rdquo; for the components.
Anatomy of a component A component is a reusable piece of software that encapsulates some parts of the user interface.
A component it&rsquo;s similar to a function, or an object. All of them get some input and return some output. The function (if it is not pure) or the object will use other values like the internal state and the environment to execute the action changing the output, but basically, we can think of a function like a black box that gets some inputs and produced some outputs and it can change the internal state.
If we take a look inside the black box, probably that function will use other functions to do part of the work.
The same happens with a component. A component is a black box that gets some inputs and produces some outputs.
The inputs can be pure data, for example, the data to render in a table, or modifiers, which change the components behavior (define if the table to render is sortable), change the visualization (define the color of the table header), modify the output (define the number of rows to render) or modify the interaction (define the behavior when the user clicks in a row).
The outputs can be the rendered HTML, the events emitted by the component, the values stored in a global store, etc.
Inside the component, we can have other components, and the component can maintain an internal state and/or could use values from the environment (like the global state store, the router, etc).
At this point, we can define the component&rsquo;s interfaces or the surfaces of the black box. I usually like to divide those interfaces into two categories, depending on the developer&rsquo;s point of view:
External interface It&rsquo;s the interface the developer can use to interact with the black box (component) and to get the results. The interface could not be the same for all the frameworks, but the concepts are similar.
Props: The values the component can receive from the parent to modify the behavior. You can think of the props like the function&rsquo;s arguments. Events: The events emitted by the component. HTML &amp; CSS: The goal of a component: is to render HTML and CSS. The component returns the HTML and CSS to be rendered in the browser. Store state: The component can get values from a global store (Pinia, Vuex, Redux, etc) to modify the behavior. (This is not recommended as is hard to track, but sometimes is necessary) Slots: In some frameworks like Vue, the component can receive content from the parent to render a fragment. Are similar to props but allow to pass random content instead of values. Internal interface This is the interface that the component uses internally to interact with the subcomponents, the browser (get events, access to browser API like network), etc. The developer can&rsquo;t interact with this interface directly when is using the component. In this category we can find:
Browser events: The component captures browser events like clicking, mouseover, etc. Subcomponents: The component can use other components to do part of the work. The subcomponents are like the functions the component uses to do the work. The component interacts with the external interface. This is like a matrioska. Browser API: The component can use the browser API to do some work. For example, to get the current location, the current time, etc Network: The component do a network request to get or set data. (We will talk about this in another post). It&rsquo;s important to mention that the internal interface is transparent from outside the component, and the developer can&rsquo;t interact with it directly. This interface can change anytime without notice, and the developer should not rely on it. For example, the component can change the subcomponents, the browser events, etc, but the external interface should be stable.
Now we can understand the anatomy of a component and that will be useful to categorize them depending on how they interact with the external and internal interfaces (for example if the component has access to the network or the global store). But this is a topic to discuss in deep in the next chapter.
`,url:"https://sergiocarracedo.es/ui-components-library-ii-anatomy-interfaces/",image:"/ui-components-library-ii-anatomy-interfaces/OIG2_2_hu_8fde5cb82c99e8e7.jpeg",tags:["ui","frontend","ux","components","vue"],readingTime:"4 minutes read",date:"Apr 8, 2024"},"https://sergiocarracedo.es/typescript-enums-ii/":{title:"Typescript enums: Replacing them with const and union types",content:`Last year I wrote a post about the typescript enums: how to use them, some of the disadvantages, how to mitigate them, and how to replace the enums with const enums and read-only maps.
In that post, I showed how to replace the enums with const enums and read-only maps, and I also mentioned the union types as an alternative to the enums.
In this post, I want to go deeper into the union types as a replacement for the enums in some cases.
Union types as enum replacement If you remember one of the advantages of the enums is the grouping of the values giving them a semantic meaning, and limiting the possible values to use. They are also real values not only types, which means you can use them in the runtime, for example, to get a list of the possible values.
enum HttpResponseStatus { NotFound = 404, Forbidden = 403, Ok = 200, InternalServerError = 500 } const httpStatusNames = Object.values(HttpResponseStatus).filter(v =&gt; typeof v === &#39;string&#39;) Yes, I know that code looks weird, but the reason why is how the enums work in Typescript (please check the previous post for more details).
If we don&rsquo;t need the semantic meaning, we can use the union types to replace the enums, for example, the previous enum can be replaced by the following union type:
type HttpResponseStatus = 404 | 403 | 200 | 500 But, what if I need the list of the possible values in runtime?. This is a type, and types don&rsquo;t exist in the runtime, so we can&rsquo;t get the list of the possible values.
We can create a const array with the possible values, and use it to get the list of the possible values:
const HttpResponseStatusValues = [404, 403, 200, 500] as const type HttpResponseStatus = 404 | 403 | 200 | 500 But doing that means we need to maintain the list of the possible values in two places, the type, and the array, and that&rsquo;s not a good idea.
Typescript: Typeof and indexed access types to the rescue We want to create (or infer) the type from the array to use values and type and don&rsquo;t need to maintain the list of the possible values in two places.
We can use the typeof operator to get the type of the array:
const HttpResponseStatusValues = [404, 403, 200, 500] as const type HttpResponseStatus = typeof HttpResponseStatusValues // readonly [404, 403, 200, 500] Using typeof over the array of possible values we get the type of the array: readonly [404, 403, 200, 500], but that is not the type we want.
To get the type we want we can use the indexed access types:
const HttpResponseStatusValues = [404, 403, 200, 500] as const type HttpResponseStatus = (typeof HttpResponseStatusValues)[number] // &#39;404&#39; | &#39;403&#39; | &#39;200&#39; | &#39;500&#39; Voil√†! we have the type we want, and we can use it as a type, for example in a function&rsquo;s argument, and also use the list of the possible values in runtime:
function someFunction(status: HttpResponseStatus) { if (!HttpResponseStatusValues.includes(status)) { throw new Error(&#39;Invalid status&#39;) } // do something } How it works Let me explain a bit in detail how it works. Imagine we have the following type that represents a &ldquo;complex&rdquo; object to store the car&rsquo;s information:
type Car = { engine: { cylinders: number fuel: &#39;petrol&#39; | &#39;diesel&#39; battery: &#39;lithium&#39; | &#39;lead&#39; } wheels: { count: 4 diameter: 16 } } If we want to get the type of the engine property we can use the indexed access type:
type Engine = Car[&#39;engine&#39;] // { cylinders: number, fuel: &#39;petrol&#39; | &#39;diesel&#39;} engine is not a string, it&rsquo;s a type, and this in key, it&rsquo;s a keyof Car
As the &ldquo;indexed type&rdquo; is a type we can use another kind of type as index, for example, a union type:
type Power = Engine[&#39;fuel&#39; | &#39;battery&#39;] // &#39;petrol&#39; | &#39;diesel&#39; | &#39;lithium&#39; | &#39;lead&#39; And now we can use an arbitrary type like number as an indexed type to get all the types in an array
const HttpResponseStatusValues = [404, 403, 200, 500] as const type HttpResponseStatus = (typeof HttpResponseStatusValues)[number] // &#39;404&#39; | &#39;403&#39; | &#39;200&#39; | &#39;500&#39; The array can be something more complex, for example, an array of objects:
Please note the as const is needed to infer the type of the array as a tuple, if we don&rsquo;t use it the type will be number[] and we will not be able to use the indexed access type.
If you don&rsquo;t need the semantic meaning of the enums, and you need to use the possible values in runtime, you can use the union types and the indexed access types to get the type and the list of the possible values in runtime.
`,url:"https://sergiocarracedo.es/typescript-enums-ii/",image:"/typescript-enums-ii/_081fe03f-6eb1-40ef-9ed6-75f25067b0ad_hu_abc7fdb50b6dd81f.jpeg",tags:["typescript"],readingTime:"4 minutes read",date:"Apr 1, 2024"},"https://sergiocarracedo.es/ui-components-library-i-why-and-to-create-one/":{title:"UI components library (Chapter I): Why and to create one? Characteristics of a good UI library",content:` This is the first post of a series about why and how to create a UI components library. I&rsquo;m going to focus on the code examples in Vue.js, but the concepts are valid for any other framework like React, Angular, LitElements, etc. In this first post I&rsquo;m going to talk about the component of why you should (or should not) create a UI components library, but first let&rsquo;s define what is a UI components library.
What is a UI components library? A UI components library is a set of reusable components that can be used to build a user interface. These components can be used in different projects and applications. A component wraps app behavior, visual behavior, and presentation. Typically, the components represents the visual style of the company or the product.
An example of this is the Material Design library, which is Google&rsquo;s design system, and it&rsquo;s used in all Google&rsquo;s products and services (and as is open source anyone can create apps with the same visual style).
A components library contains the building blocks of the UI of an application.
What is a design system? You will find this concept usually related to UI components library, but they are not the same. A design system is a set of rules and guidelines that define how an application or web and all its elements should look like, and how they should behave. It can ensure the consistency of the application and the quality.
A design system is a concept over the UI components (and UI components library), as those rules apply to the components (and usually define some of them), but also apply to the whole application defining a visual language (the typography, the colors, the spacing, layout, etc).
Advantages of using a UI components library I think that using a UI components library is something that doesn&rsquo;t need discussion, it&rsquo;s a must in most cases. But let&rsquo;s see some advantages of using a UI components library.
Consistency: All the components will have the same look and feel, and will behave in the same way. This is especially important when you have a big team working on the same project, as you can ensure that all the components will look and behave in the same way. Re-usability: You can reuse the components in different projects and applications. This is a huge advantage, as you can save a lot of time and effort. This is especially important for big and complex components, for example a date picker, or a table component. You don&rsquo;t need to write the code to display the component and the logic to make it work, you just need to use the component. Maintainability: As you have a single source of code for the components, you can maintain them in a single place, and all the applications that use them will be updated (we will see in other posts how to manage the breaking changes and the versions). This is especially important when you need to fix a bug or add a new feature to a component, as you only need to do it once in one place. Encapsulation: This is related to the re-usability. With well-encapsulated components you only need to worry about the component interfaces, the internal implementation can change without affecting the rest of the application. Remember that it not only affects the code, but affects the visual style, so you can change the visual style of a component without affecting the rest of the application. In a few words, a UI components library can save you a lot of time and effort and can ensure the quality, maintainability, and consistency (visual and behavior) of your application(s).
Why to create a UI library? After the previous section, we can agree that a UI library is a good idea, but why create a new one? There a lot of UI libraries available (even for free), and most are very good and very complete. So, why create a new one?
There is no correct answer to this question, maybe the best one is: &ldquo;It depends&rdquo;. And even if you make a decision today, it can change in the future when the project scope or requirement changes.
Let&rsquo;s see some reasons to create a new UI library:
Other existing libraries are not adapting to your needs Maybe you need a very specific component, or you need to change the behavior of an existing component, or you need to change the visual style of the components in a way the third-party library can not support or the behavior is not the one you need.
Independence and control Related to the previous point, maybe you want to have full control of the components, and you don&rsquo;t want to depend on a third-party library. This is specially important when you need to make a lot of changes to the components, or when you need to have control of the roadmap. You can help to maintain the third-party library, but you can not control the roadmap.
Custom design system Third-party libraries are very good, but they are generic, and maybe you need a very specific visual style, with a third-party library it&rsquo;s very hard to achieve that.
Most third-party libraries are very customizable, some even provide an un-styled version that allows you to provide all the CSS to theme the components, but in some cases is not enough, and you need to change the structure of the components, or the behavior, and that&rsquo;s not possible just with CSS. (There are helpers to build libraries that allow to you to achieve that, I will explain in detail later.)
Why not create a UI library? If you are not sure if you need to create a new UI library, maybe you don&rsquo;t need it. If your project is a PoC or a small project, or you are not sure about the requirements, maybe you don&rsquo;t need to create a new UI library. You can start using a third-party library, and consider evolving it to a custom library if you need it.
A custom UI library requires a lot of effort, and it&rsquo;s a long-term commitment, makes the development slower in the early stage (make it faster later), needs developers with more expertise and knowledge, needs to be very well defined in terms of requirements, and needs to be maintained and updated.
In this case, my recommendation is to wrap the third-party library in your components, not just copying the properties and events, it&rsquo;s about what they mean and implementing only the ones make sense for you. This way you can change the third-party library in the future without affecting the rest of the application, keeping the same interface. I will talk about the components UI interfaces in a future post of the series.
Characteristics of a good UI library A good library should:
Be flexible, but not infinitely The components should be able to be customized in a lot of ways and should be able to be extended. For example, a button component should be able to be customized in terms of colors, sizes, shapes, etc. Add it an icon or have a loading state, be disabled, etc. But those customizations should be limited, for example, size instead of a number, can be a set of predefined values (small, medium, large, etc). This is important to ensure the consistency of the application, it&rsquo;s not nice to see buttons with a typography that differs 1px from the next one. Same with the colors (also de colors have a semantic meaning it&rsquo;s not just a visual to make it beautiful).
The components should provide the needs of the application, without the need to change the component. If you need to change the component, maybe you need a new component or something is wrong in the design, usability, etc.
It&rsquo;s very important to be strict with that and avoid very specific customizations just because the component does not fit a specific usecase. It&rsquo;s better to stop and about the use cases and try to find a solution that fits the component.
Adaptability The components should be able to be adapted to different projects and applications. To achieve that the components should provide an abstract solution to the problem, it requires thinking, not only about the current requirement, requires to understanding them deeply and think in the future requirements (without trying to cover all the future, I know it&rsquo;s hard to find the balance).
Be well documented This is more important than you can think. A good documentation can save a lot of time and effort and can make the difference between a good library and a bad one. The documentation should be clear, concise and should provide examples and use cases.
The documentation is the entry point for any new member of the team (and a no new member in the team, after a couple of months without using a component I&rsquo;m sure an experimented developer will need to read the docs again to refresh the knowledge) , and it&rsquo;s the first place to look for when you need to use a component. It&rsquo;s very important to have good documentation, and to keep it updated.
Stortbook is a very good tool for creating the documentation of the components, as is not just a text, you can experiment with the behavior of the components, and see what they look like.
Avoid repeat code and overlap component functionality Related to adaptability, and flexibility, the components should be well-defined, and should not overlap functionality. For example, if you have a button component, makes no sense (in my opinion) to have a link button component, or a submit button component. You should have a generic button component, and you should be able to customize it to make it look like a link, a submit button or a cancel button, etc.
Example Usually is easier to understand the bad behaviors and practices than the best ones. Let&rsquo;s see an example of what I found in my professional life:
Note: The objective of this example is not to blame the library or anyone, it&rsquo;s just to show an example of things you should avoid without the need to pass the process.
The library had a Dropdown component, which is totally common in a UI library. The component as you can imagine represents a list of options that can be selected. Provides properties to customize the visual style (size, label position, etc), and events to handle the user interaction.
This seems normal and logical, but by exploring the library you can find another component: Static Picker: Which does the same but allows you to select options in a tree (the previous one was a flat list). ü§Ø
And this is not all, you can find another component: Lazy Picker: Which does the same as the previous but allows loading the item children on demand. ü§Øü§Øü§Ø
The visuals were the same and the behavior was the same, the only difference was the internal implementation. of the items, but common for all the other properties and behavior, but the code was not shared, each component had the same lines of code in its file.
This is a clear example of what you should avoid. Nobody created these 3 components at the same time just for fun, the team developed the first one, and when the requirement of show the items in a tree came, instead of extending the previous component, someone decided to create a new one from the code of the previous one. And months later the requirement of the lazy load came, and the same happened.
This is why I say you need to understand the components and the requirements deeply and think about the future requirements. Even if the new requirements are not on the list of the possible future requirements you planned, you need to stop and think if the component needs a redefinition to fit all new requirements
In this case, I think you can extend the Dropdown component to allow you to show the items in a tree without breaking the compatibility with the previous version, and the same with the lazy load (for example emitting an event when the user opens a tree node to let the page or component is using out dropdown load the children).
Conclusion of the chapter I A UI library is a set of reusable components that can be used to build a user interface. It&rsquo;s a must in most cases, as it can save you a lot of time and effort, and can ensure the quality, maintainability and consistency of your application(s).
You can use a third-party library, but in some cases, you need to create a new one, in this case, you need to pay the price in the early stages and have a more experienced frontend and design team
In the next chapter of this series. I will go deeper in the interfaces of the components and how to define them to ensure the flexibility and adaptability of the components.
Chapter II: Components anatomy and interfaces
`,url:"https://sergiocarracedo.es/ui-components-library-i-why-and-to-create-one/",image:"/ui-components-library-i-why-and-to-create-one/OIG2_hu_f40b2fdb05b15014.jpeg",tags:["ui","frontend","ux","components","vue"],readingTime:"11 minutes read",date:"Feb 12, 2024"},"https://sergiocarracedo.es/2023-in-a-nutshell/":{title:"2023 in a Nutshell",content:`2023 is over and I did in the last years is time to summarize it. For me, it&rsquo;s always an interesting exercise as I&rsquo;m able to remember the challenges, achievements, and fails in the year and put things in context
Professional In December 2022 I joined a new company, Nextail, and after a year I think I can review the journey.
I joined this company as Principal Frontend Engineer, my responsibility and duty was (and is) to identify the improvements needed in the frontend architecture and team organization to improve the delivery and keep growing the app.
After a couple of hard months, I was able to present a proposal of the technical architecture and team organization to the company board and they gave me the green light.
The new architecture is focused on simplifying the frontend for the developers and homogenizing the user experience:
providing common solutions to common problems define code standards, naming conventions create a useful and updated documentation a powerful and flexible UI components library decouple views (UI) and business logic and infrastructure migrate the frontend to a modern reactive and progressive frontend framework for the views (UI) The new team organization is focused in:
remove knowledge silos promote mentoring create a transversal knowledge of the application, which seems something obvious but before the frontend team was divided into modules, and the knowledge was limited to that module. promote clean code and continuous improvement, being responsible and knowing technical debts to have a dedicated manager for the frontend team, and we found one of the best managers I know: Adrian Duran without his help and support I wasn&rsquo;t able to achieve all the goals we did In my opinion (my biased point of view) we made a lot of improvements, achieving all the goals and creating in the team a mentality of improvement.
As an anecdote that summarizes the change in the team&rsquo;s mentality: a few months ago a junior developer should add a new feature in a view. She reviewed the current code, and she proposed to refactor it a little bit as now we have new UI components and composables that provide this behavior and well-defined code standards. I&rsquo;m very proud of that
I&rsquo;m sure 2024 will be a year full of challenges, and maybe changes, but I will still work hard to keep improving my professional skills and continue learning
Community Regarding community 2023 was a year to pass the baton to others, I was very focused in professional changes and I haven&rsquo;t the necessary time to push hard in the community.
This was my last participation in the PulpoCon as co-organizer, it was a hard decision, but I strongly believe is the best one for the conference and for me. I expose my reasons in this post: Hasta luego pulpoCon
Post and Talks Last year I promised myself to write more blog posts, as it&rsquo;s a way to learn, to share, and to improve, and I was able to do it. I wrote 11 posts vs 7 in 2022.
I continue publishing articles in DZone, and I collaborated with Miguel Garcia Lorenzo and to intersting articles:
Generative AI Models: Boosting Merchandising Analytics User Experience And article and a working proof of concept about how to use LLM (ChatGPT) to ask for business information in natural language and get the results in multiple formats: tabla, chart, etc. Front-End: Cache Strategies You Should Know Open source This year I &ldquo;withdraw&rdquo; my project vue-use-model-helpers as a better initiative provides the same features. As I explain in the article, for me is an achievement as I was in the &ldquo;correct&rdquo; direction and I was able to find the same solution the community found.
I also did a few minor PR to opensource projects to fix bugs, but nothing important
Personal / Learnings This year I learned:
how important is the people you work together how motivation can make a difference both professional and personal. how a good manager makes a difference I went skiing after 16 years, and it was very nice, I was able to enjoy my free time and rest while still pushing. I also visit 2 different countries: Andorra and France, now I could say I visit all the Iberian Peninsula &#x1f602;
&#x1f389; Happy 2024!! &#x1f389;
`,url:"https://sergiocarracedo.es/2023-in-a-nutshell/",image:"/2023-in-a-nutshell/pexels-engin-akyurt-13088176_hu_59cede55b1945379.jpg",tags:["lifestyle"],readingTime:"4 minutes read",date:"Dec 29, 2023"},"https://sergiocarracedo.es/spa-vue-react-angular-app-loader/":{title:"SPA (Vue / React / Angular) app loader",content:`As big a SPA is, more resources (javascript, css, images, etc.) need to load before starting to work (render/show the application in the user&rsquo;s browser)
This, the time between the page starts to load and the user can interact with, is the TTI, Time to Interactive. This is an important metric for your application. As higher is the value lower is the user experience using the application.
TTI is a metric for any application, not only for SPAs.
How to improve TTI Improve the TTI (reduce it), depends on several factors:
Network: latency, bandwidth, &hellip; Host: Server load, CPU, disk speed, &hellip; App size: The size and the number of resources to get from the host (remember the modern browsers limit the number of parallel request per host to 6) From the Front-end point of view, the things you can act on are the app size and the number of resources to get.
You can split your application into chunks and group the resources by page, only loading the resources necessary for the first load. You can check your module bundler about how to do that.
This requires time and effort and sometimes is not possible to invest time in that, but there is something simple you can do to improve the user experience
App loader With the typical SPA configuration, when the user reaches your page, nothing is shown before the SPA loads and renders the content. This time can be just a couple of seconds, but from the user&rsquo;s point of view, nothing is happening. We are not providing feedback and that is frustrating. Probably your application will show a loader when the SPA will be loaded, and mounted in the DOM, but we should show something before that.
If you check your favorite js framework entry file, you will find a code that mounts the app into the DOM
Vue // main.ts import { createApp } from &#39;vue&#39; import App from &#39;./App.vue&#39; createApp(App).mount(&#39;#app&#39;) React import { createRoot } from &#39;react-dom/client&#39; const domNode = document.getElementById(&#39;app&#39;); const root = createRoot(domNode); root.render(&lt;App /&gt;); Angular import { Component } from &#39;@angular/core&#39;; @Component({ selector: &#39;app&#39;, templateUrl: &#39;./app.component.html&#39;, styleUrls: [&#39;./app.component.css&#39;], }) export class AppComponent { title = &#39;Test&#39;; } I could continue writing examples for other frameworks, but the basics are the same, your index.html includes an element with the app id (&lt;div id=&quot;app&quot;&gt;&lt;div&gt;), and the framework, will mount the application here after load and render it
What &ldquo;Mount&rdquo; action does is to replace the DOM element content (#app in the examples) with the DOM the framework generates, knowing that we can use it at our convenience.
Any content inside the app mount point will be shown until the framework replaces it, so we can display our loader there, and when the app is mounted it will disappear. For example:
&lt;html&gt;&lt;/html&gt; &lt;body&gt; &lt;div id=&#34;#app&#34;&gt; Loading app... &lt;/div&gt; &lt;/body&gt; How to show the loader as soon as possible: Tips and recommendations for the app loader There is another metric you should know: FCP (First Contentful Paint), this is the time from the page starts to load and when any part of the page is rendered.
We want to make this time as low as possible, our app loader will be unuseful if it takes a &ldquo;long&rdquo; time to load as we will reproduce the same as we are trying to avoid.
Remember the browser will load the HTML file first and will load the links (images, CSS, JS, etc&hellip;) in parallel (bear in mind some of those requests block the render)
To achieve it you can follow the recommendations/tips down bellow:
Use inline CSS, JS, and images If your loader depends on an external stylesheet &lt;link rel=&quot;stylesheep&quot; type=&quot;text/css&quot; href=&quot;style.css&quot;&gt; file, on an image &lt;img src=&quot;./my-image.gif&quot;&gt; or in a js file the browser will need to load it before, and this takes time (and can be blocked by another request).
Use inline CSS, JS, and images. You can include SVG images as part of the HTML document and for binary images (JPG, GIF, etc) you can use data urls
Make it simple as possible Remember the goal of this app loader is to provide feedback to the user to let her/him know the app is loading, no need to create the best loader in the world with animations, interactivity, etc&hellip;
Avoid CSS conflicts with the app If you put all the loader code inside the app mount point it will be replaced, so any css class you are applying will be removed avoiding headaches.
Eliminate render-blocking resources Some html tags trigger browser processes that blocks the rendering until the process will finish, try to remove it as much as possible:
Don&rsquo;t use @import for CSS (use link tag) Use defer and async in javascript More tips at https://blog.logrocket.com/9-tricks-eliminate-render-blocking-resources/ Example I created a simple example in Angular, but the framework is irrelevant
Check an example in code sandbox
The mount code has a 5s delivery delay to let you appreciate the loader
As you can see in the gif bellow, the loader is shown as soon as possible, as soon as the browser loads the html, and the loader is present until the app is mounted, and the user know something is happening. In terms of usability, not only in websites, in any app, (web, desktop, mobile, cli, etc) you must provide feedback to the user to let him/her know something is happening. It&rsquo;s very frustrating to wait for something without knowing if the app is frozen or not or if is doing something and the button just does not work.
With this technique you can improve the user experience of your app with a simple change of a few bytes that don&rsquo;t increase the FCP time more than a few milliseconds.
`,url:"https://sergiocarracedo.es/spa-vue-react-angular-app-loader/",image:"/spa-vue-react-angular-app-loader/pexels-jeshoots-4956_hu_604fe0b3b796a225.jpg",tags:["vue","ux","loader"],readingTime:"5 minutes read",date:"Dec 4, 2023"},"https://sergiocarracedo.es/using-cache-to-decouple-frontend-code/":{title:"The advantage of using cache to decouple the frontend code",content:`We can agree decoupling is a good practice that simplifies the code and the maintainability of the project.
A common way of decoupling the code is to divide the responsibilities into different layers, a very common division is:
view layer: in charge of render HTML and interacting with the user domain layer: in charge of the business logic infra layer: in charge of getting the data from the backend and returning it to the domain layer(here is very common to use the repository pattern, which is just a contract to get the data. The contract is unique but you can have multiple implementations, for example, one for a REST API and another for a GraphQL API, you should be able to change the implementation without changing other pieces in the code) Let&rsquo;s see a couple of examples use cases where it is very typical to put the performance over the decoupling. (Spoiler: we can have both)
Imagine, you have an endpoint that returns the list of products, and one of the fields is the category_id, the response can be something like this (I removed other fields to make a simple example)
[ { id: 1, name: &#34;Product 1&#34;, category_id: 1 }, { id: 2, name: &#34;Product 2&#34;, category_id: 2 }, ... ] We need to show the category name in the frontend (not the id), so we need to call another endpoint to get the category name, that endpoint returns something like
[ { id: 1, name: &#34;Mobile&#34; }, { id: 2, name: &#34;TVs&#34; }, { id: 3, name: &#34;Keyboards&#34; }, ... ] You can think the backend should do the join, and return all-in-one requests, but that is not always possible
We can do the join in the frontend, in the function or method in charge of recovering the products, we can do both requests and join the information. Example
async function getProductList(): Promise&lt;Product[]&gt; { const products = await fetchProducts(); const categories = await fetchCategories(); return products.map(product =&gt; { const category = categories.find(category =&gt; category.id === product.category_id); return { ...product, category_name: category.name }; }); } Our application doesn&rsquo;t need to know anything about we need 2 calls to recover the information, and we can use the category_name in the frontend without any problem.
Now imagine you need to show the list of categories, for example in a dropdown. You can reuse the fetchCategories function, as it does exactly what you need.
In your view the code is something like this:
&lt;template&gt; &lt;dropdown :options=&#34;categories&#34; /&gt; &lt;product-list :products=&#34;products&#34; /&gt; &lt;/template&gt; &lt;script lang=&#34;ts&#34; setup&gt; import { fetchCategories, getProductList } from &#39;@/repositories&#39;; const categories = await fetchCategories(); const products = await getProductList(); &lt;/script&gt; And in that point, you realize you are doing 2 calls to the same endpoint to recover the same data, data you recovered to compose the product list, and that is not good in terms of performance, network load, backend load, etc.
At this moment, you start to think about how to reduce the number of calls to the backend, in this case, to just reuse the category list. You can have the temptation of moving the calls to the view and doing the join of the products and the categories.
// ‚ùå‚ùå‚ùå Not nice solution &lt;template&gt; &lt;dropdown :options=&#34;categories&#34; /&gt; &lt;product-list :products=&#34;products&#34; /&gt; &lt;/template&gt; &lt;script lang=&#34;ts&#34; setup&gt; import { fetchCategories, fetchProducts } from &#39;@/repositories&#39;; const categories = await fetchCategories(); const products = await fetchProducts().map(product =&gt; { const category = categories.find(category =&gt; category.id === product.category_id); return { ...product, category_name: category.name }; }); &lt;/script&gt; With that, you resolved the performance problems, but you added another BIG problem: infra, view and domain coupling. Now your view knows the shape of the data in the infra (backend), and makes it hard to reuse the code. We can go deep on this and do things even worse, what happens, if your head bar (is in another component that needs the list of categories), you need to think about the application in a global way.
Imagine something more complex, a scenario where you need the categories in the header, product list, filters, and the footer
With the previous approach, your app layer (Vue, react, etc) needs to think about how to get the data to minimize the requests. And that is not good, as the app layer should be focused on the view, not on the infra.
Using a global store One solution to this problem is to use a global store (vuex, pinia, redux, etc) to delegate the requests and just use the store in the view. The store only should load the data if is not loaded yet, and the view should not care about how the data is loaded. This sounds like cache, right? We solve the performance issue, but we&rsquo;re still having infra and view coupled.
Infra cache to the rescue To decouple as much as possible the infra and the view, we should move the cache to the infra layer (the layer in charge of getting the data from the backend). Doing that we can call the infra methods at any time doing just a single request to the backend, but the important concept is that the domain, the application, and the view know nothing about the cache, about the network speed, the number of requests, etc.
The infra layer is just a layer to get the data with a contract (how to ask for the data and how the data is returned), following the decoupling principles we should be able to change the infra layer implementation without changing the domain, application or view layers. For example, we can replace the backend that uses REST by a backend that uses GraphQL, and we can get the product with the category names without doing 2 requests. But again, this is something the infra layer should care about, not the view.
There are different strategies you can follow to implement the cache in the infra layer: HTTP cache (Proxy or Browser internal cache), but in these cases for better flexibility invalidating the caches in the frontend it&rsquo;s better our application (infra layer again) manage the cache.
If you are using axios you can use Axios Cache Interceptor to manage the cache in the infra layer. This library makes caching very simple:
// Example from axios cache interceptor page import Axios from &#39;axios&#39;; import { setupCache } from &#39;axios-cache-interceptor&#39;; // same object, but with updated typings. const axios = setupCache(Axios); const req1 = axios.get(&#39;https://api.example.com/&#39;); const req2 = axios.get(&#39;https://api.example.com/&#39;); const [res1, res2] = await Promise.all([req1, req2]); res1.cached; // false res2.cached; // true You only need to wrap the axios instance with the cache interceptor, and the library will take care of the rest.
TTL TTL is the time the cache will be valid, after that time the cache will be invalidated and the next request will be done to the backend. The TTL is a very important concept, as it defines how fresh the data is.
When you are caching data a challenging problem is data inconsistency. In our example, we can think of a shopping cart. If it&rsquo;s cached and the user adds a new product if you apps a request to get the updated version of the cart it will get the cached version, and the user will not see the new product. There are strategies to invalidate the cache and solve this problem, but that is out of the scope of this post, but you need to know that is not a trivial problem: Different uses cases need different strategies.
As longer the TTL is, bigger the data inconsistency problem is, as more events can happen in that time.
But for the goal we are looking for (allowing to decouple the code easily), a very low TTL (ex. 10 seconds) is enough to remove the data inconsistency problem.
Why a low TTL is enough? Think about how the user interacts with the application:
The user will ask for a url (it can be part of a SPA or a SSR page) The application will create the layout of the page, mounting the independent components: the header, the footer, the filters, and the content (product list in the example) Each component asks for the data it needs to do The application will render the page with the data recovered and send it to the browser (SSR) or inject/update it in the DOM (SPA) All those processes are repeated in each page change (maybe partially in a SPA) and the most important thing, are executed in a short period of time (maybe milliseconds). So with a low TTL we can be pretty sure we will do only a request to the backend, and we will not have data inconsistency problems as in the next page change or user interaction the cached expired and we will get the fresh data.
Summarizing This caching strategy in low TTL is a very good solution to decouple the infra and the view:
Developers doesn&rsquo;t need to think about how to get the data to minimize the requests in the view layer. If you need the list of the categories in a sub-component you ask for it, don&rsquo;t need to about if another component is requesting the same data. Avoids maintaining a global app state (stores) Makes more natural to do multiple requests follow the contract in a repository pattern to get the data you need in the repository layer, and do the join in the infra layer. In general terms simplifies the code complexity. No cache invalidation challenged (as the TTL is very low) (Maybe for some very specific use cases) `,url:"https://sergiocarracedo.es/using-cache-to-decouple-frontend-code/",image:"/using-cache-to-decouple-frontend-code/kimi-lee-Dsg--4yCqCI-unsplash_hu_14fd35e74b196508.jpg",tags:["typescript","javascript","infra","frontend"],readingTime:"8 minutes read",date:"Nov 20, 2023"},"https://sergiocarracedo.es/typescript-tips/":{title:"Typescript tips for legacy projects: Type only you need ",content:`When you introduce Typescript in a legacy project, or you are using a library that doesn&rsquo;t provide types, you might be tempted to use any for the types you will need. But this is not a good idea, because you are losing all the benefits of Typescript. any it&rsquo;s something you must remove from code, and from your mind.
Type a shape progressively It&rsquo;s very common to have an object with a lot of properties, and trying to type completely the shape (that is the final goal) of an object you don&rsquo;t completely understand can be an overwhelming task.
But you can start creating the type from the consumer&rsquo;s point of view. In this approach, you type the elements of the object you need
This strategy gives you a starting point to define a structure with simplicity and avoiding the any type.
Let&rsquo;s show you an example:
Imagine your legacy app provides you with a function (that doesn&rsquo;t have the typescript definition) that gets a list of users from the API, and you need to write a function to calculate the age of a user.
function getUser(id) { return { birthday: new Date(&#39;1980-01-01&#39;), name: &#39;John&#39;, surname: &#39;Smith&#39;, role: &#39;user&#39;, accounts: ... ... } } The data structure the function returns is big and complex, and some users have different fields, for example you realize for the users with the role admin you have a field level for others who don&rsquo;t.
You also realize all the users have the birthdate field. That is an important field for you. With these requirements, your function doesn&rsquo;t need other data from the user, so you can start to type your user structure
export interface User { birthday: Date; } function userAge(user: User): number { const diffMs = Date.now() - user.birthday.getTime(); const ageDt = new Date(diffMs); return Math.abs(ageDt.getUTCFullYear() - 1970); } Ok, but now you want to type the return of the function that returns the user. But doing function getUser(id): User you will get a type error as the function returns more fields than the birthday field:
You need to let know typescript you only know the user has the field birthday and more field but you don&rsquo;t know them. Writing that in typescript:
interface User { [x: string]: unknown birthday: Date } Well, it&rsquo;s still a kind of any, but more restricted, for example const user2 = { name: 'Mike'} doesn&rsquo;t fit the type user as the field birthday is missing.
Again, this is the starting point to type the user object without understanding the full object and with the minimum effort, this is much better than just any as when you know completely the object you don&rsquo;t need to change the userAge function.
If you or a teammate add a new function or method that provides more knowledge of the object you can just continue completing the interface, for example if you discover a function in the legacy code to get the full name of the user, and the function lets you know (for example with the check it does) the name is always present, but not the surname, so you can complete the type with:
interface User { [x: string]: unknown name: string surname?: string birthday: Date } With time you will get a complete type for the user object, and you will be able to remove the [x: string]: unknown part.
interface User { [x: string]: unknown birthday: Date } function getUser(): User { return { name: &#39;John&#39;, surname: &#39;Smith&#39;, birthday: new Date(1999, 6, 12) } } function userAge&lt;T&gt;(user: User): number { const diffMs = Date.now() - user.birthday.getTime(); const ageDt = new Date(diffMs); return Math.abs(ageDt.getUTCFullYear() - 1970); } console.log(userAge(getUser())) Run the code in the playground
Type the function, consts, etc you need Nowadays, most of the js libraries include the type definition, the library can expose it or via an external package like DefinitelyTyped. But sometimes the types are not available, maybe because the library is old and/or because the library is not popular enough to have a type definition, or just because the library is private and only available in your company (a.k.a. legacy library).
The goal is to type (or declare in this case) completely the library, but you can just type the function, const, etc you will need. To let typescript know you are declaring your module or library you need to use the declare keyword and the module (library) name, for example:
declare module &#39;my-library&#39; { export function theFunctionIUse(a: number, b: number): number; export const libraryConst: number; ... } This can be in any file of your project and only applies to it. if you want to share the declaration with other files you can create a package with the type definition (Check an example on DefinitelyTyped) and publish it in npm or in your company registry.
Summary The goal is to type completely the legacy code and the libraries, but step by step. You can apply this strategy to improve the type &ldquo;on demand&rdquo; and avoid the any type, bearing in mind this is not the correct solution, it&rsquo;s a path to the correct solution.
`,url:"https://sergiocarracedo.es/typescript-tips/",image:"/typescript-tips/pexels-flo-maderebner-869258_hu_503c1ef2c83f9ac8.jpg",tags:["typescript","javascript","frontend"],readingTime:"4 minutes read",date:"Sep 11, 2023"},"https://sergiocarracedo.es/hasta-luego-pulpocon/":{title:"Hasta luego pulpoCon",content:`Acaba de terminar la tercera edici√≥n de la #pulpoCon, y no ser√° la √∫ltima, pero si la √∫ltima en la que yo participe como organizador.
Han sido 3 ediciones intensas en las que la conferencia ha crecido en cuanto a asistentes, patrocinadores, ponentes, etc. Y esto no es gratis, requiere un esfuerzo y dedicaci√≥n que no puedo mantener. Siempre he pensado que lo que haces lo haces bien o no lo haces.
Por eso, y aunque con no poca pena, he decidido dejar de ser organizador la #pulpoCon, pero no os preocup√©is, la conferencia seguir√°, y seguir√° creciendo, y seguir√° siendo una de las mejores conferencias de desarrollo de software en Espa√±a, el evento #gastroTech de referencia.
No quiero ni puedo dejar de dar las m√°s sinceras gracias a Rolando Caldas por permitirme compartir con √©l estos a√±os dentro de la organizaci√≥n y por todo el trabajo que hace para que la conferencia sea un √©xito, sin ninguna duda √©l es el art√≠fice de que todo esto funcione y sin su trabajo y esfuerzo la conferencia no saldr√≠a adelante.
Tambi√©n a todas y cada una de las personas que hacen esto posible: a todos los ponentes, muchos de ellos amigos, a los que he &ldquo;liado&rdquo; para que hagan una charla o workshop (a veces hasta dos), a los patrocinadores, porque hacen posible que la conferencia exista, y tenga el nivel que tiene, a los asistentes, que son los que hacen que todo esto tenga sentido, por supuesto a todos los voluntarios que nos ayudan a que todo salga bien durante los d√≠as del evento, y a muchos amigos que nos daban consejos y nos ayudaban a mejorar.
¬øPor qu√©? Como comentaba anteriormente, no puedo dedicarle el tiempo que se merece, y sobre todo no tengo las fuerzas y motivaci√≥n necesarias para seguir en el proyecto con la dedicaci√≥n que merece, un evento de este calibre necesita dar lo mejor de uno mismo para que salga adelante y con el paso de las ediciones se ha vuelto cada vez m√°s demandante, y m√°s que lo har√° porque estoy convencido de que la pr√≥xima edici√≥n ser√° a√∫n mejor y m√°s grande.
¬øY ahora qu√©? Pues ahora toca descansar, y dedicarle tiempo a mi familia y a mis gatos, a mi trabajo y a m√≠ mismo. Es necesario dar uno o dos pasos atr√°s para recuperar fuerzas y poder seguir avanzando y este es uno de esos momentos.
Estoy seguro de que la conferencia ser√° todo un √©xito en las pr√≥ximas ediciones, pero eso yo ya lo ver√© desde una posici√≥n distinta.
`,url:"https://sergiocarracedo.es/hasta-luego-pulpocon/",image:"/hasta-luego-pulpocon/cover_hu_9eca25ee51a22fdf.jpg",tags:["community","spanish"],readingTime:"3 minutes read",date:"Sep 10, 2023"},"https://sergiocarracedo.es/js-lists-performance/":{title:"Find a item in lists in js: Performance of Set vs Array",content:"In our apps, it&rsquo;s very common to store data in lists, for example, a list of users, a list of posts, a list of selected ids, etc, and also it&rsquo;s very common too need to check if a value is in the list or not.\nIn most cases, we use simple arrays to store the data, and we use the includes method to check if a value is in the array or not. This works and if your list is not too big, or you only need to find one element once you will not notice any performance issue, imagine the following case:\nWe have a list of users (User[]), this list is immutable\ntype User = { id: number; name: string; email: string; age: number; } and we also have a list of the selected users ids (number[]), Now we should render the list of users (in the order are in the list) and add a different class to the selected users. We can do something like this:\nusers.map(user =&gt; { const class = selectedUsersIds.includes(user.id) ? &#39;user--selected&#39; : &#39;user&#39;; return `&lt;li class=&#34;${class}&#34;&gt;${user.name}&lt;/li&gt;` }) As you can see in the code we need to search n times in the selectedUsersIds array, where n is the number of users.\nIf the list of users is big any performance issue will be noticeable. This is because the render function has a Big O of O(n¬≤)\nObviously, the best to solve the possible performance issues is to reduce the Big O of the function to O(log n) or O(n), but in some cases is not possible or is not worth it.\nIf you want to know more about the Big O notation, I recommend you to read this article\nSo let&rsquo;s try to use Set for that instead of Array.\nconst selectedUsersIdsSet = new Set(selectedUsersIds); users.map(user =&gt; { const class = selectedUsersIdsSet.has(user.id) ? &#39;user--selected&#39; : &#39;user&#39;; return `&lt;li class=&#34;${class}&#34;&gt;${user.name}&lt;/li&gt;` }) The Big O of the function still being O(n¬≤) (If you search about that you will find a lot of articles saying that the Big O of Set.has is O(1), this is not completely true, but the performance is much better, and it can be considered O(1) for most cases) https://stackoverflow.com/a/55057332\nReal life examples Let&rsquo;s create code to test it, and to see the real performance difference between both solutions. For that, we will generate random users list and selectedUserId list with different sizes and measure the time to render the list of users. For that we will use Timers in console.\nconst sizes = [10000, 100000, 500000, 1000000]; sizes.forEach((size) =&gt; { const timeMock = `generate mock dataset for ${size}`; // Random user list sorted randonly console.time(timeMock); const users = Array.from({ length: size }, (_, i) =&gt; ({ id: i, name: `User ${i}` })).sort((a, b) =&gt; 0.5 - Math.random()); // Random selected ids list sorted randonly const selectedIds = Array.from({ length: size / 2 }, (_, i) =&gt; (i * 2) ).sort((a, b) =&gt; 0.5 - Math.random()); console.timeEnd(timeMock); console.log(&#34;+---------------------------+&#34;); console.log(`| Size: ${size} |`); console.log(&#34;+---------------------------+&#34;); // Array .includes console.log(&#34;----------------- ARRAY ---------------&#34;); const timeArray = `array: with ${size} elements`; console.time(timeArray); console.log( users.map((user) =&gt; { return selectedIds.includes(user.id); }).lenght ); console.timeEnd(timeArray); console.log(&#34;----------------- SET ---------------&#34;); // Convert array to set const timeSetTotal = `set total: with ${size} elements`; const timeArrayToSet = `array to set: with ${size} elements`; const timeSet = `set: with ${size} elements`; console.time(timeSetTotal); console.time(timeArrayToSet); const selectedIdsSet = new Set(selectedIds); console.timeEnd(timeArrayToSet); console.time(timeSet); console.log( users.map((user) =&gt; { return selectedIdsSet.has(user.id); }).length ); console.timeEnd(timeSet); console.timeEnd(timeSetTotal); console.log(); }); I ran this code in my computer just copying the code into the browser console:\nAmd Ryzen 5 3600 6-Core Processor 3.59 GHz Ubuntu 22.04 Chrome 115 Never copy code from the internet and paste it into the browser console, it can be dangerous. You can use codesandbox or similar to run the code in a safe environment. https://codesandbox.io/s/wandering-dream-xyyt2w?file=/src/index.mjs:0-1533 (In this case the results are not accurate because the code is running in a sandboxed environment). I invite you to write your own code to test it.\nIn my tests the results where\nUsers Selected users Array Set 10,000 5,000 4.427ms 0.656ms 100,000 50,000 285.895ms 4.404ms 500,000 250,000 7223.516ms 32.631ms 1,000,000 500,000 28808.12ms 66.270ms As you can see the performance difference is huge, and it&rsquo;s more noticeable when the list is bigger. Under 100k items the difference can be acceptable and the user will not notice it, but over 100k items it cause lag in the UI degrading the user experience.\nWhy set is faster? Rather than array that stores the values in memory, Set uses a Hashtable that is intrinsically faster than an array to find a value, as as we mentioned the Big O average is O(1)\nShould I replace my arrays with sets? No, you should not. Sets are not a replacement for arrays, they are different data structures with different purposes. For example:\nSet only stores unique values, so if you need to store repeated values you should use an array The order of items in a set is the insertion order, you can&rsquo;t sort a set, The things may you want to consider in your next project or task are:\nUnderstand and know Big O of your code and how it can affect to the performance. Think if your application will need to handle big amount of data (now or in the future) and evaluate if it&rsquo;s worth to use a different algorithm, data structure, etc that will reduce the Big O. Try to use the types, object, classes, ec (like Set) the language gives to you in the use cases need it. For example if you have a list of unique values and the order doesn&rsquo;t matter, use Set instead of Array, it&rsquo;s faster and adding and removing items from a set is simpler than from an array. Read more about Set\n",url:"https://sergiocarracedo.es/js-lists-performance/",image:"/js-lists-performance/pexels-chait-goli-2031758_hu_c8eceb84d4bc4852.jpg",tags:["js","performance"],readingTime:"5 minutes read",date:"Sep 4, 2023"},"https://sergiocarracedo.es/stream-deck-linux/":{title:"elgato Stream Deck on Linux",content:`Last Christmas I got a Elgato Stream Deck as a present. My very beginning thoughts were: &ldquo;I don&rsquo;t need this, I don&rsquo;t know a real use case for that&rdquo;. But I decided to keep it and try to use it for something.
What is a Stream Deck? The Stream Deck is a device with 15 buttons (in my case), each button has a small screen that can show an icon or a text. The device is connected to your computer via USB and it has software that allows you to configure each button and the action to do when you press it. I think it&rsquo;s called &ldquo;stream&rdquo; because one of the typical use cases is to change the OBS scenes when you are streaming, but in general terms, you can control anything you want. You can think on it as a macro keyboard with customizable buttons, both action and key visualization.
Optimus keyboard the &ldquo;father&rdquo; of Stream Deck? The first thing I thought when I saw the Stream Deck was: &ldquo;This is very similar to something I saw before&rdquo;, and I remembered the Optimus keyboard, a keyboard project born in 2007 with a screen on each key, that allows you to change the key visualization and even the key function. This was a big vaporware and as far as I know not how many units were produced. But you can see the similarities
How Stream Deck works The Stream Deck is a USB device, when you plug it into your computer a software controls the visualization of each key and the action to do when you press it. The official software is available for Windows and Mac, but not for Linux.
I was curious about Stream Deck internals, and how it works. I started to search for information about it, and I found a lot of interesting things.
It depends on the computer: The Stream Deck is just a USB device, it doesn&rsquo;t have any kind of processor or memory, and all the logic is done by the computer. The Stream Deck software is responsible to control the visualization of each key and the action to do when you press it. If you unplug the Stream Deck, the keys will be blank, and if you plug it into another computer, the keys will be blank too. That means no configuration is stored in the device, and the software is responsible to control the device.
Multiple page is an artifact: You can use multiple layouts in the Stream Deck, like button pages, but this is related to the previous, the device does nothing about it, the software just changes the image and the actions when you navigate to another page.
Just one screen: As you can see in the video below, the Stream Deck it&rsquo;s just a big screen with a mask to emulate multiple small screens (and buttons).
Using in linux? The first issue was there is no official support for Linux, I started to search for community and opensource projects related to the Stream Deck and I found Streamdeck UI
Streamdeck UI It&rsquo;s a graphical interface (GUI) application written in Python, that allows you to configure each steam deck button, setting the icon, the label, and the action to do on key press. The action can be a CLI command, key press emulation, for example: emulate some key combination or keystrokes, or even write a text for you.
Tha application is nice, and easy to use.
Streamdeck golang library and Deckmaster Trying to find other applications, and how to interact programmatically with the Stream Deck I found this nice go library: https://github.com/muesli/streamdeck
This library allows to detect the Stream Decks connected to the computer and interact with it programmatically or via the CLi tool it provides.
The library it&rsquo;s very interesting because you can understand how the device works, for example, how it reads keystrokes using the HID library to interact with the device: or how it sends button images to the device
The author of this library also provides a full-featured application to configure the Stream Deck: Deckmaster
This is a CLI application that allows you to configure the Stream Deck using .toml files, something that allows you to easily add version control to the configuration. You can even have different layouts (decks) and navigate between them using the Stream Deck buttons like the original deck pages.
This application also provides &ldquo;special&rdquo; widgets for the buttons, for example, to display the CPU usage, the memory usage, the time, weather, etc.
One of the widgets (this is how it calls to each button) I use a lot is the one can display the output of a command in a Stream Deck button, for example, I have a button that displays the current song playing in the system if it&rsquo;s paused or playing, and the time of the song. Another is to display the cpu temperature, etc.
It&rsquo;s a nice application, and it&rsquo;s released with an open-source license, which means you can modify it and adapt it to your needs.
Finding useful use cases for the Stream Deck Joining deckmaster with the power of linux to execute actions and tasks using the command line I think, now I couldn&rsquo;t live without my Stream Deck. I use it for a lot of things, for example:
Change between the personal Chrome and the work Chrome instances. I can bring to the top of the windows the instance I want with just one button. Play/pause the music: My keyboard doesn&rsquo;t have media keys, so I use the Stream Deck to play/pause the music. Insert emojis: I have multiple decks to insert emojis in my text, It works like direct access to some emojis. Volume control: I have a deck to control the volume and another to mute/unmute the microphone. Audio devices chooser: I usually use headphones and its microphone, but sometimes I want to use the camera mic, and speakers, instead to go to the audio settings and changing both input and output devices, I have 2 buttons that do both changes in one key press. Display some system info: For example, the CPU and memory usage, the time, the uptime, the CPU tempo, etc Following pictures are a couple of my decks:
Conclusion I think the Stream Deck is a nice device, and it&rsquo;s very useful to provide information and do actions that can need multiple clicks or keystrokes. My feeling its I&rsquo;m using it at 20% of its potential.
`,url:"https://sergiocarracedo.es/stream-deck-linux/",image:"/stream-deck-linux/cover_hu_dead19890aee71e2.jpg",tags:["linux","hardware"],readingTime:"6 minutes read",date:"Jul 10, 2023"},"https://sergiocarracedo.es/generative-ai-models-boosting-merchandising-analytics-user-experience/":{title:"Generative AI Models: Boosting Merchandising Analytics User Experience",content:` This article was published originally in DZone. I wrote it in collaboration with Miguel Garc√≠a
In this article, we will explain how using the new Generative AI Models (LLM) can improve the experience of business users on our analytical platform. Let&rsquo;s say we provide our retail merchandising managers with a web application or a mobile application where they can analyze sales and stock behavior in real-time using natural language.
These applications usually have a series of restrictions that mainly show a generic type of analysis, which users can filter or segment based on some filters and provide information such as:
Sales behavior Sell-through Stockouts Stock behavior All these data, with greater or lesser granularity, answer questions that someone has previously determined. The problem is that not all users have the same questions, and sometimes the level of customization is so high that it turns the solution into a big whale. Most of the time, the information is available, but there is no time to include it in the web application.
In the last few years, there have been low code solutions on the market that try to speed up the development of applications precisely to respond as quickly as possible to the needs of this type of user. All these platforms require some technical knowledge. LLM models allow us to interact in natural language with our users and translate their questions into code and calls to the APIs in our platform that will be able to provide valuable information to them in an agile way.
Generative AI Merchandising Platform Use Cases To enhance our merchandising platform, we can include two use cases:
1. Iterative Business Analytical Questions Allow business users to ask iterative questions about the data we have in our data platform with the following capabilities:
Being able to ask questions in natural languageIt can be interactive, but it must also allow the user to save his personalized questions. The answers will be based on the updated data. 2. Story Telling When you provide data to the business user about the sharing of the sale, a fundamental part is storytelling. this enhances comprehension and converts data into valuable information. It would be great if we can give the user the ability to get this explanation in natural language instead of the user having to interpret the metrics.
Practical Example: Designing Chat Merchandising Overview It is a very simple idea to implement, and with a lot of business value for users, we are going to train our LLM model to be able to give a question, to know which data service provides the information. To do this, our architecture must meet three requirements:
All data is exposed via APIs. All data entities are defined and documented. We have a standardized API layer. The following diagram shows the architecture of this high-level solution:
Merchandising AI Web Platform: Web channel based on Vue through users using chat merchandising. Data Service: It provides an API Rest to consume the business data entities available in the data platform. Chat Merchandising Engine: Python backend service that performs the integration between the front and the LLM service; in this case, we are using the Open AI API. Open AI: It provides a Rest API to access Generative AI models. Business Data Domain and Data Repository: A new generation data warehouse, such as Snowflake, modeled on data domains in which business entities are available. In this PoC, we have used the OpenAI service, but you could use any other SaaS or deploy your LLM; another important point is that in this use case, we do not send any business data to the OpenAI service because all the LLM model does is translate the request made by the user in natural language into requests to our data services.
Merchandising AI Web Platform With the LLM and generative UI, the frontend acquires a new relevance, a way user interacts with it, and how the frontend responds to the interactions; now we have a new actor, the Generative AI, that needs to interact with the front end to manage the user request.
The front end needs to provide context to the user messages and be able to show the response in the way the user wants. In this PoC, we will have different types of responses from the model:
An array of data to display in a table:
An array of data to display in a chart:
The front end needs to know how the model or what the user wants to see in the response to act as required; for example, if the user asks for a chart, the front end needs to render a chart; if it asks for a table, the frontend should render a table, if it&rsquo;s just tested, then show the text (and even if there is an error we should show it in a different way).
We type the Chat Merchandise Engine response (in both the back end and front end) in consequence:
export interface TextChatResponse { type: &#39;text&#39; text: string } export interface TableDataChatResponse { type: &#39;table-data&#39; data: TableData } export interface ChartChatResponse { type: &#39;chart&#39; options: EChartOptions } export interface ErrorChatResponse { type: &#39;error&#39; error: string } export type ChatResponse = TextChatResponse | TableDataChatResponse | ErrorChatResponse | ChartChatResponse And this is how we decide which component show.
&lt;div class=&#34;chat-messages&#34;&gt; &lt;template v-for=&#34;(message, index) in messages&#34; :key=&#34;index&#34;&gt; &lt;q-chat-message v-if=&#34;message.type === &#39;text&#39;&#34; :avatar=&#34;message.avatar&#34; :name=&#34;message.name&#34; :sent=&#34;message.sent&#34; :text=&#34;message.text&#34; /&gt; &lt;div class=&#34;chart&#34; v-if=&#34;message.type === &#39;chart&#39;&#34;&gt; &lt;v-chart :option=&#34;message.options&#34; autoresize class=&#34;chart&#34;/&gt; &lt;/div&gt; &lt;div class=&#34;table-wrapper&#34; v-if=&#34;message.type === &#39;table-data&#39;&#34;&gt; &lt;q-table :columns=&#34;getTableCols(message.data)&#34; :rows=&#34;message.data&#34; dense&gt;&lt;/q-table&gt; &lt;/div&gt; &lt;/template&gt; &lt;/div&gt; With this approach, the frontend can receive the messages in a structured way and know how to display the data: as text, as a table, as a chart, or anything you can think of, and also is very useful for the backend as it can get data for side channels.
Regarding charts, you configure all relative to the chart in a JS object (for any type of chart), so in the next iteration of the PoC, you could ask the model for this object, and it can tell us how to render the chart, even the chart type that fits better the data, etc.
Chat Merchandising Engine The logic we have in the engine is very simple: its responsibility is only to act as a gateway between the front end, the Open AI service, and our data services. It is only necessary because the Open AI model is not trained in the context of our services. Our engine is responsible for providing that context. If the model were trained, the little logic we include in this engine would be in the front-end services layer.
We have implemented this service with Python since Open AI provides a library to facilitate integration with its APIs. We are using chat completion API (model gpt-3.5-turbo), but we could use the new feature function-calling (model gpt-3.5-turbo-0613).
# Initial context messages=[ {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: API_description_context}, {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: load_openapi_specification_from_yaml_to_string()}, {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: entities}, ] # Add User Query to messages array messages.append({&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: user_input}) # Call Open AI API response = openai.ChatCompletion.create( model=&#34;gpt-3.5-turbo&#34;, messages=messages, temperature=0 ) # Get messages generated_texts = [ choice.message[&#34;content&#34;].strip() for choice in response[&#34;choices&#34;] ] We have composed the context in a natural language description that includes some examples, the API specification, and the definition of the APIs.
Merchandasing Data Service is an information query API, based on OPEN API 3, this is an example of URL http://{business_domain}.retail.co/data/api/v1/{{entity}}. Following parameters are included in the API: &#34;fields&#34; to specify the attributes of the entity that we want to get; &#34;filter&#34; to specify the conditions that must satisfy the search; For example to answer the question of retrieving the products that are not equal to the JEANS family a value would be products that are not equal to the JEANS family a value would be filter=familyName%%20ne%%20JEANS We parse the response and obtain the generated URL using a regular expression, although we could opt for another strategy using some special quotes.
def find_urls(model_message_response): # Patr√≥n para encontrar URLs url_pattern = re.compile(r&#39;http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+&#39;) urls = re.findall(url_pattern, model_message_response) return urls We also asked the model to add a fragment (for example, #chart) to the URL allowing us to know what the user expects to see in the frontend
This solution is much better than searching for a string in the user input because the user can ask for a chart without using the chart word, which is the model, that ‚Äúunderstand‚Äù the question of who decides to use the chart representation.
Finally, we send this answer back to the front because the call to the data services is made from the front end itself, and this allows us to consume the data services using the user&rsquo;s own JWT tokens.
Conclusions During the last years, many organizations and teams have worked on having an agile architecture, good data governance, and an API strategy that allows them to adapt to changes in an agile way. Generative AI models can provide great business value, and it takes very little effort to start delivering value.
We develop this PoC you can see in the video, in a few hours, using Vue3, Quasar Ui for the basic components and the table, and Echarts to render the charts and Open AI. There is no doubt that algorithms are the new trend and will also be the key to the data-driven strategy; organizations that start from a standardized and agile architecture have a head start in this challenge.
`,url:"https://sergiocarracedo.es/generative-ai-models-boosting-merchandising-analytics-user-experience/",image:"/generative-ai-models-boosting-merchandising-analytics-user-experience/pexels-ksenia-chernaya-3965540_hu_c8b64256bfa63ca.jpg",tags:["frontend","ai"],readingTime:"8 minutes read",date:"Jun 29, 2023"},"https://sergiocarracedo.es/front-end-cache-strategies-you-should-know/":{title:"Front-End: Cache Strategies You Should Know",content:` This article was published originally in DZone. I wrote it with Miguel Garc√≠a
Caches are very useful software components that all engineers must know. It is a transversal component that applies to all the tech areas and architecture layers such as operating systems, data platforms, backend, frontend, and other components. In this article, we are going to describe what is a cache and explain specific use cases focusing on the frontend and client side.
What Is a Cache? A cache can be defined in a basic way as an intermediate memory between the data consumer and the data producer that stores and provides the data that will be accessed many times by the same/different consumers.
It is a transparent layer for the data consumer in terms of user usability except to improve performance. Usually, the reusability of data provided by the data producer is the key to taking advantage of the benefits of a cache. Performance is the other reason to use a cache system such as in-memory databases to provide a high-performance solution with low latency, high throughput, and concurrency.
For example, how many people query the weather on a daily basis and how many times do they repeat the same query? Let&rsquo;s suppose that there are 1,000 people in New York consulting the weather and 50% repeat the same query twice per day. In this scenario, if we can store the first query as close as possible to the user&rsquo;s device, we achieve two benefits increase the user experience because the data is provided faster and reduce the number of queries to the data producer/server side. The output is a better user experience and a solution that will support more concurrent users using the platform.
At a high level, there are two caching strategies that we can apply in a complementary way:
Client/Consumer Side: The data cached is stored on the consumer or user side, usually in the browser&rsquo;s memory when we are talking about web solutions (also called private cache).
Server/Producer Side: The data cached is stored in the components of the data producer architecture.
Caches like any other solution have a series of advantages that we are going to summarize:
Application performance: Provide faster response times because can serve data more quickly.
Reduce load on the server side: When we apply caches to the previous system and reuse a piece of data, we are avoiding queries/requests to the following layer.
Scalability and cost improvement: As data caching gets closer to the consumer, we increase the scalability and performance of the solution at a lower cost.
Components closer to the client side are more scalable and cheaper because three main reasons:
These components are focused on performance and availability but have poor consistency.
They have only part of the information: the data used more by the users.
In the case of the browser&rsquo;s local cache, there is no cost for the data producer.
The big challenges of cache are data consistency and data freshness, which means how the data is synchronized and up-to-date across the organization. Depending on the use case, we will have more or fewer requirements restrictions because it is so different from caching images than the inventory stock or sales behavior.
Client-Side Caches Speaking about the client-side cache, we can have different types of cache that we are going to analyze a little bit in this article:
HTTP Caching: This caching type is an intermediate cache system, as it depends partially on the server. Cache API: This is a browser API(s) that allows us to cache requests in the browser. Custom Local Cache: The front-end app controls the cache storage, expiration, invalidation, and update. HTTP Caching It caches the HTTP requests for any resource (CSS, HTML, images, video, etc.) in the browsers, and it manages all related to storage, expiration, validation, fetch, etc., from the front end. The application‚Äôs point of view is almost transparent as it makes a request in a regular way and the browser does all the ‚Äúmagic.&quot;
The way of controlling the caching is by using HTTP Headers, in the server side, it adds cache-specific headers to the HTTP response, for example: &ldquo;Expires: Tue, 30 Jul 2023 05:30:22 GMT,&rdquo; then the browser knows this resource can be cached, and the next time the client (application) requests the same resource if the request time is before the expiration date the request will not be done, the browser will return the local copy of the resource.
It allows you to set the way the responses are disguised, as the same URL can generate different responses (and their cache should be handled in a different way). For example, in an API endpoint that returns some data (i.e., http://example.com/my-data) we could use the request header Content-type to specify if we want the response in JSON or CSV, etc. Therefore, the cache should be stored with the response depending on the request header(s). For that, the server should set the response header Vary: Accept-Language to let the browser know the cache depends on that value. There are a lot of different headers to control the cache flow and behavior, but it is not the goal of this article to go deep into it. It will probably be addressed in another article.
As we mentioned before, this caching type needs the server to set the resources expiration, validation, etc. So this is not a pure frontend caching method or type, but it‚Äôs one of the simplest ways to cache the resources the front-end application uses, and it is complementary to the other way we will mention down below.
Related to this cache type, as it is an intermediate cache, we can even delegate it in a ‚Äúpiece‚Äù between the client and the server; for example, a CDN, a reverse proxy (for example Varnish), etc.
Cache API It is quite similar to the HTTP caching method, but in this case, we control which requests are stored or extracted from the cache. We have to manage the cache expiration (and it‚Äôs not easy, because those caches were thought to live ‚Äúforever‚Äù). Even if these APIs are available in the windowed contexts are very oriented to their usage in a worker context.
This cache is very oriented to use for offline applications. On the first request, we can get and cache all the resources need it (images, CSS, JS, etc.), allowing the application to work offline. It is very useful in mobile applications, for example with the use of maps for our GPS systems in addition to weather data. This allows us to have all the information for our hiking route even if we have no connection to the server.
One example of how it works in a windowed context:
const url = ‚Äòhttps://catfact.ninja/breeds‚Äô caches.open(&#39;v1&#39;).then((cache) =&gt; { cache.match((url).then((res) =&gt; { if (res) { console.log(&#39;it is in cache&#39;) console.log(res.json()) } else { console.log(&#39;it is NOT in cache&#39;) fetch(url) .then(res =&gt; { cache.put(&#39;test&#39;, res.clone()) }) } }) }) Custom Local Cache In some cases, we will need more control over the cached data and the invalidation (not just expiration). Cache invalidation is more than just checking the max-age of a cache entry.
Imagine the weather app we mentioned above. This app allows the users to update the weather to reflect the real weather in a place. The app needs to do a request per city and transform the temperature values from F to ¬∫C (this is a simple example: calculations can be more expensive in other use cases).
To avoid doing requests to the server (even if it‚Äôs cached), we can do all the requests the first time, put all the data together in a data structure convenient for us, and store it in, for example in the browser‚Äôs IndexedDB, in the LocalStorage, SessionStorage or even in memory (not recommended). The next time we want to show the data, we can get it from the cache, not just the resource data (even the computation we did), saving network and computation time.
We can control the expiration of the caches by adding the issue time next to the API, and we can also control the cache invalidation. Imagine now that the user adds a new cat in its browser. We can just invalidate the cache and do the requests and calculations next time, or go further, updating our local cache with the new data. Or, another user can change the value, and the server will send an event to notify the change to all clients. For example, using WebSockets, our front-end application can hear these events and invalidate the cache or just update the cache.
This kind of cache requires work on our side to check the caches and handle events that can invalidate or update it, etc., but fits very well in a hexagonal architecture where the data is consumed from the API using a port adaptor (repository) that can hear domain events to react to the changes and invalidate or update some caches.
This is not a cache generic solution. We need to think if it fits our use case as it requires work on the front-end application side to invalidate the caches or to emit and handle data change events. In most cases, the HTTP caching is enough.
Conclusion Having a cache solution and good strategy should be a must in any software architecture, but our solution will be incomplete and probably not optimized. Caches are our best friends mostly in high-performance scenarios. It seems that the technical invalidation cache process is the challenge, but the biggest challenge is to understand the business scenarios and uses cases to identify what are the requirements in terms of data freshness and consistency that allow us to design and choose the best strategy.
`,url:"https://sergiocarracedo.es/front-end-cache-strategies-you-should-know/",image:"/front-end-cache-strategies-you-should-know/traffic_flow_but_using_a_tool_to_a_city_with_tron_legacy_style_S1538122565_St25_G7.5_hu_9a2e47f4a49e80f2.jpeg",tags:["frontend","devops"],readingTime:"8 minutes read",date:"May 7, 2023"},"https://sergiocarracedo.es/withdraw-your-library-there-is-another-better/":{title:"Withdraw your library because there is another one better: It's something nice!",content:`In 2019 (almost 4 years ago) I wrote an article about how to handle a custom v-model in Vue 2 (Spanish), in that article I explained how to create a custom v-model in Vue 2, the good practices synchronizing the prop and the internal status and how to avoid the warning message &ldquo;Avoid mutating a prop directly since the value will be overwritten whenever the parent component re-renders. Instead, use a data or computed property based on the prop‚Äôs value.&rdquo;
Summarizing, the strategy was to create a local variable that will be the one that will be modified by the input and that will emit the event to the parent component, and a watcher that will be in charge of modifying the local variable when the prop changes.
A year and a half later when the composition API came to Vue2, and after bing bored with repeating the &ldquo;same&rdquo; code in each component to keep the status in sync, I decided to create a library to simplify this task, that is how vue-use-model-helpers was born.
This library was very useful for me and the teams I worked on, and We used it in several personal and professional projects simplifying a lot of our work.
When Vue 3 was released, I created a new version specific to this version, and I started to think about making the library compatible with Vue 2 and Vue 3 at the same time and improving the TypeScript support. To do that I found vue-demi and in this process, I discovered vue-use and a really amazing library.
This library includes the composable useVModel that does the same that my library does but better.
Even more, reading about the Vue compiler macros I found the macro defineModels that uses vue-use under the hood and allows you to create a custom v-model in a very simple way.
So,
I decided to deprecate my library and strongly recommend using vue-use instead.
What about the projects that are using vue-use-model-helpers? Nothing, they can still use it as before, but the migration to useVModel is quite simple, just replace the import and replace the useModel function with useVModel adding the options { passive: true } and you will get the same behavior.
Should I think that is a defeat? I don&rsquo;t think so. I think is a win for me: I learned a lot in the process of creating the library, we were using a lot and it was very helpful, we could use this behavior almost a year before useVModel were released.
And internally I feel quite nice that the community and widely adopted solution to this problem be very similar to the solution I designed and created. I feel proud of that.
But as I mentioned, makes no sense to keep maintaining a library that is not necessary anymore, now it is time to move on and try (if it&rsquo;s possible) to contribute to vue-use (not only about useVModel), I love Vue composition API and composable, I feel them very useful and powerful.
`,url:"https://sergiocarracedo.es/withdraw-your-library-there-is-another-better/",image:"/withdraw-your-library-there-is-another-better/unsplash_a2g3LM0cGFg_hu_144b2432941346c4.jpg",tags:["oss","vue"],readingTime:"3 minutes read",date:"Apr 30, 2023"},"https://sergiocarracedo.es/css-variables-scoping/":{title:"CSS variables scoping to create and theme flexible ui components",content:`The real name of the CSS variables is CSS Custom Properties is a draft standard (yes when I wrote these lines is on Candidate Recommendation Snapshot), but is widely supported by modern browsers.
CSS variables allow us, like another kind of variable in another programming language to store a value we can reuse across our document.
For example, if we define a CSS variable for the primary color doing: --primary-color: #f00;, then we can use it in any component like:
.my-component { color: var(--primary-color); } Usually you &ldquo;attach&rdquo; your variable to :root, which means the variable will be available in all the document
:root { color: var(--primary-color); } In this example :root is the variable scope.
Using together SCSS If you want to assign values from SCSS variables to CSS variables, you can not do the &ldquo;normal&rdquo; notation:
// ‚ùå This doesn&#39;t work $scss-var: #f00; --my-var: $scss-var; In the example, the value of --my-var is literally $scss-var, not the value of $scss-var, this behavior was done to provide the maximum compatibility with the plain CSS https://sass-lang.com/documentation/breaking-changes/css-vars
To make it work you need to use the Sass interpolation syntax: #{my scss script code}:
// ‚úÖ This works $scss-var: #f00; --my-var: #{$scss-var}; Scope The variables are only available in the element where is defined and its children, that is the scope of the variable. Outside there the variable doesn&rsquo;t exist.
If you try to access to use a variable that is not in the scope you will not get an error, but the property that is using the not existing variable will be ignored.
Hoisting Like the JS variables, the CSS variables are moved to the top, so you can use them before defining.
.my-element { color: var(--primary-color); } :root { --primary-color: #f00; } Override As I mentioned before the variables has a scope where the variable exists, but: what happens if a variable with the same name is defined in 2 scopes: It happens the same as in a JS variable, the near local scope overrides other values:
:root { --color: #0f0; } .my-element { --color: #0ff; color: var(--color); } This behavior is very convenient when we work with UI components which have different styles depending on modifiers.
CSS Variables in UI components Imagine we have a simple button component, like that
&lt;button class=&#34;ui-button&#34;&gt; Button content &lt;/button&gt; .ui-button { background: #333; color: #fff; font-size: 12px; padding: 4px 10px; } This button has different variants by color (default, red and green) and size (default, small and big), using BEM, we can add a modifier class like .ui-button--green or .ui-button--big and use that to overwrite the styles, for example:
.ui-button { background: #333; color: #fff; font-size: 12px; padding: 4px 10px; &amp;--green { background: #1F715F; } &amp;--big { font-size: 16px; padding: 6px 20px; } } This way works perfectly, but we need to know which properties to overwrite, and need to do it explicitly for each modifier, so it&rsquo;s easy to forget something, or if we need to add a new property affected by the modifiers, add it in all of them
If we rewrite the styles using CSS variables, parameterizing the component styles, we can override the CSS variable values for each modifier, without changing the CSS styles itself for the modifiers, only changing the value of the variables:
.ui-button { --bg-color: #333; --text-color: #fff; --font-size: 12px; --padding: 4px 10px; background: var(--bg-color); color: var(--text-color); font-size: var(--font-size); padding: var(--padding); &amp;--green { --bg-color: #1F715F; } &amp;--red { --bg-color: #0ff; } &amp;--big { --font-size: 16px; --padding: 6px 20px; } &amp;--small { --font-size: 10px; --padding: 3px 5px; } } You can see a working example at: https://codesandbox.io/s/autumn-bush-4i4iem?file=/index.html
Variable scope priority In CSS the elements can use more than a class, so that means the element&rsquo;s CSS variables have multiple scopes at the same level, for example if we apply the green and red modifiers at the same time
&lt;button class=&#34;ui-button ui-button--green ui-button--red&#34;&gt; Green + red &lt;/button&gt; Both ui-button--green and ui-button--red define the same --bg-color variable, What value will be applied to the element?
In cases like that the class order is the priority, so the last class applied overrides the value last and its value is applied, on the example the button will be red, but for &lt;button class=&quot;ui-button ui-button--red ui-button--green&quot;&gt; the button will be green
Summarizing The use of CSS variables and scopes is a powerful tool when you are developing components in general, but more if your components have modifiers, it requires an extra work in the beginning to parameterize the component, but after that makes it simpler to create variants and modifiers.
`,url:"https://sergiocarracedo.es/css-variables-scoping/",image:"/css-variables-scoping/pexels-pixabay-50996_hu_4cffebf2ee6c3c5e.jpg",tags:["css","ui"],readingTime:"4 minutes read",date:"Mar 5, 2023"},"https://sergiocarracedo.es/typescript-enums/":{title:"Typescript enums, const enums and readonly maps",content:`Enum basics Enums is one of the nice things Typescript bring to the Javascript development&rsquo;s environment. Enums allows you to define a set of named values (constants), generally with a semantic meaning.
One of the advantages over the regular constants is the grouping, making easy to know the different values you can use in certain place (and limiting the possible values to use)
In Typescript an enum has this shape:
enum HttpResponseStatus { NotFound, Forbidden, Ok, InternalServerError } In the example, the enum represent a list of possible (a simplified list) HTTP response status, and using the names it&rsquo;s easy to remember which status we want to use in each case.
The way to use a enum, it&rsquo;s very straightforward, just: Enum name + dot + Enum value name: HttpResponseStatus.NotFound, Typescript will replace it by a value
By default, and if you don&rsquo;t specify more, Typescript converts each the enum value to a number, starting on 0. In or example NotFound value is 0, Forbidden is 1, etc.
This doesn&rsquo;t fit the expected use case, we expect, for example NotFound&rsquo;s value be 404, Ok be 200, etc.
You can even define the starting number
enum Chapters { Four = 4, Five, Six, Seven } But, to meet the expected values we only need to assign the desired values for each enum&rsquo;s value name like:
enum HttpResponseStatus { NotFound = 404, Forbidden = 403, Ok = 200, InternalServerError = 500 } You can use expressions to define the values, for example calculations, bit operations, etc. Event random numbers (the random values is evaluated just one time, so it will be constant in the runtime), or values returned by a function
enum MyEnum { A = 404, B = 1 &lt;&lt; 2, C = 1 * 3, D = Math.random(), E = someFunction(123) } After defining an enum you can use it as a type, with limitations*, for example:
function handleResponse(responseCode: HttpResponseStatus) (*) The main limitation is you can assign any number to a numeric-enum type and that is intended (https://github.com/Microsoft/TypeScript/issues/26362#issuecomment-412198938).
For example handleResponse(123) is valid, even if the value 123 is not a value in the enum HttpResponseStatus. This doesn&rsquo;t happen with string-enums
Now you know the enum basics, let go deeper
Enums in runtime Typescript&rsquo;s enums have a representation in runtime, but maybe is not as you can expect, let&rsquo;s see how the HttpResponseStatus enum is compiled to vanilla JS:
var HttpResponseStatus; (function (HttpResponseStatus) { HttpResponseStatus[HttpResponseStatus[&#34;NotFound&#34;] = 404] = &#34;NotFound&#34;; HttpResponseStatus[HttpResponseStatus[&#34;Forbidden&#34;] = 403] = &#34;Forbidden&#34;; HttpResponseStatus[HttpResponseStatus[&#34;Ok&#34;] = 200] = &#34;Ok&#34;; HttpResponseStatus[HttpResponseStatus[&#34;InternalServerError&#34;] = 500] = &#34;InternalServerError&#34;; })(HttpResponseStatus || (HttpResponseStatus = {})); and if you do console.log(HttpResponseStatus) this is the result:
{ &#34;200&#34;: &#34;Ok&#34;, &#34;403&#34;: &#34;Forbidden&#34;, &#34;404&#34;: &#34;NotFound&#34;, &#34;500&#34;: &#34;InternalServerError&#34;, &#34;NotFound&#34;: 404, &#34;Forbidden&#34;: 403, &#34;Ok&#34;: 200, &#34;InternalServerError&#34;: 500, &#34;Test&#34;: 0.48543608526338566, &#34;0.48543608526338566&#34;: &#34;Test&#34; } Even the RxJS core team lead wrote a tweet about that: https://twitter.com/benlesh/status/1510983348944056327
The reason of this behavior is mainly because:
Reverse Mapping this allows to use the enum in both directions, get the value from the value&rsquo;s name or get the name from the value&rsquo;s name from the value. Computed values: The object that represents the enum is computed in a function to allow computed values, like the random we used before in the example. Reverse mapping can be useful in some cases, for example if you want to show the value&rsquo;s name instead of the value, even in a dropdown, to list all values&rsquo; names and get the selected value, but in this case you need to do an extra conversion to avoid repeated values: https://gist.github.com/sergiocarracedo/ac219a9b3f700b2e721cc9c2964b36c9
Const enums In most use cases you don&rsquo;t need Reverse mapping neither computed values, then you can use const enums, just adding the keyword const before the enum definition.
const enum HttpResponseStatus { NotFound = 404, Forbidden = 403, Ok = 200, InternalServerError = 500 } In this case Typescript&rsquo;s compiler just will replace the uses of the enum items by the value, for example:
someFunction(HttpResponseStatus.NotFound) // Compiler output someFunction(404 /* NotFound */) Union types Other simple way to &ldquo;emulate&rdquo; enum&rsquo;s behaviour keeping type safety and without overload the bundle with extra code is just using union types
type HttpResponseStatus = 404 | 403 | 200 | 500 This solution lost the spirit of an enum, but the IDE can to the &ldquo;magic&rdquo; suggestion the available values when you try to fill a function argument typed as HttpResponseStatus
Both solutions are nice in terms of the bundle size, but removes the possibility of knowing the value&rsquo;s name.
Const object In the case we want to have the value&rsquo;s name in runtime we can use a plain object to emulate the enum behavior, then our enum becomes:
const HttpResponseStatus = { NotFound: 404, Forbidden: 403, Ok: 200, InternalServerError: 500 } We can use it as an enum, referencing a value in the same way HttpResponseStatus.NotFound, but what about the typing?
If we check the type of the object we get this:
const HttpResponseStatus: { NotFound: number; Forbidden: number; Ok: number; InternalServerError: number; } We lost the possibility of use the type, for example in a function&rsquo;s argument: function someFunction(status: HttpResponseStatus) will not work, and we should use number as status&rsquo; type.
Const assertion The Typescript&rsquo;s const assertion solves this as forces the object&rsquo;s properties to be readonly, and for that Typescript&rsquo;s compiler is able to infer the property types as and convert it as type:
const HttpResponseStatus = { NotFound: 404, Forbidden: 403, Ok: 200, InternalServerError: 500 } as const // The type infered by typescript const HttpResponseStatus: { readonly NotFound: 404; readonly Forbidden: 403; readonly Ok: 200; readonly InternalServerError: 500; } This is an object and we still having the keys in runtime as in a regular enum
Now we can create a type that will content the union of all properties values as type
type HttpResponseStatusEnum = typeof HttpResponseStatus[keyof typeof HttpResponseStatus] Explaining it a bit in detail:
typeof HttpResponseStatus is the type of the object: keyof typeof HttpResponseStatus returns all the keys of the object: &quot;NotFound&quot; | &quot;Forbidden&quot; | &quot;Ok&quot; | &quot;InternalServerError&quot; and finally we use again typeof HttpResponseStatus to get the object type, and we access to the type by index, but as we apply that over an union type, we are applying the index to each element in the union https://www.typescriptlang.org/docs/handbook/2/conditional-types.html#distributive-conditional-types and then we can use this &ldquo;Enum type&rdquo; as type:
function someFunction(status: HttpResponseStatusEnum) Summary As you read, there several ways to achieve the same or similar behavior, and now you have information enough to decide which solution use depending on the use case, if you are worried about the bundle size the code output maybe it&rsquo;s the moment to start to use another solution, Typescript documentation recommends the object with as const solution, but if you don&rsquo;t need the enum values&rsquo; names you can use the const enum or just the union type.
In modern TypeScript, you may not need an enum when an object with as const could suffice (from https://www.typescriptlang.org/docs/handbook/enums.html#objects-vs-enums)
`,url:"https://sergiocarracedo.es/typescript-enums/",image:"/typescript-enums/delila-ziebart-b0GSCFJ-Gzg-unsplash_hu_86a5b567ca78d770.jpg",tags:["typescript"],readingTime:"6 minutes read",date:"Jan 29, 2023"},"https://sergiocarracedo.es/2022-in-a%20nutshell/":{title:"2022 in a Nutshell",content:`2022 is over, for me, it was a very intense year, with a lot of challenges, changes, laughs and memorable moments. Let&rsquo;s do a small summary.
Professional This was a year of change, I joined to a new company a month before the end of the year, after 2 years working in a exciting project with very smart and nice people, and after some time of resting and thinking about the future and my career.
It&rsquo;s too early to summarize the changes, but I guess 2023 will be an exciting and challenging year professionally speaking.
This year also I was able to achieve some goals I&rsquo;m proud of, like complete the refactor of a project&rsquo;s frontend to hexagonal being more decoupled of the backend and making the frontend&rsquo;s code more maintainable and simple, it was a big effort but it was worth it.
Community 2022 was the year of the community&rsquo;s reborn, during the pandemic all the community activities were stopped, in 2022 most of the in person meeting&rsquo;s restrictions were removed, and we start to schedule new events.
I left the organization of PHPVigo as I have not programmed in PHP from a long time. I believe that, in general, programming languages are not very important, but in the case of a programing group, makes sense that the organizers know the language, use it , know the updates, the news, etc. and it&rsquo;s time to step aside and give the opportunity to other community&rsquo;s members of give a step forward.
I also started to support the group VigoJUG, and I know it, it&rsquo;s a language I not familiar with, but my role is supporting the events and the organizers (as I can do with any other group).
But the most remarkable community event I participated as co-organizer it was the #pulpoCon22 (https://2022.pulpocon.es/), a three-day event with workshops, side events, talks, and a lot of networking. You can read a nice summary written by Rolando Caldas, other co-organizer of the event.
Post and Talks The year&rsquo;s beginning I was very busy and I didn&rsquo;t have time to write as posts as I would like, only 6 posts, and the same happened to the talks. But fortunately, in the last quarter I was able to change that trend and I could prepare 2 talks and do them:
one about the Composition API in Vue that I did privately for the frontend team of a company the other is a talks about UI Components. This talk was hosted by Nextail and publicly streamed live on Twitch. You can watch it on YouTube &#x1f604; I also started to publish posts on DZone, and just when the year ended they pushed one of my articles to the front page.
Open source I continue use the project I created 2 years ago (https://github.com/sergiocarracedo/vue-use-model-helpers) and thinking in the ways to improve it.
Personal / Learnings This year I learned that is very important to rest enough and take care of myself to be productive. You can strive, push a lot, but you need to make it sustainable in time if not it&rsquo;s not profit for anyone. Anyway, the balance of the year was very positive, I made a strong group of friends and teammates, and this is the foundation for growing professionally and personally.
&#x1f389; Happy 2023!! &#x1f389;
`,url:"https://sergiocarracedo.es/2022-in-a%20nutshell/",image:"/2022-in-a%20nutshell/moritz-knoringer-4_MwbIq0CME-unsplash_hu_9a8475a2f92cf169.jpg",tags:["lifestyle"],readingTime:"3 minutes read",date:"Dec 31, 2022"},"https://sergiocarracedo.es/tech-roles-in-the-frontend/":{title:"Frontend team roles and skills, breaking the borders",content:`15-20 years ago, web application developers had the knowledge and the technical skills necessary to create an application: HTML, CSS, JS, PHP/Python/ASP, Web Server management, MySQL/Postgres (both data and service management), etc.
With time the web applications became more complex with bigger requirements. New technologies, frameworks, and paradigms bounce into the development and pushed developers to specialize in some areas arising new roles from this specialization.
Nowadays, the equivalent typical roles for a web application that cover the exposed tasks/skills and the typical requirements are: Designer, Frontend developer, Backend developer, Platform engineer, and, in a data-oriented company: Data engineer, Data scientist, etc.
I want to focus this post on the first three roles: Designer, Frontend developer, and Backend developer
You can think I made a mistake because the title of the post says tech roles, and I added the designer&rsquo;s role. Well, for me, it&rsquo;s a technical role, maybe it&rsquo;s diffuse, but they are the nearest role to the application&rsquo;s user, and they are a kind of bridge between the users (and product) and the frontend team(s).
Let me represent the roles or the teams by the distance to the application user (This representation can vary depending on the application, but it covers a lot of cases).
I&rsquo;m going to do some simplifications to explain this:
design team takes care of the first thing the user sees on an application: the visuals, frontend team is in charge of implementing those visuals and the user interaction, backend manages the user interactions with the data and implements the business logic, devops (platform) creates the context (servers, databases, networking) for everything to work. Frontend role interfaces and borders To define the frontend role (or team) responsibilities, let&rsquo;s start defining the borders: I&rsquo;m going to do it on the right side of the image:
The natural interface between the frontend and the backend is a contract: an API. It could be a REST HTTP API, gRPC API, GraphQL API, or even an internal controller of the application, etc. This border seems a hard border, but we&rsquo;ll see that it&rsquo;s not as hard as you might think.
After defining the right border, let&rsquo;s do the same with the left border, the one with the design. This is typically a static design of the elements, layouts, and screens of the application, a Figma document, or something similar.
This is a simplification, I&rsquo;m aware there are other tools, ways to define the visual and functional requirements, etc.
Frontend team responsibilities Knowing the borders, let&rsquo;s define the high-level responsibilities in the frontend team:
Convert design to HTML/CSS App layouts Create and maintain the app&rsquo;s components Add and maintain animations to the components and transitions Handle use interaction Load and send data to API Manage and show backend error messages Form/entities validation Implement and maintain business logic Implement and maintain UI logic These responsibilities (and more) are quite different and require different skills to manage them. As I mentioned before, in the past, was very common for a single developer to get all these and solve the related tasks without thinking about this separation, the same happened long before with the backend and frontend separation, which didn&rsquo;t exist and now is very common to have Frontend and backend developers.
Nowadays, we can have more specialized roles for each responsibility in the frontend. We can group again into 2 subgroups: Frontend visuals and interactions and Frontend internals. I usually call this &ldquo;the backend of the frontend&rdquo;
The frontend visuals and interactions takes the responsibility of converting the designs to a code the browser can understand (HTML and CSS), implements the animations, the layouts, the user interaction (ex. emit an event on user click), renders the data. All typically into a component of a framework like Vue, React, etc. This group of responsibilities requires more knowledge in HTML, CSS (SCSS), SVG, browser events.
The Frontend internals takes the responsibilities nearer to the backend: connect to the backend get the data, prepare it for a view, validate the forms, implement the business requirements related to the data flow, constraints, etc. This group requires more knowledge in Typescript, browser&rsquo;s API, HTTP, async
It&rsquo;s important to say that the borders are not hard, and that is good as we will see as permits mobility, spread, and improvement.
Both groups require to understand of the business domain, but in different ways, visuals requires a better understanding of how the user will interact with the domain and how the domain reacts to those interactions, and internals requires a better understanding of data flows, domain, events, etc. Again, we should understand that the borders are diffuse.
We could go even further and create more specialized groups inside the frontend visuals and inside frontend internals groups, but the logic behind this separation is the same as I exposed above.
Roles segmentation, granularity, and overlapping Going back to the simple scenario, we had a single role: frontend developer, but now, considering the previous grouping, we can define different roles in our frontend team, roles that cover the skills and responsibilities of multiple subgroups of responsibilities.
This role segmentation makes it easy to find people that fit better on the team. As wider is the role as harder to find a person to cover the role with all the skills necessary for the position, and it&rsquo;s not about reducing the position&rsquo;s requirements, it&rsquo;s that the same position (ex: Frontend developer), today usually requires more knowledge than 5 years ago.
For example, if the company needs a specialist in css/svg animation, I think makes no sense to require knowledge in GraphQL for this position, if the person knows it, then perfect, but if not, she/he can learn it.
It&rsquo;s important that she/he fits in the position at the beginning and starts to share knowledge about animation, she/he can expand her/his knowledge to other responsibility areas later, making the onboarding process and the adaptation process smoother.
Just another example, usually (not a general rule) the people that came from a frontend bootcamp have more skills and feels more comfortable in the visual roles than in the roles nearer to the backend. Should we discard that talent just because it doesn&rsquo;t cover all the frontend skills? I don&rsquo;t think so, as they get more experience, they will spread the knowledge areas and get tasks from other roles.
Mobility between roles An advantage of not having hard borders between these roles makes easy mobility. The developers can start to be involved in new roles, increasing their knowledge organically, for example, a visual&rsquo;s role developer could need to change a use case, solve a bug fix, or implement a small feature, but still be in its comfort bubble, just increasing it a bit at a time and after some time can start to do more and more tasks on the internals and the opposite, a developer in the internals group can do a task in the visuals.
Another advantage is that the areas of overlapping make it easy to understand and empathize with others and others&rsquo; tasks, and understanding what &ldquo;I could do to make the other&rsquo;s work easy&rdquo;.
Overlapping roles In this situation of narrow and diffuse role definitions, to have developer skills that overlap with another one&rsquo;s skills it&rsquo;s not an issue it&rsquo;s an advantage, they can split the tasks, work together and share knowledge in the areas they don&rsquo;t overlap.
Breaking the borders But, we still have hard borders between design, frontend and backend, why don&rsquo;t break them?
All I mentioned before about mobility, overlapping, and specialization apply to the borders with the design and the backend, obviously we can do this deep looking and segmentation in the roles for design and the backend.
Who does not know a designer that learned HTML, CSS, and JS, maybe just curiosity, to make her/his design real, or looking for a better job opportunity, why ignore these design skills? Let she/he collaborate with the design team or just help them to understand the technical possibilities of the frontend.
&ldquo;Crossing&rdquo; the right border is more common, we call them full-stack, but following the previous, there are different types of full stack developers, depending on where they put their knowledge areas limits on left and right. In my opinion let frontend developers, the ones whose roles or knowledge areas are more on right, &ldquo;cross the border&rdquo; and participate in the backend&rsquo;s tasks, at least in the ones more related to the frontend it&rsquo;s always productive and increment the communication and collaboration between teams, this developer don&rsquo;t need to know all the details and internals, of the backend, for example, if the backend uses hexagonal architecture, this person could implement or maintain controllers and application uses cases, using already implemented services and entities.
And the opposite, a backend developer could implement the frontend repository that reads from the API and converts the data into domain entities without needing to know how the frontend framework works in deep.
Getting strengths of the team members This segmentation a wrote about is not about segmenting or dividing the teams to create more or about to create more structs in the organization, it&rsquo;s about identifying the strengths of the team members and taking advantage of that, empowering the work empathy with other teams and team members through a better understanding of other areas of knowledge and their needs and at the same time minimizing the knowledge silos via overlapping these areas.
I believe that in the upcoming years the knowledge necessary to create a big web application will be bigger and probably we will see some kind of split between the frontend visuals team and frontend pure development, but I strongly believe, this is an arbitrary division, and people with specialization but that can get into other roles&rsquo; task will be very useful and productive for any organization.
`,url:"https://sergiocarracedo.es/tech-roles-in-the-frontend/",image:"/tech-roles-in-the-frontend/pexels-pixabay-207896_hu_9402d70a50fcd3d7.jpg",tags:["frontend"],readingTime:"8 minutes read",date:"Dec 28, 2022"},"https://sergiocarracedo.es/typescript-typing-challenges-and-type-utilities/":{title:"Typescript typing challenges and type utilities",content:`Recently I was looking for more knowledge about Typescript typing, how to create more advanced and better types, and I can say that it&rsquo;s a big deal, there are a lot of things you can do and, you never imagined before.
During that &ldquo;investigation&rdquo; I found very interesting resources I want to share with you.
Type Challenge https://tsch.js.org/
It&rsquo;s a repository that includes a lot of, very well-documented, and designed typing challenges. Each challenge provides you a definition of it in a README.md file, a test-cases.ts file with the tests the type you should define should pass, and finally the file template.ts where you should do your job creating the type necessary to meet the requirements and to pass all the tests.
I recommend you to start with the easy ones, without pressure, try to solve them using the Typescript&rsquo;s documentation (which is very good), and if finally, you can&rsquo;t solve it, or you don&rsquo;t want to spend too much time on an exercise check the community solutions, but then spend all the time you need to understand the solution and the background concepts, I&rsquo;m pretty sure you will learn along the way.
For me, it was a lesson of humility, before this challenge I thought I knew Typescript typing, but after that, I know still need to learn about Typescript &#x1f605;
&#x27a1;&#xfe0f; The user Eugene Obrezkov is doing this challenge and documenting the solutions he did, explaining them. Very recommended
Example Implement the built-in Readonly&lt;T&gt; generic without using it. From: https://github.com/type-challenges/type-challenges/blob/main/questions/00007-easy-readonly/README.md
My solution to this challenge, &#x26a0;&#xfe0f; SPOILER ALERT &#x26a0;&#xfe0f; is
type MyReadonly&lt;T&gt; = { readonly [K in keyof T]: T[K] } The explanation is that we are creating the type MyReadonly that receives a generic type T and this type is an object whose keys are if K (not of type K, the K value) which is a value that is one of the keys of the generic type T, for this key we are setting is read-only, and for the type of that key, we are getting the type of the key (K) in the generic type.
This simple solution requires knowledge about keyof type operator, mapped types, mapping modifiers, etc&hellip; So it&rsquo;s quite useful to improve your typing skills using real challenges.
Hardcode typing The previous example is simple, it&rsquo;s more complex than the typical types you can use in the day-to-day but still simple. When you continue advancing in the exercises they became more and more complex, and they will require all your knowledge about how the types work in Typescript, that is what I call hardcore typing, squeezing the type system at the maximum to get the results you want. It&rsquo;s worth it to try this challenges get a strong typing knowledge and how to apply them in the real world.
Type utilities If you check the challenges&rsquo; code, especially the tests, you will find lines like:
type cases = [Expect&lt;Equal&lt;MyReadonly&lt;Todo1&gt;, Readonly&lt;Todo1&gt;&gt;&gt;] This is one of the nice things about the Type Challenge, is the type utilities they use, for example Expect&lt;T&gt; and Equal&lt;X, Y&gt;
Expect is simple, only check the type is true (technically speaking if extends true,). but help us to do other validation of type. Equal checks if two types are the same type, the type definition is not as simple as you can expect
export type Equal&lt;X, Y&gt; = (&lt;T&gt;() =&gt; T extends X ? 1 : 2) extends (&lt;T&gt;() =&gt; T extends Y ? 1 : 2) ? true : false I could try to explain the internals, but this answer in stackoverflow does it much better than I could.
These kinds of types are useful to complement your application types, this is the reason why there are libraries that provide them, also Type Challenge released its own types utilities as a package https://www.npmjs.com/package/@type-challenges/utils
Let&rsquo;s see a couple of them
TS Toolbelt https://github.com/millsp/ts-toolbelt
It&rsquo;s a collection of more than 200+ type utilities, they describe themselves as the lodash of the type system. It abstracts the complex type checks.
Utility types https://github.com/piotrwitek/utility-types
Another type&rsquo;s library that describes itself again as the lodash of types &#x1f604;. it&rsquo;s not big as TS Toolbelt but includes commonly used types. For example DeepPartial works like the native Partial but does it recursively.
This library provides also &ldquo;real&rdquo; type guards, I mean functions that do type narrowing and validates a variable on runtime.
Other type-challenge To finalize I want to share one more type challenge just in case you want to challenge your typescript typing skills: https://js.checkio.org/
`,url:"https://sergiocarracedo.es/typescript-typing-challenges-and-type-utilities/",image:"/typescript-typing-challenges-and-type-utilities/pexels-david-buchi-1070345_hu_2584d04af99fe522.jpg",tags:["typescript","js"],readingTime:"4 minutes read",date:"Dec 12, 2022"},"https://sergiocarracedo.es/using-charts-frontend-echarts-with-examples/":{title:"Using charts in the frontend: Echarts with examples",content:`If you are working in frontend and in data visualization one of the tasks you need to solve is to represent data in different ways (charts) and there are multiple ways of resolving it:
Do it from the scratch, with js and css, svg, canvas, etc Using a low-level library that abstracts for you things like dom manipulation, scales, etc, d3.js is a very good one: I wrote a blog post about it Using chart libraries Let&rsquo;s see some pros and cons of the different approaches there is no perfect solution, depends on your needs.
From the scratch d3.js Charts library Customization effort Low Mid Depends on the library (High) Team experience required Higher High Low Experience required Higher High Low Out-of-the-box features None Axis, data transformations, draw svg/canvas management Full charts Time to production High High Low The charts&rsquo; library way Using a charts library to generate data visualizations could look like a simple way to solve the task, but is not, it depends on your requirements and how much you need to customize the chart if your requirements in visual style and behavior are simple any library can work, but the thing can get complicated when you need to customize them.
Throughout my career, I explored all those ways and a lot of different charts libraries: amCharts, chart.js, highcharts.js, etc&hellip; and to meet the requirements we had I started to create and use custom chart components based on d3.js, but I discovered ECharts
ECharts ECharts is an opensource Javascript (and Typescript) visualization library written in pure javascript and based on zrender incubated under the Apache Software Foundation (ASF) and created originally by Baidu (that is the reason why you will found a lot of Chinese entries looking for ECharts, but you shouldn&rsquo;t worry about it, there is a lot of documentation in English too).
This library allows you to create a lot of different and customizable charts (series types, as we will see later), you only need to visit the examples page to see it, from a simple line chart to a 3D globe with flight lines, including bars, donut, boxplot, candlestick, map, scatter, heatmap, tree, treemap, sunburts, parallel, sankey, funnel, gauge, themeriver, calendar, and completely custom charts.
Probably you find the chart you need to meet your requirements with the available charts out-of-the-box, but in any case, you can go deep into the documentation and start to customize your chart visualizations.
What makes ECharts different Working with other libraries you can customize things but far as the customization level of ECharts. You have literally thousands of params to customize it, anything you want to configure exists in the config.
This configuration is a plain js object and includes all, the chart data, the chart definition, the chart visual configuration, and the chart behavior configuration.
My first thought was having a JS API it&rsquo;s better to customize the charts, but believe me, just using this big object you can do customization and fine-tuning.
Another fully customizable thing is the theme, you can use pre-build themes or create a new one customizing every aspect of the charts. For example, you can customize the default series&rsquo;s colors for all charts without the need to set them in every chart
As I mentioned before, all the configs are js&rsquo; plain objects, so you can store this data as JSON and then import as an Object and use Object.assign to set default shallow values, or lodash defaults (_.defaults) \` for example to set the defaults for the tooltip
const echartsDefaults = { tooltip: { show: true, trigger: &#39;axis&#39;, position: [&#39;50%&#39;, &#39;50%&#39;] }, xAxis: { itemStyle: { color: &#39;#f00&#39; } } } ... const chart = echarts.init(document.getElementById(&#34;app&#34;)); chart.setOption(_.defaults( { tooltip: { trigger: &#39;item&#39; }, xAxis: { type: &#34;category&#34;, data: [&#34;Mon&#34;, &#34;Tue&#34;, &#34;Wed&#34;, &#34;Thu&#34;, &#34;Fri&#34;, &#34;Sat&#34;, &#34;Sun&#34;] } }, echartsDefaults)) // The actual options // { // tooltip: { show: true, trigger: &#39;item&#39;, position: [&#39;50%&#39;, &#39;50%&#39;] }, // xAxis: { itemStyle: { color: &#39;#f00&#39; }, type: &#34;category&#34;, data: [&#34;Mon&#34;, &#34;Tue&#34;, &#34;Wed&#34;, &#34;Thu&#34;, &#34;Fri&#34;, &#34;Sat&#34;, &#34;Sun&#34;] } // } Canvas vs SVG One of the most common questions, when you start to develop a chart solution is if use SVG or Canvas.
SVG defines the visual representation as a document, exactly the same as HTML defines a page as a document, the events work similarly as in HTML, and the visualization of the elements can be modified using simple CSS. SVG also it&rsquo;s better in terms of accessibility. It&rsquo;s a vectorial representation, so the charts will look nice in any resolution, and in any SVG object change the browser will take care of the redraw.
Canvas it&rsquo;s a pixel-oriented (bitmap) way to represent drawings (SVG can also include bitmap), basically after drawing a pixel the browser stop taking care of the object you drew and only stores the pixels. This makes you need to care of the redraw if something changes.
Canvas gives better performance on small surfaces or large numbers of objects to draw and SVG is better for large surfaces and small numbers of objects.
ECharts uses ZRender a library that abstracts the 2D draw exposing the same API to render canvas or SVG. This allows you to decide which renderer to use but without do extra changes in your chart, it&rsquo;s just a flag.
As a general rule canvas is recommended for large datasets (&gt;1000 items), and SVG in low-end Android or specific charts. More info at https://apache.github.io/echarts-handbook/en/best-practices/canvas-vs-svg/
This feature makes ECharts very different from other chart solution as they use SVG or Canvas, but you can&rsquo;t decide (d3.js also allows you to decide the renderer) the render, you should do the render in the system the charts library uses.
Basic example Let&rsquo;s do a simple example, a bar chart You can compare how to create a similar chart in d3.js here
The amount of code is higher even for the basic case, and the d3.js version doesn&rsquo;t include the tooltip and the gradients
Creating a chart is simple (check the code) as fill a js plain object, maybe you would need time to find the property you need to use, but basically, we define the xAxis, yAxis, the series (the values), the series type, and the style of those elements in the same object.
Multiple series example The representation type (chart) type is defined by the series, so we can mix different visualization types in the same chart. Let&rsquo;s see an example.
Series could share the axis, but in the example, we added a new axis with a different scale and units.
Animations example ECharts gives you the possibility of transforming a series type into another using animation, it&rsquo;s easy as changing the Echart&rsquo;s options object and ECharts will do the animation for you
Custom series Besides the series types I mentioned before (there are a lot), you can use the custom series type and using a function you can define programmatically how each render should be rendered. This feature gives you more control over the representation.
And even, if you really need it you can write a plugin type to create a new series type for a custom data representation, for example, a word cloud viualization: https://github.com/ecomfe/echarts-wordcloud
But in most cases you don&rsquo;t need to go into deep and create a custom series type or use a custom one, you can use multiple series to achieve the result you want. For example, this line chart with confidence band uses a line type and area type stacking the series
Using other features like visualMap you can set different colors on different series parts.
Vue, React, Angular You can use ECharts with your favorite js framework, there are wrappers available, and they take care of the data responsibility, and of resizing the chart on window width change.
Vue: https://vue-echarts.dev/ React: https://git.hust.cc/echarts-for-react/ Angular: https://xieziyu.github.io/ngx-echarts Summary ECharts is an amazing charts library that allows you to customize it in a lot of different ways, and that probably solves your requirements without writing code, just using customization options.
There are a lot of features I didn&rsquo;t mention but are very interesting, I&rsquo;ll do it in another post.
I invite you to try it and experiment a bit, soon you will realize you can do almost anything just using options, without coding any line.
If you like this post and you are interested in ECharts, let me know with a comment or a tweet and I will write more posts with other use cases.
`,url:"https://sergiocarracedo.es/using-charts-frontend-echarts-with-examples/",image:"/using-charts-frontend-echarts-with-examples/pexels-lukas-590022_hu_402624c29eb77ce9.jpg",tags:["data visualization"],readingTime:"7 minutes read",date:"Nov 22, 2022"},"https://sergiocarracedo.es/typescript-type-narrowing/":{title:"Typescript type narrowing, type guards and  type predicates ([var] is [type])",content:`One useful feature in Typescript is union types, for example string | number | null, it&rsquo;s a way to specify an argument, return, or variable that can get values with different types.
Type Narrowing is a technique that allows Typescript compiler to reduce the types of a value evaluating guard clauses in compilation time.
Let&rsquo;s see a simple example, imagine we have a function to uppercase a value that can be number, string, or just null, into the function we need to handle the different cases, but at the same time Typescript can understand the types and reduce value type to the correct type in each code&rsquo;s branch.
function uppercase(value: string | number | null ): string { // Here value&#39;s type is string | number | null if (value === null) { console.log(value) // Here value&#39;s type is null return &#34;&#34; } if (typeof value === &#39;number&#39;) { // Here value&#39;s type is number value = value.toString() } // Here we can be sure the value&#39;s type is string return value.toUpperCase() } You can check it by yourself by hovering value across the code in the Typescript&rsquo;s playground
Note type narrowing is not to reduce the types to just one type, in our example line 2 (if (value === null)) makes sure the value is null, but a guard like if (value) only removes the possibility of being null, so after this guard value&rsquo;s type is number | string
Why type narrowing When a value can have multiple possible types (union type), it&rsquo;s important to handle the values in the correct way to avoid runtime errors, a simple function like the example,:
function uppercase(value) { return value.toUpperCase() } is completely valid in vanilla Javascript, but can fail on runtime if the value is a number or null. Using Typescript gives you the possibility of writing better code as it &ldquo;forces&rdquo; you to write code to handle the case for each type and that is when do type narrowing.
Type guards As I mentioned, type guards are a way of doing type narrowing setting a condition that the typescript compiler can evaluate and unequivocally reduce the types the variable can be.
It&rsquo;s important to note the type guards are relative to typings in compilation time, not in the run time, so, not all the guards you can think are valid as type guard, remember, the value must be able to be inferred on compilation time, not by the value in runtime.
There are a lot of different type guards: typeof, instanceof, in operator, type predicates, discriminated unions, equality operator, etc&hellip;
I don&rsquo;t want to go deep into all the types, the official Typescript documentation is very good, and not too much to add, but anyway I want to focus and provide more info about type predicates that has an interesting syntax and is not very common, but useful.
Type predicates In some cases, the logic to do type narrowing can be a bit complex (more than a simple typeof or a discriminated union) and it will be nice to extract the type narrowing logic, let&rsquo;s see an example, imagine we have this interfaces
interface Shape { type: &#39;square&#39; | &#39;ellipse&#39; } interface Circle extends Shape { type: &#39;ellipse&#39; radius: number } interface Ellipse extends Shape { type: &#39;ellipse&#39; radius1: number radius2: number } interface Square extends Shape { type: &#39;square&#39; side: number } and a function to calculate the area depending on the shape
function area(shape: Circle | Square | Ellipse): number { if (shape.type === &#39;square&#39;) { // shape&#39;s type is Square return shape.side * 2 } if (shape.type === &#39;ellipse&#39; &amp;&amp; !(&#39;radius2&#39; in shape)) { // shape&#39;s type is Circle return shape.radius * shape.radius * Math.PI } // shape&#39;s type is Ellipse return shape.radius1 * shape.radius2 * Math.PI } This works perfectly, the code, the type narrowing, etc. Now imagine you want to extract the logic on knowing if the shape is a circle, just move the logic to a function
function isCircle(shape: Shape): boolean { return shape.type === &#39;ellipse&#39; &amp;&amp; !(&#39;radius2&#39; in shape) } And now this is the function after the refactor
function area(shape: Circle | Square | Ellipse): number { if (shape.type === &#39;square&#39;) { // shape&#39;s type is type is Square return shape.side * 2 } if (isCircle(shape)) { // shape&#39;s type is Circle | Ellipse return shape.radius * shape.radius * Math.PI } // shape&#39;s type is Circle | Ellipse return shape.radius1 * shape.radius2 * Math.PI } Note that now the type after the function isCircle is still being Circle | Ellipse, so the narrowing is not working, but why?. The narrowing is not working as the isCircle returns boolean and the compiler it&rsquo;s not smart enough to know the semantic meaning of this boolean, this is why we need a type predicate
Just changing a bit the function&rsquo;s return type we can achieve our goal:
function isCircle(shape: Shape): shape is Circle { return shape.type === &#39;ellipse&#39; &amp;&amp; !(&#39;radius2&#39; in shape) } [argument] is [type] return type still being a boolean but now has a meaning and lets Typescript compiler know if an argument is of the specified type.
At this point I want to let you know that the typescript compiler is not perfect, and while I was writing this post I found a bug in the type narrowing: if Circle and Ellipse share the attribute radius the compiler still inferring Circle | Ellipse on the isCircle function.
Summarizing The benefits of type narrowing are just the benefits of using Typescript, strong typing, and more control over the values, but it&rsquo;s good to know more about how the compiler works to have a better understanding of the language.
`,url:"https://sergiocarracedo.es/typescript-type-narrowing/",image:"/typescript-type-narrowing/jakob-braun-HTGrBFwlYLA-unsplash_hu_27da69b335eb9824.jpg",tags:["typescript","js"],readingTime:"5 minutes read",date:"Nov 15, 2022"},"https://sergiocarracedo.es/negative-feedback-from-engineers-to-managers/":{title:"Giving (Negative) Feedback From Engineers to Managers",content:`A couple of weeks ago my friend Miguel Garcia Lorenzo releases this quite interesting post about the continuous feedback from the engineering managers.
I agree with all the contents of the posts, but I want to add some comments or ideas from the engineer&rsquo;s point of view (it&rsquo;s more about the point of view of a non-manager person).
It&rsquo;s obvious not all people have the same personality, priorities, and communication skills, and the comments I will write in the next lines don&rsquo;t apply to all people.
As the mentioned post says, feedback is quite important in organizations, it&rsquo;s a tool to improve and to know in detail the current situation.
As Miguel says in his post, the feedback has to be constructive, and I believe that should be both ways, the engineer can complain about something (that is feedback too) but it should be constructive.
But, what about the feedback of the feedback, what is the reaction of the manager or company to a negative feedback
The reaction to the feedback For me, this is the key thing of the feedback: what does it happen when the message is not positive?
By default, most of us will not give negative feedback without knowing the response or reaction, this is a natural behavior, just a way to protect ourselves.
I want to introduce the concept of level of negativity: it&rsquo;s the &ldquo;value&rdquo; a person set to a message, for example, in general terms, saying that the delivery will be delayed because of you is more negative (higher level) than just saying you are not happy with the coffee.
Every person assign different levels of negativity to the same situation, and that is related to how the person perceive the possible bad response to the &ldquo;negative thing to tell&rdquo;. If the level of negativity is higher, I think is less likely for the person to give feedback to the manager, and this is something the managers can handle it creating the correct feedback environment (I&rsquo;m going to do deep in that a few lines later)
It&rsquo;s important to say that not all the negative feedback is to complain, negative feedback can also be, for example:
A delay in the delivery You don&rsquo;t feel comfortable with some technical decisions (that doesn&rsquo;t mean they should do what you want, but is good to express it) You are not sleeping well, or changes in your life can affect you and your motivation. Ask salary review etc. There are things that you can expect a negative response (but not necessarily), or you are not comfortable saying.
The main question is how the manager will handle this feedback: As Miguel mentioned honesty is key, maybe your feedback is the feedback you should give, but the company or the manager cannot do anything about it, or is just expected from the company, the manager must be honest and tell you, the worse thing the manager can do it&rsquo;s just getting your feedback and ignore it.
We agree, feedback is necessary, but I think is the responsibility of the managers to create the correct environment to make it easy to give negative feedback.
The environment for feedback Continuous: to have the possibility of giving feedback should be normal, and recurrent in the time thing. If an engineer needs to force a meeting to say something negative, the possibilities of not doing it are high. We can achieve that with regular all-hands, 1-on-1, informal talks drinking coffee, etc. No one likes to say &ldquo;we need to talk&rdquo;, that is a bad way to start a feedback conversation. Natural: the feedback, positive and negative in general should be something natural, I think even the manager should ask for negative feedback, but do it naturally, without forcing anyone to say something negative it that moment. Honesty: As I mentioned before, is key. Consistent and reasoned: The perceived response to the feedback should be the same or similar in all the cases, if the engineer asks for a salary review, and it&rsquo;s rejected, but a teammate do the same, and it&rsquo;s accepted, the first engineer need to know why the result is different. Maybe KPI are not satisfied, the engineer should know it No revenge: Bad consequences of giving negative feedback will do that person will not do more negative feedback, and probably the whole team will follow this person not doing it as well&quot;. I strongly believe those points make a better environment for negative (and positive) feedback, some of them seem trivial or very basic but are not at all.
Obviously, giving a negative feedback shouldn&rsquo;t be a festival of bad words, saying bad things about teammates or complaining without giving a constructive message, we, the engineers, have our own responsibilities in giving feedback
The engineer feedback&rsquo;s responsibilities Constructive: The objective of giving (negative) feedback is to improve or change things, your feedback should be always constructive. Respectful: You can say the same without losing respect for anyone. Concise: You need to be clear explaining the key points of your feedback, mainly the negative one Honest: Being honest is key here as well. Open to hearing: feedback is not a monologue, you should be open to hearing and to dialog. In general it&rsquo;s simple: negative (and positive) feedback should be assertive and honest
Giving feedback is teamwork, managers should create the correct environment and the engineers should be brave and be able to express &ldquo;negative&rdquo; things too.
All of us have a responsibility in how we do the feedback, as an engineer you can&rsquo;t think is just a manager&rsquo;s responsibility to &ldquo;extract&rdquo; the feedback from you, and as a manager, you can&rsquo;t think the engineer will give you feedback just because of the reasons.
My experience is overall very good. In the companies I worked in the last 10 years this environment was nice and it was easy to give feedback (positive or negative), but not always it was the same before that and outside the IT sector, it was even harder.
`,url:"https://sergiocarracedo.es/negative-feedback-from-engineers-to-managers/",image:"/negative-feedback-from-engineers-to-managers/jon-tyson-82ZEOTntP8g-unsplash_hu_d95ad487432c4b4b.jpg",tags:["management"],readingTime:"5 minutes read",date:"Nov 8, 2022"},"https://sergiocarracedo.es/arquitecturas-limpias-como-necesidad/":{title:"Las arquitecturas limpias como necesidad",content:` Este post fue escrito originalmente e incluido en la revista de la #pulpoCon22
Todo tiene su momento y las soluciones que valen hoy, pueden no servir para ma√±ana.
Por eso, cuando hablamos de arquitecturas limpias, clean code, desacomparte del framework, etc. en muchos casos nos suenan a que nuestro producto va a ser mejor solo por el hecho de aplicarlas.
Pero no siempre es as√≠, hay fases en la que la simplicidad del c√≥digo prima sobre su ‚Äúbelleza‚Äù, donde la rapidez en el delivery prima sobre la abstracci√≥n, pero lo importante es que seamos conscientes antes o despues necesitaremos arquitecturas limpias para seguir creciendo y escalando.
Y esto no deber ser un trauma, es la madurez que poco a poco va alcanzando el proyecto en el que trabajas.
Llegar√° un momento en el que desacoplarte del framework ser√° una necesidad de la que tu y tu equipo os dareis cuenta, por que un dia quereis actualizar el framework y sabes que va a ser un dolor, porque quieres mejorar el testing y necesitas levantar una base de datos para testar una tarea simple, porque el equipo crece, y para empezar a tocar el c√≥digo se necesita un onboarding muy largo
Ese es el momento donde entiendes que las cosas se pueden hacer de otra forma, que ahora es mejor para el crecimiento de tu proyecto.
Las arquitecturas limpias no son un capricho, o algo que ‚Äúqueda bien‚Äù , no son un objetivo, son simplemente un medio para resolver necesidades de los proyectos.
En el germen del proyecto, no son necesarias, pueden incluso llegar a ser un estorbo, pero tienes que ser conciente que seg√∫n crece el proyecto las arquitecturas limpias ser√°n, cada dia que pase, una necesidad que vendr√° a ayudarte.
`,url:"https://sergiocarracedo.es/arquitecturas-limpias-como-necesidad/",image:"/arquitecturas-limpias-como-necesidad/arquitecturas-limpias-1650627_hu_768bb0b6f84946e1.jpg",tags:["crafters"],readingTime:"2 minutes read",date:"Sep 8, 2022"},"https://sergiocarracedo.es/generics-in-golang-1.18/":{title:"Generics in Golang 1.18",content:`At the moment of writing this post Go 1.18 wasn&rsquo;t released (the latest version for 1.18 is the Release Candidate 1), but we can still play using the playground (enabling the dev branch) or installing the RC
go install golang.org/dl/go1.18rc1@latest go1.18rc1 download One of the most interesting novelties is the generics in a similar way we have in other languages (C#, Java, Typescript, etc&hellip;). Generics allow us as developers, for example, to create a function that works with different types. The classical example Go uses in its blog to explain why generics (highly recommended), is a function that does some operation over an array, for example, reverse the array. Now we must write the same function for different types, ex: int, float, string, etc
func ReverseInts(s []int) { first := 0 last := len(s) - 1 for first &lt; last { s[first], s[last] = s[last], s[first] first++ last-- } } func ReverseStrings(s []string) { first := 0 last := len(s) - 1 for first &lt; last { s[first], s[last] = s[last], s[first] first++ last-- } } ... As you see the code inside the function is exactly the same for both types, only the changes the types in signature. That is not nice because we should maintain the same logic in 2, 3, or more different places.
How we can achieve it using generics: adding a new element to the signature between brackets [typeName constraint] and using this T definition as the argument Type Like in this example:
func Reverse[T any] (s []T) { first := 0 last := len(s) - 1 for first &lt; last { s[first], s[last] = s[last], s[first] first++ last-- } } Reverse[int]([]int{1, 2, 3, 4}) Reverse[string]([]string{&#34;1&#34;, &#34;2&#34;, &#34;3&#34;, &#34;4&#34;}) Try it on playground
Note you can have multiple generics in the same function even the return type can be typed: func [T any, U any](arg0 T) U
Constraints In the examples above we are using any as a constraint that means all the types can be used with the function, but in most of the cases we will need to limit the types we can use the function,
The next simpler constraint is the union type:
The union type is a list of possible types: int | float64. In our previous example, trying to Reverse an array of strings will return the error string does not implement int. That means Go is not comparing the types themselves, is comparing the interface of the types, and that is important for the next type of constraint.
Imagine we want to create a function to get the minimum value in the array, we could write something like:
// Not this function only works for positive numbers. but it&#39;s for example pourpouse func Max[T any](values []T) T { var max T for _, v := range values { if v &gt; max { max = v } } return max } a := []int{1, 2, 3, 4, 5} fmt.Println(&#34;Max:&#34;, Max[int](a)) If we run above we will get the error invalid operation: v &gt; max (type parameter T is not comparable with &gt;). That is because not all the types represented by any implement the operator &gt; and are not comparable.
We can solve this using func Max[T int|string](values []T) T as signature, but there is a better way: using the constraints package (in the moment of writting this it was removed from the standard library and moved to )(exp/constrains](https://pkg.go.dev/golang.org/x/exp/constraints) https://go-review.googlesource.com/c/go/+/382460/)
So we can so
import &#34;golang.org/x/exp/constraints&#34; func Max[T constraints.Ordered](values []T) T { .... } Type approximation Is very common in Go creating custom types from a &ldquo;primitive&rdquo; type
type MyString = string The problem with generics is that MyString is not the same type as string, so func [T string|int]MyFunc(arg T) will not work with MyString.
The way to solve it is the type aproximation: that is a type that underlying is the type specified. Let&rsquo;s see it with an example: ~string represent any type that is a pure string or is string underlying as our MyString
More info in the Go spec
Generic Structs Go also supports generics in Structs:
type MyGenericStruct[T string | int, U constraints.Ordered] struct { id T value U } // So this works and makes sense c := MyGenericStruct[int, string]{1, &#34;2&#34;} d := MyGenericStruct[string, int]{&#34;c&#34;, 2} That means we can use generics in methods (but in a limited way), we can use generic in the receiver, but not in the method, this was [pushed to go 1.19]
// Works func (m MyGenericStruct[T, U]) GetValue() U { return m.value } // Doesn&#39;t Work func (m MyGenericStruct[T, U]) [A any]GetValueAndAdd(add A) U { return m.value + add } For the methods, we could use generics defined in the struct as a roundabout, but I think is not very elegant
type MyGenericStruct[T string | int, U constraints.Ordered, A any] struct { id T value U } func (m MyGenericStruct[T, U]) [A any]GetValueAndAdd(add A) U { return m.value + add } any The new keyword any we used above it&rsquo;s just an alias of interface{}, and we could use it in any place we were using interface{}, ex: map[string]any
Summarizing In my opinion, generics in Go 1.18 are a big improvement in terms of flexibility creating reusable logic independent of the types but keeping the language robust. Union types are only allowed in the constraints so there is no ambiguity in the types as in other languages inside the function.
`,url:"https://sergiocarracedo.es/generics-in-golang-1.18/",image:"/generics-in-golang-1.18/go-generics-4016522_hu_9108fd5d6ebd7859.jpg",tags:["golang"],readingTime:"5 minutes read",date:"Feb 28, 2022"},"https://sergiocarracedo.es/running-workflows-in-you-k8s-cluster-argo-workflows/":{title:"Running workflows in you k8s cluster: Argo Workflows",content:`Simplifying it, a workflow is a list of tasks to run in some order and/or fulfilling some dependencies, for example, if we have 5 tasks to run: A, B, C, D, E. Task C depends on finishing tasks A and B, task D depends on finishing C and E
Something like this:
There are several tools to orchestrate that, but we&rsquo;ll focus on Argo Workflows
Argo Workflows is an open-source container-native workflow engine for orchestrating parallel jobs on Kubernetes.
Running over Kubernetes is one of the most characteristic things of Argo differentiating it from others.
Every task you define will run in a container or using other words, you must create a container to run the tasks, and all of it will run in your Kubernetes cluster
Install Argo Workflows Installing Argo Workflows is very easy, you only need to apply a manifest in your cluster to configure Argo&rsquo;s services in the cluster:
For example https://raw.githubusercontent.com/argoproj/argo-workflows/master/manifests/quick-start-postgres.yaml
More info: https://argoproj.github.io/argo-workflows/quick-start/
Argo Workflows also provides a URL to access a UI to manage Workflows, Events, Reports, Users, Docs, etc&hellip;
To keep track of the workflows, etc. Argo needs persistence for example: Postgres, MySQL, etc&hellip;
As is indicated in the official documentation is highly recommended create a namespace (ex. argo) in the cluster to &ldquo;install&rdquo; into it all Argo&rsquo;s services.
Workflows There are two types of workflows: Regular workflows and Cron Workflows
Both are basically the same, but a cron workflow creates a Regular workflow automatically when should be executed according to the cron syntax, ex. */3 * * * * *. Note it can create more than one workflow
Workflows are defined as Kubernetes manifest that should be applied to the same namespace as Argo services.
This manifest defines all the tasks and their dependencies
apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: name: my-workflow apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: my-workflow spec: entrypoint: tasksDependencies # This is the name of the template to run first templates: - name: exampleTask inputs: parameters: - name: msg container: image: docker/whalesay command: [cowsay] args: [{{ inputs.parameters.msg }}] - name: tasksDependencies dag: tasks: - name: sayHello template: exampleTask arguments: parameters: - name: text value: &#34;Hello&#34; - name: sayNiceJob template: exampleTask dependencies: [ sayHello ] arguments: parameters: - name: text value: &#34;Nice Job&#34; - name: sayRunning template: exampleTask dependencies: [ sayHello ] arguments: parameters: - name: text value: &#34;Running&#34; - name: sayFinished template: exampleTask dependencies: [ sayRunning, sayNiceJob] arguments: parameters: - name: text value: &#34;It&#39;s over&#34; spec: entrypoint: exampleTask # This is the name of the template to run first templates: - name: exampleTask inputs: parameters: - name: msg container: image: docker/whalesay command: [cowsay] args: [&#34;{{ inputs.parameters.msg }}&#34;] - name: tasksDependencies dag: tasks: - name: sayHello template: exampleTask arguments: parameters: - name: msg value: &#34;Hello&#34; - name: sayNiceJob template: exampleTask dependencies: [ sayHello ] arguments: parameters: - name: msg value: &#34;Nice Job&#34; - name: sayRunning template: exampleTask dependencies: [ sayHello ] arguments: parameters: - name: msg value: &#34;Running&#34; - name: sayFinished template: exapleTask dependencies: [ sayRunning, sayNiceJob] arguments: parameters: - name: msg value: &#34;It&#39;s over&#34; Let&rsquo;s explain this example, but in the first view you can see the power of Argo
A template defines a job to be done, can be a container (as in our example), a script, a resource (to do operations on the cluster&rsquo;s resources directly from the workflow) and suspend that is just to wait the time defined.
In our example, we defined a template called exampleTask (this name must be unique and can be used to refer to this template).
For the task, we define one input parameter, a msg to print. This value can be referenced later.
This task uses a container with the image docker/whalesay from docker registry, but you can use your own and private registry, Argo runs the command [cowsay] and uses the input values defined previously as command arguments [{{inputs.parameters.text}}]
Templates also can define Template Invocators, that are used to call other templates and do execution control, in our example we are using DAG ((Directed Acyclic Graph)[https://airflow.apache.org/docs/apache-airflow/1.10.12/concepts.html#:~:text=In%20Airflow%2C%20a%20DAG%20%E2%80%93%20or,and%20their%20dependencies)%20as%20code.]), but we can use also steps, but that allows us to create better dependencies
In our case we are defining 3 tasks, all use the same template with different params (but we can use different templates for different tasks), the entry point is sayHello and sayNiceJob and sayRunning after that, and finally sayFinished only will run after sayNiceJob and sayRunning.
An after applying the manifest: kubectl -n argo -f workflow.yml Argo runs it This is how the workflow looks after running it
Clicking over a task we can get information about the run: the summary, the input and outputs, the container involved in the run, and the logs or the run
To summarizing, if you have a Kubernetes cluster and need to run workflows, Argo is a very good option.
I will write more blog post in the future about Argo, for example how to configure the access security.
`,url:"https://sergiocarracedo.es/running-workflows-in-you-k8s-cluster-argo-workflows/",image:"/running-workflows-in-you-k8s-cluster-argo-workflows/cover_hu_2dcebb68a44be914.jpg",tags:["devops","k8s"],readingTime:"4 minutes read",date:"Dec 28, 2021"},"https://sergiocarracedo.es/dealing-with-infinite-pagination/":{title:"Dealing with infinite pagination",content:`In the software development context, pagination is the process of dividing a list of items (rows) into groups of the same size.
For example if we have these items:
const items = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16] and we want to paginate in pages (groups) of 5 elements we should know the list or the total items in the list, and we can calculate the number of pages, and how to get the items on a page (I&rsquo;m assuming that the page value starts in 0)
const itemsPerPage = 5 const items = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16] const pages = Math.ceil(items.length / itemsPerPage) const getPageItems = (items, page, itemsPerPage) =&gt; { return items.slice(page * itemsPerPage, itemsPerPage) } This is the ideal situation where the list of items is known, and we are working only on frontend or backend.
The common situation is when your frontend shows the items and the paginator, and you get the data from an API. In this case, you will do a request like https://example.com/list-items/?page=1 or better using offset and limit instead of page for more flexibility: https://example.com/list-items/offset=0&amp;limit=5
In this situation to render the paginator, we must know the total number of items or pages, that is why the response of the server should be something similar to:
{ &#34;pagination&#34;: { &#34;totalItems&#34;: 16 }, &#34;items&#34;: [...] } Then, after loading the first page we will get the total items in the server, and we can calculate the number of pages and render the paginator.
If our database table from where we get the items is big, the operation of counting the total number of events can be very expensive.
Pagination without knowing the total count of items (a.k.a. Infinite pagination) The concept is very similar to the infinite scrolling, where the user does scroll and when the scroll is in the last item, your component loads a few items more, and so on.
The different thing is we must show a paginator. How we should show the paginator?
As we don&rsquo;t know the total number of pages we have some questions to answer:
How many pages we must show in the paginator? What is the last page? The algorithm This algorithm will help us to know how many pages we must show and how to know the last page
In our paginator component we must have two variables:
page is the current page pages is the total number of pages lastPage is the last confirmed page (default value Number.POSITIVE_INFINITY) pages is a dynamic value, the initial value is 1
Load the first page using the offset and limit params. offset will be 0 (it&rsquo;s the first page), and limit will be, let&rsquo;s say 10 If the count of loaded items is less than the limit in this case 10, then we know this is the last page If not, if the count is 10, we increment the value of pages by 1, then our component must render a new page button. There is a special case: What happens if the last page has 10 items? In that case when the user tries to go to this page, when we will load it we will get an empty list of items, then, we must set the value of lastPage to the last page with items and use lastPage to only render this number of pages.
With this simple algorithm we can create an infinite paginator, and obviously, it has some disadvantages:
The user only can navigate the pages in order (Can&rsquo;t go to page 10 from page 1 without pass through the page 2,3,4,5&hellip;) If we have the case of 10 items (limit = page items) in the last page, is strange for the user to go back to the last page and remove the next button Despite these disadvantages is a good solution if our backend doesn&rsquo;t let us know the total number of pages.
`,url:"https://sergiocarracedo.es/dealing-with-infinite-pagination/",image:"/dealing-with-infinite-pagination/infinite-pagination-GVYRkT5f1tA-unsplash_hu_ec150deafb85556a.jpg",tags:["frontend"],readingTime:"4 minutes read",date:"Nov 22, 2021"},"https://sergiocarracedo.es/integration-tests-in-golang-with-dockertest/":{title:"Integration tests in Golang with dockertest ",content:`Do integration testing (or system testing) usually means to have a database populated with data, services like redis, elasticsearch, etc&hellip; working, In general, any infrastructure with which our software interacts.
The most common way to do it is to have a replica of our production infrastructure. Actually, it&rsquo;s relatively easy to achieve using containers, for example, docker containers.
We can set up and run a container for every service we need to replicate, we can orchestrate it with docker-compose and create some makefiles or just a simple script to prepare the infrastructure and run the integration tests.
If your tests are independent (they should), you must find the way to &ldquo;restart&rdquo; the infrastructure services between tests, and this can be hard to get with a separated infrastructure setup and tests (the infra is set up in a script and the tests are in Go files)
dockertest If you are using Golang, you can use dockertest, a library with which you can manage and orchestrate the containers in your Go test files.
Manage the test infrastructure container from the Go files allow us to control which service we need in each test (for example, some package is using a database but not Redis, makes no sense to run the Redis for this test)
Installing dockertest To install dockertest, just run
go get -u github.com/ory/dockertest/v3 Using dockertest The simplest way to set up the infrastructure with dockertest is to add the setup code in the TestMain function in your test file.
TestMain is a function is called before running the tests in the package More info
This is an example of how to set up a MySQL service using dockertest
package mypackage_test import ( &#34;database/sql&#34; &#34;fmt&#34; &#34;log&#34; &#34;os&#34; &#34;testing&#34; _ &#34;github.com/go-sql-driver/mysql&#34; &#34;github.com/ory/dockertest/v3&#34; ) var db *sql.DB func TestMain(m *testing.M) { // uses a sensible default on windows (tcp/http) and linux/osx (socket) pool, err := dockertest.NewPool(&#34;&#34;) if err != nil { log.Fatalf(&#34;Could not connect to docker: %s&#34;, err) } // pulls an image, creates a container based on it and runs it resource, err := pool.Run(&#34;mysql&#34;, &#34;5.7&#34;, []string{&#34;MYSQL_ROOT_PASSWORD=secret&#34;}) if err != nil { log.Fatalf(&#34;Could not start resource: %s&#34;, err) } // exponential backoff-retry, because the application in the container might not be ready to accept connections yet if err := pool.Retry(func() error { var err error db, err = sql.Open(&#34;mysql&#34;, fmt.Sprintf(&#34;root:secret@(localhost:%s)/mysql&#34;, resource.GetPort(&#34;3306/tcp&#34;))) if err != nil { return err } return db.Ping() }); err != nil { log.Fatalf(&#34;Could not connect to docker: %s&#34;, err) } // RESERVED FOR DATABASE MIGRATIONS code := m.Run() // You can&#39;t defer this because os.Exit doesn&#39;t care for defer if err := pool.Purge(resource); err != nil { log.Fatalf(&#34;Could not purge resource: %s&#34;, err) } os.Exit(code) } Populate database Now we have the database service working, but this database is empty. dockertest is using a generic MySQL image for the container and nothing related to our app is there.
If you follow my posts, you would remember I wrote a post about database migrations (if not you can take a look at it). In that post I talked about go-migrate a tool to run database migrations but, in it, I focused on the usage as CLI tool, now we will use it in our Go code
In the previous code in the line where we wrote // RESERVED FOR DATABASE MIGRATIONS we will add this code
m, err := migrate.NewWithDatabaseInstance(&#34;file://&lt;path-to-migration-folder&gt;, &#34;mysql&#34;, driver) if err != nil { log.Fatalf(&#34;Error running migrations: %s&#34;, err) } err = m.Up() if err != nil { log.Fatal(err.Error()) } Then after dockertest ups the database, the migration tool populates the database and our integration tests can run with the same data in the database.
If the app has more than one package (that is the common situation), I put the services&rsquo; setup code in an independent file which is called from every package:
// it_utils.go package it_utils func IntegrationTestSetup() (*dockertest.Pool, *[]dockertestResource { // Setup the services //return the pool and the resources } func IntegrationTestTeardown(pool *dockertest.Pool, resources []*dockertest.Resource) { for _, resource := range resources { if err := pool.Purge(resource); err != nil { fmt.Printf(&#34;Could not purge resource: %s\\n&#34;, err) } } } Then in each package&rsquo;s test we only need to add
package my_package func TestMyTests (t *testing.T) { if testing.Short() { t.Skip() } pool, resources := itutils.IntegrationTestSetup() defer itutils.IntegrationTestTeardown(pool, resources) t.Run(&#34;your test&#34;, func(t *testing.T) { ... } } func TestOtherTests (t *testing.T) { if testing.Short() { t.Skip() } pool, resources := itutils.IntegrationTestSetup() defer itutils.IntegrationTestTeardown(pool, resources) t.Run(&#34;your other test&#34;, func(t *testing.T) { ... } } Doing it in that way on every test block the service runs in a new container making the test completely independent.
As a last tip, I recommend putting the integration test in a different package to avoid circular imports.
`,url:"https://sergiocarracedo.es/integration-tests-in-golang-with-dockertest/",image:"/integration-tests-in-golang-with-dockertest/dockertest-pexels-miguel-3785927_hu_7da0de34dc818339.jpg",tags:["devops","testing","golang"],readingTime:"4 minutes read",date:"Nov 9, 2021"},"https://sergiocarracedo.es/vue-composition-api-how-to-split-and-reuse-code/":{title:"Vue Composition API: How to split and reuse code",content:"The Vue Composition API arrived at us in Nov 2018, 2 years ago, as a preview and become with some controversial, because people believed composition API would replace the traditional Object API, but would not.\nAnyway, after start to develop Vue apps using the composition API I don&rsquo;t want to go back, maybe for very small components makes you write more code than with the Object API, but in most cases, you can take advantage of the composition API features.\nOne of these features is code splitting, and by extension, code reusability.\nOptions API With the Options API we could reuse code through the mixins. The mixins in Vue works like a kind of object composition. Your component will use all the data, methods, etc presents in the mixin, and you can rewrite it. Let see an example.\n// Mixing export default { props: { color: String, size: String }, computed: { colorClasses: () =&gt; { return [`color-${this.color}`] }, sizeClasses: () =&gt; { return [`size-${this.size}`] } } } Imagine we have components we can colorate or change the size, then our mixin includes the properties and the way (computed) to get the classes to apply, the mixing above do exactly that.\nNow let write the component\n// Component &lt;template&gt; &lt;div :class=&#34;[&#39;component-a&#39;, ...colorClasses, ...sizeClasses]&#34;&gt; .... &lt;/div&gt; &lt;/template&gt; &lt;script&gt; import colorSizeMixin from &#39;...&#39; export default { mixins: { colorSizeMixin } } &lt;/script&gt; This thing works, but only looking at the component file, it&rsquo;s very hard to know from where we get the colorClasses and sizeClasses. You need to go to the mixin definition to know from where we are getting the values or which properties you could use in your component.\nMixins have another limitation, mixin can&rsquo;t adapt the behavior (in a simple way), I mean, you can&rsquo;t change the mixin behavior passing to it a flag, in this example, for example, a list of valid colors to accept.\nComposition API Let&rsquo;s do the same with the composition API\n// useClasses.js import { computed } from &#39;vue&#39; export default (props) =&gt; { const colorClasses = computed(() =&gt; [`color-${props.color}`]) const sizeClasses = computed(() =&gt; [`size-${props.size}`]) return { colorClasses, sizeClasses } } // Component (Vue 3) &lt;template&gt; &lt;div :class=&#34;[&#39;component-a&#39;, ...colorClasses, ...sizeClasses]&#34;&gt; .... &lt;/div&gt; &lt;/template&gt; &lt;script&gt; import { defineComponent } from &#39;vue&#39; import useClasses from &#39;./useClasses.js&#39; export default defineComponent({ props: { color: String, size: String }, setup(props) { const { colorClasses, sizeClasses } = useClasses(props) return { colorClasses, sizeClasses } } }) &lt;/script&gt; Now we have the same functionality, the colorClasses and sizeClasses can be reusable in another component as we had using mixins.\nComposition API has some advantages:\nIt&rsquo;s very easy to see from where the colorClasses and sizeClasses comes, and what input needs We don&rsquo;t need to &ldquo;import&rdquo; or use all the methods as in the mixins, for example here we can only get the sizeClasses computed value (const { sizeClasses } = useClasses(props)) without any modification in the useClasses.js We can parameterize the behavior, for example: // useClasses.js import { computed } from &#39;vue&#39; export default (props, allowedColors) =&gt; { const colorClasses = computed(() =&gt; allowedColors.indexOf(props.color) !== 1 ? [`color-${props.color}`] : [] const sizeClasses = computed(() =&gt; [`size-${props.size}`]) return { colorClasses, sizeClasses } } Now we can pass to the useClass an array with all allowed colors that can be different in different components, with mixins that it&rsquo;s hard to achieve.\nSplitting the code Using this technique we can split our component&rsquo;s code into different &ldquo;uses&rdquo; files, and if we put the related functionalities in the same &ldquo;use&rdquo; file we can reuse it. For example, we have a that needs to control the window scroll, we can write a &ldquo;use&rdquo; file like this:\n// useScroll.js import { onBeforeUnmount, onMounted, Ref, ref } from &#39;@vue/composition-api&#39; export default () =&gt; { const scrollY = ref(0) const scrollX = ref(0) // Before update the reactive values we store it in a local variable let localX = 0 let localY = 0 const onScroll = (e) =&gt; { localX = window.scrollX localY = window.scrollY } //We only update reactive values every 100 to avoid a update it too much setInterval(() =&gt; { if (localX !== scrollX.value) { scrollX.value = localX } if (localY !== scrollY.value) { scrollY.value = localY } }, 100) onMounted(() =&gt; { window.addEventListener(&#39;scroll&#39;, onScroll) }) onBeforeUnmount(() =&gt; { window.removeEventListener(&#39;scroll&#39;, onScroll) }) return { scrollX, scrollY } } Note that we can set the scroll position directly to the scrollX and scrollY but we want to avoid triggering the re-render of the component several times, and we store the values in a local variable, and every 100ms we dump the values to the reactive variables.\nvue-use-web is a library inspired on that.\nSome tips passing values to the &ldquo;use&rdquo; files In the example file useClasses.js I passed the component props, which means we pass all props, this is not very good, because the component using the &ldquo;use&rdquo; file must pass the necessary properties, and this couldn&rsquo;t happen.\nIt&rsquo;s better to define in the &ldquo;use&rdquo; signature the params we need. This is the previous code rewrite using this.\n// useClasses.js import { computed } from &#39;vue&#39; export default (color, size) =&gt; { const colorClasses = computed(() =&gt; [`color-${color}`]) const sizeClasses = computed(() =&gt; [`size-${size}`]) return { colorClasses, sizeClasses } } And in the component\n... const { colorClasses, sizeClasses } = useClasses(props.color, props.size) ... You probably realized, that now the computed variables inside the use file will never be updated event if the property changes. That&rsquo;s because the prop.color is a string and is passed as copy not as reference.\nTo solve that, we must pass the properties through a function:\n... const { colorClasses, sizeClasses } = useClasses(() =&gt; props.color, () =&gt; props.size) ... And change our &ldquo;use&rdquo; file, adding the () to call the wrapper function and get the &ldquo;live&rdquo; value of the property\n// useClasses.js import { computed } from &#39;vue&#39; export default (color, size) =&gt; { const colorClasses = computed(() =&gt; [`color-${color()}`]) const sizeClasses = computed(() =&gt; [`size-${size()}`]) return { colorClasses, sizeClasses } } As personal opinion I being using Composition API for a long time, even in Vue 2.x, and I prefer it over Options API because I feel the code is better, easier to read, easier to reuse, and could be code non-related with Vue, I mean you can write all the logic without use anything related with Vue and after all the &ldquo;calculations&rdquo; put the results in a reactive variable, so this code could be used in other frameworks.\n",url:"https://sergiocarracedo.es/vue-composition-api-how-to-split-and-reuse-code/",image:"/vue-composition-api-how-to-split-and-reuse-code/vue-composition-api-3y1zF4hIPCg-unsplash_hu_fe70c7b03cd657e2.jpg",tags:["vue"],readingTime:"6 minutes read",date:"Nov 2, 2021"},"https://sergiocarracedo.es/database-migrations-in-golang/":{title:"Database migrations in Golang.",content:`During the development of an app, it&rsquo;s very common to do changes in the database schema, for a new feature you need to add a new table, add a new column to an existing table, alter the type of existing column or delete a column.
When you work alone you could do it manually, run the queries to alter the database schemas manually.
What happens when your team is not a one-person team? You need to share the queries to change the schema with your teammates, and they should know what changes they applied before to know if there are some new changes.
To simplify this task database migration tools were born. These tools do all this thing on behalf us.
Let see how the workflow with these tools works:
When a member of the team needs to change something in the schema, she/he creates a text file with the sentences to achieve the new schema. This file is usually stored in the repository, for example in a folder called migrations. if we also store it in the repository we can share easily and track changes (New files added, migration files should never be modified) When a new migration file is detected the migration tool will run and apply the changes to the database Run the migration should be idempotent, that&rsquo;s that you can run it several times with the same migration files, and the final database schema must be the same. To achieve that usually, the migration tools store in a database table the last migration that ran ok and apply the new ones.
go-migrate Go Migrate is a migration tool written in Golang. It can work as a CLI or as a Go library.
As a CLI tool, you can use it for projects in any language, not necessarily Go.
Go migrate read the migrations from a source, that they could be: files, GitHub Repo, Bitbucket, AWS S3, Google cloud storage, etc, and applies the changes in the database.
It supports several database types, but SQL and non-SQL, like PostgreSQL, MySQL, MongoDB, Clickhouse, Cassandra, etc&hellip; See the complete list of supported databases.
To track which migrations need to be applied, it stores the status in the database.
Installing go-migrate (CLI) For Go 1.16+ just execute in your terminal
go install -tags &#39;postgres&#39; github.com/golang-migrate/migrate/v4/cmd/migrate@latest You can also download the binary from here
Check the documentation for more instructions
Your first migration file In these examples I&rsquo;m going to use Postgres as target database
Our goal is to get the database schema our app needs from the migration files. The first migration file should create the tables we need. Imagine we need a &lsquo;user&rsquo; table.
We must create a file with the following name schema: {version}_{title}.up.{extension}, for example: 1_add_users_table.up.sql, with the following content
CREATE SCHEMA common; CREATE TABLE common.users( id SERIAL NOT NULL, name VARCHAR NOT NULL, email VARCHAR UNIQUE NOT NULL, created_at TIMESTAMP NOT NULL DEFAULT NOW(), PRIMARY KEY (id); Then we can run migrate:
migrate -source file://migrations -database postgres://user:pass@localhost:5434/database up Go migrate will check the last completed migration version and applies the following, in this case, we never run go migrate so will execute our file.
Adding more migration files Imagine that we need to add a new column, for example age. We will create a file with the name 2_add_age_to_users.up.sql with content down below:
ALTER TABLE common.users ADD COLUMN age INT; Anyone in the team can run the migrate command again and get the new column.
if you run again the command, nothing happens because go-migrate knows all migration were applied.
You can execute go-migrate in a deployment pipeline like GitHub Action to put your database in the correct schema
One of the advantages of putting the schema updates in migration files, store them in the repo and run go-migrate in the deployment pipeline, is that the database schema can be synced with the app version. I mean, imagine you are working on a new feature in a new repo branch, you can define the migration files you need for this feature and commit them at the same time your code. if your code is promoted to the main branch, when the code is deployed, the database update its schema
Migration rollback go-migrate also allows us to do a migration rollback, that is a database query or queries to put the database schema as before run the equivalent up file.
In our example we can write the &lsquo;down&rsquo; file for the second migration 2_add_age_to_users.up.sql must have the name 2_add_age_to_users.down.sql (the same name replacing up by down)
ALTER TABLE common.users DROP COLUMN age; If we want to roll back to version 1 we must run:
migrate -source file://migrations -database postgres://user:pass@localhost:5434/database down 2 Down migration files are usually not written because usually can mean data loss.
Taylor Otwell the creator of Laravel said in an interview:
My view on that recently, in a past year, has been that you just never rollback. Ever. You would always go forward. Because I don‚Äôt know how you roll back without losing customer data. At least for my own projects like Forge or Envoyer, I could never really guarantee that I wasn‚Äôt losing data, so I think if at all possible, what I would try to do is write an entirely new migration that fixes whatever problem there is, and it would just migrate forward.
https://laraveldaily.com/still-need-migrations-taylor-says-no/
Next steps In this post, I talked about how to use go-migrate as CLI but we can use it in our Golang programs. That it&rsquo;s very useful for example to run an integration test. I will write a post about how to manage integration&rsquo;s test in Go.
`,url:"https://sergiocarracedo.es/database-migrations-in-golang/",image:"/database-migrations-in-golang/go-migrate-pexels-james-wheeler-1598075_hu_46e52e608c1a951.jpg",tags:["devops","golang","database"],readingTime:"5 minutes read",date:"Oct 25, 2021"},"https://sergiocarracedo.es/creating-custom-events-in-js/":{title:"Creating custom events in JS",content:`When you try to encapsulate features but need to interact with async events or events generated by user interaction, one of the easiest ways to achieve that is using event and event handlers (often called callbacks).
Imagine you have a UI component to render a TO-DO list with a button to create a TO-DO item through a form. This component is self-encapsulated, you only need to put the component in your app, and the component itself renders the list, the button, the form, do the request to API, etc.
You want to do some action after the TO-DO creation, for example, display a toast or an alert with some message.
You can alter the component and add that behavior into the component, but this reduces the reusability of the component because in other parts of your app (or in other app) this behaviour makes no sense.
Events and callbacks to the rescue A callback for an event is a piece of code that runs when the event is dispatched. We are using that a lot in JS, for example when we want to do something after the user interacts with an element.
element.addEventListener(&#39;click&#39;, () =&gt; { alert(&#39;hello&#39;) }, ...) With the line above, the code (listener or callback) will run when the user clicks the element
Browser&rsquo;s API provides a lot of events we can handle, but we want to do it by ourselves, and it&rsquo;s very easy:
First, we need to expose outside the component I mentioned before a method to set the callback for example setToDoCreateEventHandler
To simplify the example we will create a simple event dispatch system that only allows one handler
This function will receive as param a function will be called when the event dispatches
For example:
// Main app import myTodoComponent from &#39;myTodoComponent&#39; myTodoComponent.setToDoCreateEventHandler(() =&gt; alert(&#39;TO-DO created&#39;)) The implementation of the method in the component could be something like:
// Component let toDoCreateEventHandler: Function = () =&gt; {} export function setToDoCreateEventHandler (handler: Function): void { toDoCreateEventHandler = handler } We have the variable toDoCreateEventHandler where to store the handler, by default I set an empty function () =&gt; {} that doesn&rsquo;t do anything just to avoid manage null values (but you can allow null or undefined as a handler and check it before dispatch it) Our exposed setToDoCreateEventHandler function is in charge of set the handler to the variable Ok, now we can store the handler, but we need to dispatch it, to do it we only need to execute the handler in the part of the component where the TODO creation is complete, imagine is after sending the values to API and get an OK
// Component ... axios(...).then(() =&gt; { ... // Do other things toDoCreateEventHandler() }) ... That&rsquo;s all &#x1f602;, after saving the TODO doing a call to the API using axios (in this example) we call the handler stored in the variable, and our code outside the component will be executed.
As you can see it&rsquo;s very easy to create custom components.
We can improve our event handler allowing to add more than one listener/handler, for example
// Component let toDoCreateEventHandlers: Function[] = [] export function addToDoCreateEventHandler (handler: Function): void { toDoCreateEventHandlers.push(handler) } ... axios(...).then(() =&gt; { ... // Do other things toDoCreateEventHandlers.forEach(handler =&gt; handler()) }) ... We also need to define a method to remove a handler, but I will let you do it.
You could think we can achieve this using Promises, and you are partially right, but promises only can be resolved once
Event dispatch / handler has the advantage you can attach to the event at any moment (and wait for the new dispatches) and are widely used in the standard JS libraries.
`,url:"https://sergiocarracedo.es/creating-custom-events-in-js/",image:"/creating-custom-events-in-js/custom-events-1314544_hu_c5f42bcea5d02a2b.jpg",tags:["js"],readingTime:"3 minutes read",date:"Oct 18, 2021"},"https://sergiocarracedo.es/how-to-create-a-table-with-a-fixed-header/":{title:"How to create a table with a fixed header",content:`Tables in HTML are one of the older things in the standard, even before CSS we had HTML tables. In the past tables were used to markup the webpages, but the correct usage is to display tabular data.
Over time the standard improves the table styling covering most of the use cases you can consider.
But, there is a use case it&rsquo;s not easy to get with the table attributes or style properties, I&rsquo;m talking about creating a table with a fixed/sticky header (or footer).
The behavior we want to get is, for a large table, make it possible to scroll the table content showing always on top of the header.
Possible solutions There is more than one solution to this problem. It depends on your needs.
The simplest solution I found is to use CSS to set position: sticky to the th elements:
This solution can work in most of cases, but have some limitations, for example position: sticky is not supported in legacy browsers https://caniuse.com/css-sticky.
In my opinion, the most important limitation is you must set a background color for the header elements to avoid overlapping the table content. This is not always a good solution for all cases.
Following is the same without set the background color for the th elements Another possible solution is to use display: grid and add the position: fixed to the first row
But I don&rsquo;t feel comfortable with this solution because:
It has the same problem with the header&rsquo;s background color We lose the semantic tags (we are displaying as a table, but the html is not a real table, we have a list of div or other tags that don&rsquo;t set a semantic meaning) We must know the number of cols of the table because we must set it in CSS, for example grid-template-columns: repeat(4, 1fr) Duplicating header technique Why don&rsquo;t just put the header outside the table and only scroll the tbody. You can see the result in the following codesandbox. Please notice now, the header can have a transparent background working well with the gradient page background.
Ok, but probably you noticed it, this solution as we did it doesn&rsquo;t work well, the header column sizes are not the same as the content, and if the content has a horizontal scroll the header doesn&rsquo;t follow the scroll position.
To fix it we need some javascript to sync the header columns width and the scroll position.
It&rsquo;s important don&rsquo;t hide the original table head using something like display: none or alter its width with position: absolute we want the original header with the same column width as in a regular table to copy these values to the cloned header. The best way to do it is using visibility: collapse; that for table rows or columns hide the element, and the space occupied is removed, but the size of the columns is still calculated https://developer.mozilla.org/en-US/docs/Web/CSS/visibility
It&rsquo;s also necessary to set the table-layout: fixed to avoid the browser tries to fit the columns space, we want to use the same space we copy from the original header
To copy these widths we can use something like this. The column width sync function is called when the table can change the width, for example on window resize. We can improve this just observing when the table is resized, not when the window is resized using the ResizeObserver
const syncColsWidth = () =&gt; { const thead = document.getElementById(&#34;thead&#34;); const theadClone = document.getElementById(&#34;thead-clone&#34;); const theadCols = thead.getElementsByTagName(&#34;th&#34;); const theadCloneCols = theadClone.getElementsByTagName(&#34;th&#34;); for (i in theadCols) { theadCloneCols[i].style.width = \`\${theadCols[i].offsetWidth}px\`; } }; window.onresize = syncColsWidth; We must also sync the scroll position
const onScrollTable = () =&gt; { const wrapper = document.getElementById(&#34;table-wrapper&#34;); const clone = document.getElementById(&#34;wrapper-header&#34;); clone.scrollLeft = wrapper.scrollLeft; }; document.getElementById(&#34;table-wrapper&#34;).onscroll = onScrollTable; And with that, we have a fully fixed header for our table.
This solution adds more complexity than the CSS one, but it&rsquo;s more flexible and allows us to use gradient or image backgrounds.
`,url:"https://sergiocarracedo.es/how-to-create-a-table-with-a-fixed-header/",image:"/how-to-create-a-table-with-a-fixed-header/table-fixed-header_hu_a01cc4d5db984a91.jpg",tags:["frontend","ui"],readingTime:"4 minutes read",date:"Oct 12, 2021"},"https://sergiocarracedo.es/mis-streams-favoritos-de-twitch/":{title:"Soy casi un boomer y consumo Twitch: Mis streams favoritos",content:` Disclaimer: Este es mi primer post en castellano en bastante tiempo, el motivo es el contenido del que voy a hablar est√° mayoritariamente en castellano y ser√≠a un poco raro hacerlo en ingl√©s
Como dice el t√≠tulo, ya soy casi un boomer, lo que para muchos estereotipos me har√≠a ser el t√≠pico consumidor de televisi√≥n en mis horas de entretenimiento, pero desde unos hace 4-5 a√±os he ido reduciendo mi consumo televisivo poco a poco a pr√°cticamente 0.
No voy a entrar mucho en los motivos de este cambio, pero puedo resumirlo en: Baja calidad del contenido y exceso de publicidad
Al principio mis h√°bitos de consumo de entretenimiento se movieron a YouTube, porque YT me ofrec√≠a entretenimiento y recursos de aprendizaje. Escrib√≠ un par de posts hablando sobre mis canales favoritos: versi√≥n 2020 y versi√≥n 2018, y parte siguen estando ahi.
Cuando a principios de 2019 estuve recibiendo un tratamiento de quimioterapia , y como durante las recuperaciones ten√≠a tiempo para, entre otras muchas cosas ver entretenimiento, empec√© a descubrir Twitch a partir de que uno de los YouTubers que ya conoc√≠a tambi√©n stremeaba, y de ahi comenc√© a tirar el hilo de otros streamers.
Y ya llegado el 2020, el a√±o de pandemia y confinamientos, esto ya despunt√≥, de hecho ya no soy solo consumidor de Twitch, sino tambi√©n creador ya que lo hemos usado como plataforma de difusi√≥n de las charlas de las comunidades de t√©cnicas como en PHPVigo y especialmente LaretasGeek
Qu√® es Twitch? Para los que no conozcais Twitch (si es que queda alguien que no lo conozca a estas alturas) es una plataforma de emisi√≥n de videos en directo (streams), cada canal tiene un chat que puede interactuar con el streamer (si la mayor√≠a de los streams los gestiona una sola persona). La diferencia con otras plataformas es que los videos solo permanecen disponibles despu√©s de la emisi√≥n durante unos dias, es decir est√°n pensados para consumirse en directo.
Cualquiera de nosotros puede seguir un canal de forma gratuita (para que nos avisen cuando empiezan los streams).
C√≥mo ganan dinero en Twitch los streamer? Adem√°s te puedes subscribir al canal, lo que implica el pago, a d√≠a de hoy con los cambios que ha introducido Twitch, de 4‚Ç¨/mes en Espa√±a (de los cuales el streamer se lleva entre el 50% y 70%) y tambi√©n puedes regalar subscripciones a otros.
Normalmente los streamers usan herramientas que muestran en el stream sobreimpreso cuando alguien se suscribe o regala suscripciones a otros. Lo que refuerza el hecho de hacerse suscriptor.
Aunque la mayor√≠a de las suscripciones no tiene coste directo para los seguidores ya que si dispones de Amazon Prime, puedes hacer una suscripci√≥n gratis al mes.
Tambi√©n se pueden hacer donaciones directas (los bits)
Qu√© contenido hay en Twitch? Twitch naci√≥ como una plataforma para retransmitir partidas de videojuegos, que siguen siendo mayor√≠a, pero tambi√©n hay otros muchos tipos de contenido no relacionados con videojuegos, como &ldquo;Just Chatting&rdquo;, Donde el streamer habla con su audiencia y &ldquo;Difusi√≥n de ciencia y tecnolog√≠a&rdquo;, en realidad cualquier tipo de video en directo es susceptible the ser transmitible en Twitch, de hecho este a√±o se han transmitido partidos de futbol de la liga espa√±ola (de forma legal) y m√°s recientemiente, Ibai, ha retransmitido la copa am√©rica
Mis canales favoritos the twitch Carola: https://www.twitch.tv/carola Este streamer es el que estamos viendo m√°s ahora mismo en mi casa. Comenz√≥ principalmente jugando a GTA5 Roleplay, creando personales muy interesantes y sabiendo gestionar para mi gusto muy bien la improvisaci√≥n y la planificaci√≥n de los personales. Adem√°s es Gallego y su humor es tambi√©n muy gallego a veces y sobre todo por que nos saca una carcajada casi cada vez que lo vemos y eso vale mucho. En los √∫ltimos 2 a√±os ha hecho grupo con MenosTrece, Ricoy, y Agustabell (otros streamers) principalmente para jugar juegos de supervivencia, como Scape From Tarkov
BuckFernandez: https://www.twitch.tv/buckfernandez Este streamer y youtuber, que debe estar cercano a mi edad, es productor musical, rapero, etc. Sus streams no son tan masivos como los de otros, pero los basa en hacer un contenido de calidad que le guste a √©l y a sus habituales. Puedes encontrarlo componiendo m√∫sica, jugando a juegos retro con consolas reales (no solo emuladores), haciendo res√∫menes humor√≠sticos de pel√≠culas de serie B de los 80s
MenosTrece https://www.twitch.tv/menostrece Creo que fue de los primeros YouTubers de entretenimiento que comenc√© a seguir. En Twitch juega a solo o con otros juegadores como Carola, Agus o Ricoy, y mayoritariamente juegos como Escape from Tarkov, Rust, 7 Days to die, Day by daylight, etc
En los siguientes nos voy a entrar en tanto detalle porque los veo de forma m√°s causal
Agustabell212 https://www.twitch.tv/agustabell212 Ricoy23 https://www.twitch.tv/ricoy23 Agus y Ricoy son primos y juegas habitualmente juntos, son muy buenos en Rust
Silithurh https://www.twitch.tv/silithur Es un streamer que juega a un poco de todo, y muchas veces interactua con otros de los mencionados y tiene la capacidad de transmitir tranquilidad y relajaci√≥n.
Angel Martin https://www.twitch.tv/angelmartin Admiro su capacidad de cr√≠tica, su iron√≠a, y el hecho de que se ha sabido reinventar: viniendo del mundo de la TV y ahora ha creado un stream con bastantes seguidores y para mi gusto de muy buena calidad
Ibai https://www.twitch.tv/ibai Ibai es la revelaci√≥n de 2020, previamente casteaba (retransmitia esports, o cosas tan locas como (carreras de canicas)[https://www.youtube.com/watch?v=bJgfisVKe6w]) y a rotos records de audiencia en Twitch, por ejemplo cuando retransmiti√≥ las campanadas. Lo que me gusta principalmente son las &ldquo;entrevistas&rdquo; distendidas que hace a gente que no va a otras plataformas.
Auronplay https://www.twitch.tv/auronplay Qu√© decir de Auronplay, debe ser ahora mismo el streamer con mejores numerous en habla hispana. No soy un gran conmsumidor de todo su contenido, pero a veces lo pongo de fondo y es divertido
Outconsumer https://www.twitch.tv/outconsumer Otro streamer que parece de mi quinta, y que habla de baloncesto, otros deporte, y juega, pero siempre con un mensaje tranquilo y con transforndo educativo
IamCristinini https://www.twitch.tv/iamcristinini Es otra de las grandes del roleplay
Canales de stream t√©cnicos y de divulgaci√≥n CodelyTv https://www.twitch.tv/codelytv Poco que m√°s decir de CodelyTV que seruramente no conozcas. Aparte de la plataforma de cursos online de la que disponen, divulgan sobre tecnolog√≠a todas las semanas en Twitch para disfrute de todos nosotros. Con un contenido interesante, entretenido que genera debates y situaciones divertidas.
DovCSV https://www.twitch.tv/dotcsv Canal dedicado principalmente a noticias sobre AI y ML
LaHiperActina https://www.twitch.tv/lahiperactina Canal dedicado a la divulgaci√≥n de biomedicina
Alva Majo https://www.twitch.tv/5ro4 Dedicado a hablar de la creaci√≥n de video juegos con un estilo muy particular y mucha sorna
Salva Spin https://www.twitch.tv/salvaespin Salva es un dibujante profesional, trabaja para DC dibujando a Deathpool, en algunos de sus steams muestras como trabaja realizando los dibujos
Hay otros muchos streamers que sigo y otros que conozco y que he visto muy puntualmente como puede ser elxokas , elisawavess, gtv_genesis , sarinha_3, Little Mary , JuanGuarnizo, etc&hellip;
Podemos dejar para otro post la opini√≥n sobre si el contenido es siempre es el adecuado para audiencias de ciertas edades. Spoiler: No siempre, como en todas las plataformas, por desgracia hay contenido t√≥xico o ejemplo que en mi opinion no son buenos.
Espero que os gustase mi primer post en castellano, que como veis fu√© publicado en agosto un mes en el que mayoritariamente todos estamos pensando en las vacaciones, de ahi que no escribiese nada t√©cnico.
Si queries compartir alg√∫n canal que conozcas o que te guste no dudes en dejarlo en los comentarios.
`,url:"https://sergiocarracedo.es/mis-streams-favoritos-de-twitch/",image:"/mis-streams-favoritos-de-twitch/cover_hu_3a7179ef8b3319fa.jpg",tags:["streaming","twitch","lifestyle"],readingTime:"6 minutes read",date:"Aug 9, 2021"},"https://sergiocarracedo.es/act-how-to-run-github-actions-in-local/":{title:"How to run GitHub Actions in local with Act",content:`GitHub Actions is a powerful tool, you can run tests, build your app, deploy it, even mining bitcoins &#x1f614;
When you are creating an action or workflow in GitHub Actions, it is very common the needing of test it, check if all works as you need and if something is not correct, fix it, in other works as other software GitHub Actions need to be iterated.
Doing it in GitHub have disadvantages:
Waste running time: even if you are using a free plan you have a 2000 minutes per month limit Fill your repository history with &ldquo;trash&rdquo; commit: As the action definition is in the repo, every change is a new commit, if you are testing something that is very common to fill your repo history with the commits of every change. You can squash these commits into one, but is an extra step Is slow ACT ACT comes to help us, this tool allows us to run GitHub Actions on our local computer.
It creates an environment equal to GitHub provides, uses Docker images to run the actions, the environment variables and filesystem are all configured to match what GitHub provides.
Install Act is available for Linux, Windows and MacOS, the installation instructions (and dependencies) depend on your OS, I&rsquo;m going to focus on Ubuntu, but you can check the official docs to know how to install in your OS.
In Ubuntu, you need Go 1.16+ as a dependency and run
go install github.com/nektos/act@latest Is very useful to add the Go bin path to your PATH For example, if you are using zsh as shell
echo &#34;export PATH=$PATH;~/go/bin/&#34; &gt;&gt; ~/.zshrc Running your action Running an action on your computer is simple as go to your repo root folder and run:
act In the first run, Act will ask you to choose the default image to run the action. The image to choose depends on your action, but I recommend you to choose, the medium image, because the micro image for example can&rsquo;t install Python
If your action needs &ldquo;secrets&rdquo; will fail. We must set the secrets and that is easy as pass an argument to act
act -s GITHUB_TOKEN=&lt;your_token&gt; -s OTHER_SECRET=&lt;value&gt; It&rsquo;s important to bear in mind that GitHub always injects the GITHUB_TOKEN secret, but in our local environment, we must provide the value. If your action needs this value (for example, after build, deploy to GHPages) you must provide it. To create a token in GitHub you only need to navigate to Settings &gt; Developer settings &gt; Personal access tokens or just https://github.com/settings/tokens
Dispatching custom events By default, act runs the &ldquo;on: push&rdquo; configured action in your workflow file, but maybe you want to run another workflow depending on other triggers, and you can, only need to pass the event name as an argument
act pull_request act workflow_dispatch act release ... Use a specific workflow Sometimes you have more than workflow files, usually, they don&rsquo;t run on the same dispatcher, but anyway, you can set the workflow file to use.
act -W &lt;path to workflow file&gt; Listing the actions Running act -l or act release -l you can list the action that will run
Draw workflow Sometimes can be interesting to see the action&rsquo;s dependency tree, using the -g flag you will get an exit like that
Run a specific job If you want to test a specific job into the workflow you can pass the job name using -j argument, for example
act -j deploy There are more flags. You can check the full list here
GitHub Enterprise Act can login to private GitHub Enterprise servers as simple as add --github-instance &lt;your-company-ghe-server&gt; in the command
Summary Act it&rsquo;s a wonderful tool to run GH Action in local when you are creating or iterating it, avoiding using the real repo to test it. But Act is something more, you can also use it as a local task runner using all the power, and the actions in the GitHub Actions marketplace to create your local task, and this task can easily move to the cloud if you need it.
`,url:"https://sergiocarracedo.es/act-how-to-run-github-actions-in-local/",image:"/act-how-to-run-github-actions-in-local/act-jake-givens-iR8m2RRo-z4-unsplash_hu_d6f2961ff74bdee2.jpg",tags:["devops","act","github"],readingTime:"4 minutes read",date:"Jun 14, 2021"},"https://sergiocarracedo.es/vue-use-model-helpers/":{title:"Simplifying the use of custom a Vue v-model",content:`One year ago I wrote a post about how to manage your custom v-model in your component (Spanish).
The problem still being the same, if you try to mutate the value of the property into the component you will get this error message:
Error message: Avoid mutating a prop directly since the value will be overwritten whenever the parent component re-renders. Instead, use a data or computed property based on the prop‚Äôs value.
That is normal, because the correct way of update a property value in the parent component is emit an event, for example: emit('input', newValue).
I think the most convenient way to manage this situation is to create a local copy of the property in the component and observe its changes, and then emit the event, we also must observe property changes to keep the local copy updated if the parent component changes the property value.
This requires writting repetitive code for every single property (remember that in Vue 2.x we can use the .sync modifier to make other properties distinct of value 2-way bound)
To simplify my life (and yours) I created and published in npmjs a package that takes advantage of Vue composition API to make the code more reusable.
The package is vue-use-model-helpers
IMPORTANT The package works on both Vue 2.x and Vue 3, the way of using it is the same, but you must use the correct package version.
For Vue 2.x:
npm i vue-use-model-helper@2.x --save # or yarn add vue-use-model-helper@2.x For Vue 3:
npm i vue-use-model-helper@3.x --save # or yarn add vue-use-model-helper@3.x Usage This package encapsulates the logic of creating the local property&rsquo;s copy, the watchers to observe the property and the local value, and the event dispatch.
In your components must import the useLocalModel helper function:
import { useLocalModel } from &#39;content/blog/2021/vue-use-model-helpers/index&#39; Then you must pass an array with the names of the properties you want to manage because, yes, in Vue 2.x the helper can manage .sync. You don&rsquo;t need to take care of the event name, the helper can recognize the property type and emits the correct event.
The helper returns a copy of every property with the name local + [property name capitalized] as a ref.
You can use destructuring to get the copies: const { localValue, localUsername } = useLocalModel(['value', 'username'])
Putting all together:
import { useLocalModel } from &#39;content/blog/2021/vue-use-model-helpers/index&#39; export default defineComponent({ ... props : { value: String, username : String } , setup(props) { const { localValue, localUsername } = useLocalModel([&#39;value&#39;, &#39;username&#39;]) return { localValue, localUsername } } ... }) If, for example, you are using the localValue in an input in your component, every single time the user updates the input, the helper emits the event on behalf of you.
I think this helpers package simplifies the component readability and allow you to write less repetitive code.
Any comments to improve the package are highly welcome!
`,url:"https://sergiocarracedo.es/vue-use-model-helpers/",image:"/vue-use-model-helpers/use-model-josh-riemer-OH5BRdggi2w-unsplash_hu_3569ed820b085f8f.jpg",tags:["vue"],readingTime:"3 minutes read",date:"May 10, 2021"},"https://sergiocarracedo.es/tips-using-typescript-and-vue/":{title:"Tips using Typescript and Vue",content:`Typescript is a great &ldquo;language&rdquo;, makes it possible to create more maintainable and understandable software, but requires extra effort to type the variables, the functions&rsquo; arguments, etc&hellip;
Vue 2.x, and even more Vue 3 provide a great typescript integration, providing the necessary types to use your app, but not always are trivial, and you need to know the types you must use in every case.
I want to share with all of you the lessons I learned in my experience using Vue and TS, the typical questions, and the &ldquo;problems&rdquo; I found in the way.
Vuex Typing the Vuex&rsquo;s store can&rsquo;t be straightforward, my first time typing the store was frustrating because I didn&rsquo;t know types use.
State The state is a JS object, in type you can type it as a generic Record&lt;string, any&gt; but this is not nice. It&rsquo;s better creating and interface that define all the store items types, for example, imagine this store:
const store = { name: &#39;Sergio&#39;, lastLogin: new Date(2021, 0, 1, 22, 34), config: { darkTheme: true, fontSize: 23 }, friends: [{ id: 1, name: &#39;Juan&#39; }, { id: 2, name: &#39;Felipe&#39; }] } We must create an interface for this object:
interface Friend { id: number; name: string; } interface StoreState { name: string; lastLogin?: Date, config: { darkTheme: boolean; fontSize: number; }, friends: Friend[] } const store: StoreState = { name: &#39;Sergio&#39;, lastLogin: new Date(2021, 0, 1, 22, 34), config: { darkTheme: true, fontSize: 23 }, friends: [{ id: 1, name: &#39;Juan&#39; }, { id: 2, name: &#39;Felipe&#39; }] } Mutations For the mutations, Vuex provides the type MutationTree&lt;S&gt;, defined as:
interface MutationTree&lt;S&gt; { [key: string]: Mutation&lt;S&gt;; } type Mutation&lt;S&gt; = (state: S, payload?: any) =&gt; any; Basically is a map of mutation functions, as you can see, a mutation function get the state type, but the payload can be anything and return anything
const mutations: MutationTree&lt;StoreState&gt; = { setName (store, payload: string) { store.name = payload } } As the payload is defined by the type as any it&rsquo;s a good practice type your payload in every mutation function
Actions It&rsquo;s similar to the mutations, but with a peculiarity:
interface ActionTree&lt;S, R&gt; { [key: string]: Mutation&lt;S, R&gt;; } type Action&lt;S, R&gt; = ActionHandler&lt;S, R&gt; | ActionObject&lt;S, R&gt;; Without going deeper, the S is the state of the vuex module, and R is the Root State. In a simple case (without using vuex modules) S and R are the same.
Getters Same as actions,
interface GetterTree&lt;S, R&gt; { [key: string]: Getter&lt;S, R&gt;; } For example:
const getters: GetterTree&lt;StoreState, StoreState&gt; = { friendCount(store): number { return store.friends.length } } As in the store payload params, it&rsquo;s a good practice to type getter return
Composition API If you are using composition API in the setup function we can type our properties as we did in the store. Make sure you are using defineComponent instead Vue.extend to make it work
interface Props { value: boolean, title: string } export default defineComponent({ name: &#39;my-component&#39;, props: { value: Boolean, title: String }, setup (props: Props) { ... } }) You can also type the properties directly in the props entry, but as typescript interfaces don&rsquo;t exist at runtime we can&rsquo;t use the interface directly as the property type
// Doesn&#39;t work because Friend doesn&#39;t exists in the runtime { props: { friend: { type: Friend } } } // Doesn&#39;t work because Object doesn&#39;t implement Friend properties { props: { friend: { type: Object as Friend } } } But, we can pass the type as return of a function, then Vue instance the interface instances the interface and can check the value type
// Works { props: { friend: Object as () =&gt; Friend, friends: Array as () =&gt; Friend[], name: String as () =&gt; string } } Read more about that
Remember to type &ldquo;native&rdquo; types because String is not the same as string (String is an object and string is a type) More info about this in Stackoverflow
Add extra properties to Vue Component Object By default, Vue provides us a defined structure for the Vue Component Object, for example, the property data, props, etc&hellip; Using vanilla JS we can add a new property to the Vue Component Object without doing extra works, for example, we want to add a property called layout that makes our root component can use different layouts in our view.
export default { name: &#39;my-component&#39;, layout: &#39;2-cols&#39; } If we try to do this using typescript we will get an error because the property layout wasn&rsquo;t defined in the Vue Component Object. To fix it we must extend the definition creating a definition file in our src/, for example, src/typings.d.ts
# src/typings.d.ts import Vue from &#39;vue&#39; declare module &#39;vue/types/options&#39; { interface ComponentOptions&lt;V extends Vue&gt; { layout?: string; } } Add extra properties to the Vue Instance As in the previous chapter, we could want to add a new property to the Vue Instance, for example, to add a global functionality like a toast, etc: vm.$toast.open()
Remember you can do it doing something like this, for example, during the plugin installation:
Vue.prototype.$toast = { open: () =&gt; { .... } } as ToastHandler Then we must add to our definition file these lines to declare the new Vue instance properties and their types
# src/typings.d.ts import Vue from &#39;vue&#39; declare module &#39;vue/types/vue&#39; { interface Vue { $toast: ToastHandler; } } Typescript can be tough at the beginning, but gives you more confidence in your code and make it more readable, for example
{ props: { friend: Object as () =&gt; Friend, person: Object } } For example in the case of friend you only need to go to the type declarations to know the &lsquo;friendstructure and properties, even your IDE can provide you autocomplete, but forperson\` is very hard to know the object structure. I hope this post can help you using Typescript and Vue.
`,url:"https://sergiocarracedo.es/tips-using-typescript-and-vue/",image:"/tips-using-typescript-and-vue/vue-typescript-tips-rF4kuvgHhU-unsplash_hu_c3ee57984022b40a.jpg",tags:["typescript","vue"],readingTime:"5 minutes read",date:"Apr 25, 2021"},"https://sergiocarracedo.es/understanding-d3-js-introduction/":{title:"Understanding d3.js - A simple line chart",content:`Before starting is important to say that d3.js is not a chart library is a library for making charts, if you are expecting to pass the data to the library and set 4, 5,&hellip; 10 params and get the chart to render, d3.js is not your library.
d3.js is more than that, is a very flexible library to manage data (data-driven documents) and create the representation of that data (not only charts). d3.js provides you the different pieces to create your customized data visualizations (for example charts)
Let&rsquo;s start creating a simple line chart:
Scales, domain, and range Simplifying, there are two pieces in the chart: Axis and line. Let&rsquo;s talk about the axis. In the example the chart we have two axes: X and Y, but we can have more or less, depending on the chart type.
Axis is one of the reference lines of a coordinate system, every single point represents a value in the coordinate system and also a position in the canvas which is displaying the chart.
In our example, in the y-axis the &lsquo;1&rsquo; value of the chart is drawn in the position 31px from the axis start, but in SVG or canvas the origin of the coordinate system start on the top left corner, and &lsquo;y&rsquo; grows in the direction to monitor bottom. Then to print the 1 value in our chart we must use 319 - 31 (288) as y position. That&rsquo;s complicated, and can be even more, if the relation between screen coordinates and chart coordinates aren&rsquo;t lineal, for example, a logarithmic chart.
d3.js brings us a component to help us to abstract the conversion between screen coordinates and chart coordinates:
d3-scale d3-scale is our component. There are a lot of different types of scales: Continuous (Linear, Power, log, identity, time, radial), Sequential, Ordinal, etc&hellip; For our example chart, we will use Linear.
Before continuing with scale I&rsquo;m going to introduce 2 important concepts: domain and range
Domain is the complete set of values chart can use, in our case is all the values between 0 and 10
Range is the coordinates (in the screen) where the chart can draw, in this case from 0 to 319
Putting all together:
const xScale = d3.scaleLinear().domain([0, 10]).range([319, 0]) Note the range is between 319 and 0 not between 0 and 319
This returns a function that links the values in the chart (domain), and the values in the &lsquo;canvas&rsquo; (range)
xScale(0) // 319 xScale(5) // 159.5 xScale(10) // 0 Scale also provides some methods for do extra actions, for example, if you want to get the value in the chart from the &lsquo;canvas&rsquo; coordinates (the typical use case is getting the chart value in the mouse position) we can use
xScale.invert(159.5) // 5 Probably you are thinking the same as me the first time I see that: It&rsquo;s a function that has methods? WTF!. If you want you can go farther but in swallow, if the function has arguments returns the &lsquo;main&rsquo; value, if not returns an object of functions.
We can do the same for the y-axis:
const yScale = d3.scaleLinear().domain([0, 20]).range([0, 700]) Ok, we have all the tools to manage the data and the canvas, now we must render the data and the axis
Rendering the chart d3.js can render charts in SVG and Canvas but we will focus on the SVG rendering because is fast enough for most charts and makes easy interaction and styling. To render our line chart, first, we must prepare our placeholder.
&lt;html&gt; &lt;body&gt; &lt;div class=&#39;chart&#39;&gt;&lt;/div&gt; &lt;/body&gt; &lt;/html&gt; For our comfort I&rsquo;m going to define the chart margins and the width and height. The margins are necessary to render the axis, because the size of the chart refers to the draw area.
const margin = { top: 10, right: 10, bottom: 50, left: 50 } const width = 1280 - margin.left - margin.right const height = 420 - margin.top - margin.bottom const n = 20 // Number of points in x axis const maxY = 10 // Max y value Now we will use d3 to add our SVG chart
const svg = d3 .select(&#39;.chart&#39;) .append(&#39;svg&#39;) .attr(&#39;width&#39;, width + margin.left + margin.right) .attr(&#39;height&#39;, height + margin.top + margin.bottom) Line by line:
.select('.chart') selects the DOM element previously we defined .append('svg') appends a &lt;svg&gt; element to the &lt;div class='chart'&gt; .attr('width', width + margin.left + margin.right) adds an width attribute to the &lt;svg&gt; element .attr('height', height + margin.top + margin.bottom) adds an hight attribute to the &lt;svg&gt; element We defined the scales above, but we will do it again, now using our variables to make the chart more reusable
const xScale = d3.scaleLinear().domain([0, n - 1]).range([0, width]) const yScale = d3.scaleLinear().domain([0, maxY]).range([height, 0]) Rendering the x-axis To render the X axis we will create a new SVG group that holds the axis
svg .append(&#39;g&#39;) .attr(&#39;class&#39;, &#39;x axis&#39;) .attr(&#39;transform&#39;, &#39;translate(&#39; + margin.left + &#39;,&#39; + (height + margin.top) + &#39;)&#39;) .call(d3.axisBottom(xScale)) Line by line:
.append('g') appends the new group for the axis .attr('class', 'axis x-axis') adds 2 classes to the group (this allows us to style it using CSS) .attr('transform', 'translate(' + margin.left + ',' + height + ')') moves the axis group bellow the chart draw area and gives space .call(d3.axisBottom(xScale)) call the d3.axisBottom function. This function is in charge of rendering the axis, the axisBottom means that the ticks of the axis will be rendered bellow the axis line. There are 3 more functions: d3.axisTop, d3.axisLeft, d3.axisRight to render the axis in different orientations. About the call method: This method call the function passed as argument and uses as first argument for the function of the selected element. In this case is the same as execute: d3.axisBottom(xScale)(svg.select('.xaxis')) (Remember that d3.axisBottom returns a function). The advantage of using call is you can concatenate methods because call returns the selection, not the result of the function
After that we can see our x-axis &#x1f389;
Rendering the y-axis Rendering the Y axis is almost the same as X axis:
svg .append(&#39;g&#39;) .attr(&#39;class&#39;, &#39;y axis&#39;) .attr(&#39;transform&#39;, &#39;translate(&#39; + margin.left + &#39;,&#39; + margin.top + &#39;)&#39;) .call(d3.axisLeft(yScale)) Rendering the line Before do the chart rendering we need the data, in this case, we will use random values
const dataset = d3.range(n).map((d) =&gt; { return { x: d, y: d3.randomUniform(yMax)() } }) /* [ { x: 0, y: 2.679771859053788 }, { x: 1, y: 5.447777017888336 }, ... { x: 19, y: 0.083980807899251 } ] */ Now we will create the line generator that is a function that returns a
const line = d3 .line() .x((d) =&gt; xScale(d.x)) .y((d) =&gt; yScale(d.y)) Line by line:
.line() is the basic line generator .x((d) =&gt; xScale(d.x)) function that for every point in data set returns the x position in the draw area, this is the reason why we are using the scale function. d represent every dataset point .y((d) =&gt; xScale(d.y) same as previous line but referred to y-axis We can also add an extra call to the line generator to configure the interpolation behavior, for example: .curve(d3.curveMonotoneX) that makes the curve softer. But for now, a simple interpolation (linear) is enough.
Ok, now we have the line generator, and it&rsquo;s time to draw the line in our chart.
First, we add a SVG group for the line we will draw
const lineWrapper = svg .append(&#39;g&#39;) .attr(&#39;transform&#39;, &#39;translate(&#39; + margin.left + &#39;,&#39; + margin.top + &#39;)&#39;); Line by line:
.append('g') appends a SVG group element (g) to the svg element. This group will contain the chart draw area .attr('transform', 'translate(' + margin.left + ',' + margin.top + ')') set the transform attribute to move the group, this makes easy managing the draw, because we don&rsquo;t need to take care of the real position, for the draw area the coordinate system starts on (0, 0) And the render the line
lineWrapper .append(&#39;path&#39;) .datum(dataset) .attr(&#39;class&#39;, &#39;line&#39;) .attr(&#39;d&#39;, line) Line by line:
.append('path') appends a path element (to draw the line) .datum(dataset) assigns the dataset to the path element (we will use it in the next lines) .attr('class', 'line') add the class line to the path element to make asy the css styling .attr('d', line) add the d attribute, to generate it class to the line generator passing the element, and the datum We used datum because data is static, but d3 provides other methods to link dynamic data to the dom elements, but I will talk about that in the future
And this is all for the moment. d3 is a powerful tool but needs you to know some concepts before starting to create charts. The example we did in this post is very, very simple but I think is a good starting point.
My idea is to write more post about d3 going deeper into the current concept and how to configure and customize the current components (for example configure the axis, the labels, etc) `,url:"https://sergiocarracedo.es/understanding-d3-js-introduction/",image:"/understanding-d3-js-introduction/d3js_intro_hu_c082707e07eb007d.jpg",tags:["d3","js"],readingTime:"7 minutes read",date:"Apr 19, 2021"},"https://sergiocarracedo.es/custom-svg-icon-set-management-with-vue/":{title:"Custom SVG icon set management with Vue",content:`Manage an icon set can seem a simple task but can be tricky.
First, there a lot of ways to use an icon on a web page, all with pros and contras:
Let&rsquo;s use this SVG image created by Anu Rocks for all examples.
We will assume our icon set is monochrome.
&lt;img&gt; tag The simplest way of adding an icon is using the &lt;img&gt; tag as the other images.
Example: &lt;img src=&quot;/i/example.svg&quot; alt=&quot;Battery&quot;&gt;
You can also use the tag &lt;object&gt; with same result: &lt;object type=&quot;image/svg+xml&quot; data=&quot;/i/example.svg&quot;&gt;&lt;/object&gt;
Advantages: Simple method Image can be cached by the browser: if you use in multiple places, only need to download once Good usability using the alt attribute Disadvantages: Lost all SVG styling by CSS (Still using style over the image tag, but for example, you can&rsquo;t change the color of the stroke or ) Lost all the possibilities of the SVG&rsquo;s DOM manipulation CSS Another way to insert an icon on your page is creating an HTML placeholder and use the image as a background
HTML:
Lorem ipsum dolor &lt;span class=&#34;icon icon-battery&#34; aria-label=&#34;Battery&#34; title=&#34;Battery&#34;&gt;&lt;/span&gt; 50% CSS:
.icon { display: inline-block; width: .5em; height: .5em; } .icon-battery { background: url(/i/example.svg) no-repeat center center; background-size: contain; } Note the use of .5em as width and height to create a container width a size relative to container font size, .5em is better than 1em, because 1em uses the full size, but you can play with the value.
This method is very similar to the previous one in terms of advantages and disadvantages. You can also insert text into the span tag and
Font This method requires some extra work to convert SVG to font. It could be done online:
Once we have the icons as font, we must import the font
@font-face { font-family: &#39;My icon set font&#39;; font-style: normal; font-weight: normal; font-display: auto; src: url(&#34;../webfonts/my-icon-set.eot&#34;); } To use the icons we can use the glyph associated with an icon, for example, our battery icon can be the A
Lorem ipsum dolor &lt;span style=&#34;font-family: &#39;My icon set font&#39;&#34; aria-label=&#34;Battery&#34;&gt;A&lt;/span&gt; 50% There is another way: use the :before pseudo selector and the content property in your CSS to insert the font glyph
.icon { font-family: &#39;My icon set font&#39;; } .icon-batery:before { content: &#39;A&#39; } Lorem ipsum dolor &lt;i class=&#34;icon icon-battery&#34; aria-label=&#34;Battery&#34;&gt;&lt;/i&gt; 50% Advantages It&rsquo;s very easy changing the icon color, the icon inherits the context color Font can be cached Disadvantages Requires extra work to convert the icons to fonts Add a new icon requires to update the font No tree-shaking, unused icons still there Is confusing for screen readers, because we are inserting text Sometimes align with the regular text fonts isn&rsquo;t good Note that is possible define a font using just SVG, but this method is not fully supported by commonly used browsers: https://caniuse.com/svg-fonts
Inline SVG This is my favorite method in most cases. The method consists in inserting the SVG markup in your HTML.
In our example
Lorem ipsum dolor &lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34;&gt; &lt;path fill-rule=&#34;evenodd&#34; d=&#34;M17,5 C18.5976809,5 19.9036609,6.24891996 19.9949073,7.82372721 L20,8 L20,8.17 L20.1933113,8.24671351 C21.1458614,8.66012858 21.8418803,9.55339202 21.9763495,10.6214854 L21.9949073,10.8237272 L22,11 L22,13 C22,14.2181391 21.2716631,15.274045 20.201161,15.7433631 L20,15.822 L20,16 C20,17.5385075 18.8418794,18.8065215 17.3498634,18.9798168 L17.1762728,18.9949073 L17,19 L5,19 C3.40231912,19 2.09633912,17.75108 2.00509269,16.1762728 L2,16 L2,8 C2,6.40231912 3.24891996,5.09633912 4.82372721,5.00509269 L5,5 L17,5 Z M17,7 L5,7 C4.48716416,7 4.06449284,7.38604019 4.00672773,7.88337887 L4,8 L4,16 C4,16.5128358 4.38604019,16.9355072 4.88337887,16.9932723 L5,17 L17,17 C17.5128358,17 17.9355072,16.6139598 17.9932723,16.1166211 L18,16 L18,15 C18,14.5004355 18.3670085,14.0840077 18.8498945,14.0112465 L18.9632725,14.0006747 L19.075685,13.9972247 C19.5546159,13.9618905 19.9369487,13.5888432 19.9929352,13.1192658 L20,13 L20,11 C20,10.4871642 19.6139598,10.0644928 19.1166211,10.0067277 L18.8833789,9.99327227 C18.424297,9.93995063 18.0600494,9.57570299 18.0067277,9.11662113 L18,9 L18,8 C18,7.48716416 17.6139598,7.06449284 17.1166211,7.00672773 L17,7 Z M11,9 C11.5522847,9 12,9.44771525 12,10 L12,14 C12,14.5522847 11.5522847,15 11,15 L7,15 C6.44771525,15 6,14.5522847 6,14 L6,10 C6,9.44771525 6.44771525,9 7,9 L11,9 Z&#34; fill=&#34;currentColor&#34;/&gt; &lt;/svg&gt; 50% Note the use of currentColor for the fill attribute to inherit the fill color from the CSS context (color)
Advantages It&rsquo;s very easy changing the icon color, the icon inherits the context color Even, if your icon is not monochrome you can change the color of every single path. Can manipulate SVG elements independently Icons can be animated (not only a doing a transform of the full icon, but you can also manipulate every path independently) Disadvantages Non-cachable. If the icons appear in several places you must insert the full code. In static sites adds a lot of &ldquo;bytes&rdquo; to the HTML. A change in your icon, require to find and replace on all icon occurrences. Encapsulated inline SVG We can improve the previous method using Vue (or another framework: React, Angular, etc, or just using web components). The idea is encapsulated every single icon in a Vue component:
&lt;template&gt; &lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; :width=&#34;size&#34; :height=&#34;size&#34; viewBox=&#34;0 0 24 24&#34;&gt; &lt;path fill-rule=&#34;evenodd&#34; d=&#34;M17,5 C18.5976809,5 19.9036609,6.24891996 19.9949073,7.82372721 L20,8 L20,8.17 L20.1933113,8.24671351 C21.1458614,8.66012858 21.8418803,9.55339202 21.9763495,10.6214854 L21.9949073,10.8237272 L22,11 L22,13 C22,14.2181391 21.2716631,15.274045 20.201161,15.7433631 L20,15.822 L20,16 C20,17.5385075 18.8418794,18.8065215 17.3498634,18.9798168 L17.1762728,18.9949073 L17,19 L5,19 C3.40231912,19 2.09633912,17.75108 2.00509269,16.1762728 L2,16 L2,8 C2,6.40231912 3.24891996,5.09633912 4.82372721,5.00509269 L5,5 L17,5 Z M17,7 L5,7 C4.48716416,7 4.06449284,7.38604019 4.00672773,7.88337887 L4,8 L4,16 C4,16.5128358 4.38604019,16.9355072 4.88337887,16.9932723 L5,17 L17,17 C17.5128358,17 17.9355072,16.6139598 17.9932723,16.1166211 L18,16 L18,15 C18,14.5004355 18.3670085,14.0840077 18.8498945,14.0112465 L18.9632725,14.0006747 L19.075685,13.9972247 C19.5546159,13.9618905 19.9369487,13.5888432 19.9929352,13.1192658 L20,13 L20,11 C20,10.4871642 19.6139598,10.0644928 19.1166211,10.0067277 L18.8833789,9.99327227 C18.424297,9.93995063 18.0600494,9.57570299 18.0067277,9.11662113 L18,9 L18,8 C18,7.48716416 17.6139598,7.06449284 17.1166211,7.00672773 L17,7 Z M11,9 C11.5522847,9 12,9.44771525 12,10 L12,14 C12,14.5522847 11.5522847,15 11,15 L7,15 C6.44771525,15 6,14.5522847 6,14 L6,10 C6,9.44771525 6.44771525,9 7,9 L11,9 Z&#34; fill=&#34;currentColor&#34;/&gt; &lt;/svg&gt; &lt;/template&gt; &lt;script&gt; export default { name: &#39;battery-icon&#39;, props: { size: [String, Number] } }) &lt;/script&gt; I added the size property to make the icon resizable without need CSS, and as you know icon aspect ratio you only need one dimension. If the icon aspect ratio weren&rsquo;t 1:1, you only need to do the math in the component:
For example a icon with a 16:10 aspect ratio (Using height as the base dimension):
&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; :width=&#34;size * 15 / 24&#34; :height=&#34;size&#34; viewBox=&#34;0 0 24 15&#34;&gt; You must create a component for every icon, and insert in your page is simple, just insert the component
Lorem ipsum dolor &lt;battery-icon size=&#34;11&#34;&gt; 50% With this method, you can encapsulate the icon markup, and his logic (for example you can add a property to set the color of the bar in the battery icon, or even the bar size, creating a dynamic icon)
If your page is not an SSR page you skip the cache disadvantage because your icon markup is not repeated in your code (but it&rsquo;s in the browser memory)
Also, the disadvantage of replacing all the occurrences is fixed with this method.
Using this method, unused icons don&rsquo;t are bundled in the build because your bundler (for example webpack) do the threeshaking before build.
One nice example of project using this method is https://www.npmjs.com/package/vue-material-design-icons, a Vue wrapper for the great project https://materialdesignicons.com/
`,url:"https://sergiocarracedo.es/custom-svg-icon-set-management-with-vue/",image:"/custom-svg-icon-set-management-with-vue/svg-iconset_hu_e4e2758bb0f1598e.jpg",tags:["svg","ui"],readingTime:"6 minutes read",date:"Apr 1, 2021"},"https://sergiocarracedo.es/azure-static-web-apps/":{title:"Deploying a static website in Azure Static Web Apps",content:`Nowadays JAMStack is trendy because a lot of reasons: performance, security, decoupling, etc.
JAMStack consists of pre-rendering your website or application&rsquo;s frontend and get the dynamic data from an API.
Your pre-rendered markup can be host in a server, but as it is static, your host only needs to serve static files, just HTML, CSS, and JS. You don&rsquo;t need to run any code on the server.
There are a lot of different options to deploy a static website, for example:
Github Pages Netlify Vercel etc. In this post I will talk about Azure Static Web Apps, this is a service to host your static (and lambda functions)
At the moment of writing this post, it&rsquo;s in a preview, and it&rsquo;s free.
How to deploy your app, for example, a Vue app.
Go to https://portal.azure.com/ and create a new account if you haven&rsquo;t one yet.
Into the resource group click in + Create resource
Choose Static Web App (preview)
You must select the subscription, the Resource group to assign this resource, the name of your app (to find it later in the Azure panel), the region where you want to serve the static files (choose one near your clients or users)
You need to link with your Github account and choose the repo (and the branch) to deploy.
Azure will create behalf you a Github Action to build your website and deploy it to SWA.
Something like this
name: Azure Static Web Apps CI/CD on: push: branches: - main # This action will run on push to main jobs: build_and_deploy_job: if: github.event_name == &#39;push&#39; runs-on: ubuntu-latest name: Build and Deploy Job steps: - uses: actions/checkout@v2 with: submodules: true - name: Build And Deploy id: builddeploy uses: Azure/static-web-apps-deploy@v0.0.1-preview with: azure_static_web_apps_api_token: \${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN }} repo_token: \${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments) action: &#34;upload&#34; app_location: &#34;/&#34; # Path to your app in the web server api_location: &#34;api&#34; # Api source code path - optional output_location: &#34;dist&#34; env: VUE_APP_MY_VARIABLE: value # Environment variables needed to build your app For more information about the actions you can read the official documentation: https://docs.microsoft.com/en-us/azure/static-web-apps/github-actions-workflow
After doing this the Github action will run after every push to master, build the app, and deploy to azure.
Azure provides you an url to access your website, like *.azurewebsites.net, you can also add your custom domain.
Configuring routes If you need to configure the routes, for example securing a route to allow only access to your company users, add a fallback route, or create a redirect you need to create a file named routes.json which is in the root directory after the build, in Vue you must store this in the static folder.
For example, this file forces users to must authenticated to access to any route and if the page requested is not found redirects to 200.html
{ &#34;routes&#34;: [ { &#34;route&#34;: &#34;/*&#34;, &#34;allowedRoles&#34;: [&#34;authenticated&#34;] } ], &#34;platformErrorOverrides&#34;: [ { &#34;errorType&#34;: &#34;NotFound&#34;, &#34;serve&#34;: &#34;/200.html&#34;, &#34;statusCode&#34;: 200 } ] } Mor info about routes.json https://docs.microsoft.com/en-us/azure/static-web-apps/routes
I didn&rsquo;t mention you can use lambdas to run server-side code (it is not free), but maybe I will write more about that in the future.
To summarizing, Azure Static Web Apps is another option to deploy static web apps, if you are using Azure in your company can be a good option to keep all the infrastructure services on the same platform, and it&rsquo;s a simple alternative to deploy an App Service
`,url:"https://sergiocarracedo.es/azure-static-web-apps/",image:"/azure-static-web-apps/azureSWA_hu_ce4acdb52799d19b.jpg",tags:["devops","azure"],readingTime:"3 minutes read",date:"Jan 20, 2021"},"https://sergiocarracedo.es/2020-in-a-nutshell/":{title:"2020 in a nutshell",content:`2020 is almost complete, and it was a year that we will remember for a long time. I don&rsquo;t want to be dramatic, but 2020 left a deep mark on all of us.
Profesional In the professional terrain, in January I started a new job as a pure frontend guy in a big company. This job gives to me a big opportunity of work with amazing people. This job forces me to speak in English most of the time, this is the reason why I started to write this blog in English. It was a challenge and still being, but I&rsquo;m always very happy to be challenged. In November, I moved to a new job an awesome professional opportunity, but a great opportunity to be part of a project whose goal is to help others and to make a better and more equitable world.
Community This year was a bad year for the developer groups. On February 27th, we did our last face-to-face event in PHPVigo, and we had to make the tough (but right) decision to cancel the PulpoCon 2020, but after crazy months we started to work hard to make it possible to do online events, inaugurating the PHPVigo&rsquo;s Twitch channel. I also began to help a little bit in other groups.
Talks Despite the bad year for the face-to-face meetings I was able to do a few talks:
New features in ES2020 in https://www.youtube.com/watch?v=ziZO5KQM_KU&t=5248s Creating your own Vue UI components library: From scratch to NPM: https://www.youtube.com/watch?v=z_K5iuSjCDo Both in Spanish. I also did two internal talks in my old company (in Spanish too). One of my goals for the near future was to do a talk in English, and I achieved this on December, in an company internal talk. I&rsquo;m very proud of that because one year ago I almost didn&rsquo;t speak English.
Open source This year I created 2 new open-source projects.
OBS stream widgets, When we started to do online talks, we and other groups needed to show information on the screen (labels, titles, countdown, etc), and run the final contest to give participants some free licenses and other goodies. This project wrote in JS, is a set of tools to do that. Gandi-ddns-node I needed to update a subdomain with my local (dynamic IP), this is the reason why I wrote this script which uses Gandi.net API to update a domain or subdomain if your local IP changes. I also did a few PR to opensource projects, fixing bugs, or adding new features, my little contribution to the open-source world.
Personal I learned a lot of things this year: new frameworks, tools, etc. I also started programming in a new language with a different paradigm to the other languages I knew: GoLang.
I also made new teammates, friends, and colleges, I fly for the second time in my life &#x1f605;
I&rsquo;m sure I forget a lot of things in this 2020&rsquo;s summary but in general terms, regardless of the pandemic, this was a good year.
I wish 2021 will be the year we will forget the pandemic thanks to the vaccines, science, and great health professionals.
Happy 2021!
`,url:"https://sergiocarracedo.es/2020-in-a-nutshell/",image:"/2020-in-a-nutshell/pexels-cottonbro-3401900_hu_6aa0da2db8b9aef4.jpg",tags:["lifestyle","streaming"],readingTime:"3 minutes read",date:"Dec 30, 2020"},"https://sergiocarracedo.es/blog/2020/safari-svg-filters-memory-leak/":{title:"Webkit (Safari) memory leak using SVG filters",content:`The context A few weeks ago I was developing a website using NuxtJs (which is not important for the problem but the is the context &#x1f609; ). That was a website that needs to use a few images as building blocks, the typical section separator with a shape different to a simple line, in this case, the section&rsquo;s separator had 2 colors and a shadow.
I decided to use SVG images for several reasons, for example the images had to adapt to different screen widths, and the images were simple (more complex than a line but simple, They were similar to 2 waves with 2 colors). The SVG files are very useful in these use case, because the file size is small, and you can scale them infinitely without lost definition.
One important thing in this issue as we&rsquo;ll see later is that this image had a shadow.
The problem Well, I was almost done templating the website, and we started to test the website in different browsers and platforms, and everything was fine, except because Safari users complain about the browser displays a notice:
This webpage is using significant memory. Closing it may improve the responsiveness of your Mac.
As side effect, the website animations were not fluid.
Reproducing the problem and fixing it I needed to test the website by myself to find the reason of this issue. I don&rsquo;t own a Mac computer, but I have a BrowserStack account.
BrowserStack is a SaaS that allows you to connect, using your browser, to remote devices with other browsers / OS. You can even select old browser versions, a mobile device (like iPhones, etc)
Off-Topic: BrowserStack supports my open source project SirenoGrid
So, with my BrowserStack account, I could try to reproduce the problem. I open a Safari 13.1 (lastest version) instance in MacOS Catalina, and opened the website, and using the devtools got info about memory usage, and it was over 1.4 GB!!!! And if you reloaded the website memory usage grows until the system got unresponsive.
The first thing I thought is that the problem was the animations. I remove them: Nothing changes.
I remove javascript: Nothing &#x1f615;
I continue removing things and trying other things to understand the problem. No results &#x1f62e;
After a long time, I was already a little desperate I disabled all website content blocks, and I realized that when it was a block with an SVG image in the screen, the memory usage was high.
I tried the same for a block with a png image and memory still normal.
WTF?? It should be SVG images.
Then, I replaced all SVG images with PNG versions and website works with reasonable memory usage &#x1f389;
Isolating the issue After fixing the problem and with more time I wanted to know because SVG files were causing this high memory usage.
I searched on Google information for issues in Safari and SVG and I didn&rsquo;t find anything that fit my issue, some diffuse references.
I created a sandbox to isolate the issue, I will not go into detail, but after some test and fail iterations I realized that the issue was related to SVG Filters.
With this information, I repeated the Google search, and I found information relative to Safari, SVG and filters issues 1, 2 (Some of them are very old), and the most similar: https://github.com/mapbox/mapbox-gl-js/issues/7476
The bug If you open in Safari a normal webpage, for example: https://apple.com which is a webpage with images, javascript, animations, video, etc. The Safari Timeline inspector says the memory usage is around 205MB
I prepared a bug demo codesandbox, just with 2 simple svg images:
https://5emtw.csb.app &#x2757; Open with responsibility in Safari &#x2757;
And in the first load that is the memory usage is around 600 MB!!!! &#x1f92f; That&rsquo;s crazy
And if you reload the page a few times the situations will be worse
After 3 reloads the memory usage is 1.26 GB, absolutely crazy.
The same page, and the same images, but without filters
https://5emtw.csb.app/nofilters.html
The memory usage is only 21 MB, that it&rsquo;s a very normal memory usage
Even with a simple filter, like:
&lt;feFlood flood-opacity=&quot;1&quot; result=&quot;BackgroundImageFix&quot;/&gt;
The memory starts to go high, I&rsquo;m not sure what cause the problem, but in my opinion is something related to composition, I guess the browser keeps in memory the raw result of applying a filter and after making all the compositions layers .
I reported that bug in the Webkit Bugzilla page: https://bugs.webkit.org/show_bug.cgi?id=218422 I hope they fix it soon.
`,url:"https://sergiocarracedo.es/blog/2020/safari-svg-filters-memory-leak/",image:"/blog/2020/safari-svg-filters-memory-leak/cover_pixbay_1807581_hu_e1240281a9db8c86.jpg",tags:["safari","bug","browserstack"],readingTime:"4 minutes read",date:"Nov 2, 2020"},"https://sergiocarracedo.es/blog/2020/creating-your-own-vue-ui-components-library-from-scratch-to-npm/":{title:"Talk: Creating your own Vue UI components library: From scratch to NPM",content:`Past September 29th I did a talk about how to create your own Vue UI components.
I explained some tips about how to organize the components&rsquo; code, talked in deep about how to set the values in package.json and their meaning. And how to publish on NPM and then how to use them in another project.
The talk was in Spanish, but slides are in English.
`,url:"https://sergiocarracedo.es/blog/2020/creating-your-own-vue-ui-components-library-from-scratch-to-npm/",image:"/blog/2020/creating-your-own-vue-ui-components-library-from-scratch-to-npm/vue-components-talk_hu_77b5fdccc80522a7.jpg",tags:["vue","npm","components"],readingTime:"1 minute read",date:"Oct 19, 2020"},"https://sergiocarracedo.es/automate-project-startup-with-yarn-create/":{title:"Automate the startup of your projects with yarn create and SAOjs",content:`Start a project in any language, framework, etc it&rsquo;s not as simple as it seems. I mean, usually, the project requires a lot of configuration files, installing dependencies, set the environment, create skeleton files, etc.
Talking about JavaScript frameworks: everyone has his own CLI tool to start a project with the required files to start to code.
For example Angular has @angular/cli and you could execute ng new [app-name]or Vue has @vue/cli and you can start a Vue project executing vue create [project name].
This is nice, but usually, the default config is not complaining your development&rsquo;s requisites you still must editing the config files to set up the project according to your needs, for example, change tsconfig.json configuration, more advanced, you would like to add some code in router file or you would like to create a folder structure for the store, or anything.
Do all those tasks every time you start a project is boring and spend time.
In the background, these CLI tools are using a skeleton repo or similar to prepare your project, but thinking in a generic use case.
So we can do the same but adapting this use case to our use case.
yarn create (or npx create) Yarn is a node package manager with vitamins, and provides the command create
$ yarn create [my-starter-kit] You only need to have installed yarn, nothing else. This command gets from your npm registry a package named create- + the name you use in the command, in our example tries to get the package create-my-starter-kit
Then, yarn reads the package.json file of the package, installs the dependencies, and run the command in the bin entry
// package.json { &#34;name&#34;: &#34;create-my-starter-kit&#34;, &#34;version&#34;: &#34;1.0.0&#34;, &#34;bin&#34;: &#34;lib/cli.js&#34;, ... } yarn create won&rsquo;t do anything else
You must create the next steps, but, think about it, the user (you, a teammate, a user of an open-source project) only need yarn as a dependency.
In your cli.js you can do things to prepare the development environment by yourself. But, let&rsquo;s see how to simplify all these tasks.
SAOjs SAO describes itself as a Futuristic scaffolding tool. and it&rsquo;s inspired in Yeoman
These tools allow you to ask the user who runs your create-app questions that you could use to make decisions to install or configure different elements.
You must create the file saofile.js, and this file must export this elements:
module.exports = { templateData: ..., prompts: ..., actions: ..., prepare: ..., subGenerators: ..., completed: ... } Prompts Let&rsquo;s talk about prompts. That must return an array of questions to want to do to the user, for example
prompts: [ { type: &#39;input&#39;, name: &#39;projectName&#39;, message: &#39;What is your project name?&#39; }, { type: &#39;list&#39;, name: &#39;packageManager&#39;, message: &#39;Your favourite package manager&#39;, choices: [&#39;yarn&#39;, &#39;npm&#39;] } ] When you run SAO (we&rsquo;ll see how to do soon), it&rsquo;ll ask the user the questions.
The &ldquo;prompts&rdquo; system is very useful, you can ask questions only if a previous question is has some value, etc.
You can also save some answers as preset for the next projects (Like Vue CLI does)
More information about prompts
Actions The other important key in the saofile&rsquo;s object is the actions. They define the actions to do. The important thing is that the actions can be conditioned by the user answers.
Add You can copy files from a templateDir (in your create package) to the target (the project to create). That&rsquo;s that we needed ;)
{ type: &#39;add&#39;, files: &#39;**&#39;, templateDir: &#39;/template&#39;, ... } You could even filter the files to add depending on the user&rsquo;s answers More info
Move Moves files in the target (the project to create)
Modify Modify files in the target (the project to create), that is very interesting too, for example, for updating config files according to what the user answered
const packageName = this.packageName { type: &#39;modify&#39;, files: &#39;package.json&#39;, handler: (data) { data.name = packageName return data } } In the example, we changed the project name in the package.json according to the user&rsquo;s previous answers.
Delete Delete files in the target.
Running SAO You can run SAO from command line or from your cli.js file:
sao({...customVariables}) .run() .catch((err) =&gt; { console.trace(err) process.exit(1) }) Summary Probably you worked before with something similar, every time you have been created a project with Vue CLI, Nuxt, React, etc.
That is a shallow introduction to all the SAO functionalities, but as you can see the possibilities to customize your project creation are high.
You could create a template with the config files you use habitually and store the creation script in NPM.js, in a private registry, and every time you create a project, invoke yarn create my-project-scaffolding and start to code without the need of prepare manually the config files or miss some dependency.
`,url:"https://sergiocarracedo.es/automate-project-startup-with-yarn-create/",image:"/automate-project-startup-with-yarn-create/automatize-node-project-creation_hu_be067f81957ed88.jpg",tags:[],readingTime:"4 minutes read",date:"Sep 22, 2020"},"https://sergiocarracedo.es/how-vue-reactivity-works-i/":{title:"How Vue's reactivity works (I): Object.defineProperty",content:`Maybe you don&rsquo;t need to know how Vue reactivity works under the hood to make Vue apps, but anyway it will be interesting and useful.
In this context reactivity, means, simplifying, the capacity to detect a data change and do something after that.
In a Vue component, reactivity, means that the component will be re-rendered (totally or partially) after a change in the value of a variable to show the component updated with the new value. For example in this basic component:
&lt;template&gt; &lt;div&gt; &lt;h6&gt;Value: {{ clickCount }}&lt;/h6&gt; &lt;button @click=&#34;onClick&#34;&gt;Add 1 more&lt;/button&gt; &lt;/div&gt; &lt;/template&gt; &lt;script&gt; export default { data () { return { clickCount: 1 } }, methods: { onClick () { this.clickCount = this.clickCount + 1 } } } &lt;/script&gt; Every time you click on the &ldquo;Add 1 more&rdquo; button, the value of clickCount variable is increased in one unit and Vue starts the mechanism to re-render the component showing the new value in the template. How is Vue able to know when a variable changes its value?
Object.defineProperty API The answer is Object.defineProperty
This is a static method that defines or modify a property on an object
const myObject = {} Object.defineProperty(myObject, &#39;myProperty&#39;, { value: &#39;myValue&#39; }) Probably you realized that is the same as myObject.myProperty = 'myValue', but there an important difference: we can configure the property behaviour, for example:
const myObject = {} Object.defineProperty(myObject, &#39;myProperty&#39;, { value: &#39;myValue&#39;, writable: false }) myObject.myProperty = 1 console.log(myObject.myProperty) // &#39;myValue&#39; In this situation, if you try to change the value of the property, it will not change, and if you are using strict mode you will get an exception.
With Object.defineProperty you could define a getter and setter for the object property as shown in the following example. Every time you try to assign a value to your property getter is called.
const myObject = {} let myProperyValue = &#39;my value&#39; Object.defineProperty(myObject, &#39;myProperty&#39;, { get: () =&gt; { console.log(&#39;getter&#39;) return myProperyValue }, set: (newValue) =&gt; { console.log(&#39;setter&#39;) myProperyValue = newValue } }) myObject.myProperty = 123 console.log(myObject.myProperty) // setter // getter // 123 Run in PlayCode
Note that if you use a getter or a setter you can&rsquo;t access to the property&rsquo;s value directly, I mean, you have to store property&rsquo;s value somewhere else place.
Back in Vue, when you create a component you should define reactive values in data key
export default { data () { return { clickCount: 1 } }, ... } Under the hood, Vue creates an object with the properties you defined using Object.defineProperty and generates a getter and a setter. Every time a variable&rsquo;s value changes, the setter intercepts the change and launches Vue&rsquo;s re-render process with the new value.
This is the reason why you cannot add new variables to your component directly
export default { data () { return { clickCount: 1 } }, ... methods: { someMethod () { this.newClickCount = 1 ... } } ... } In the example above, newClickCount will not be reactive because Vue can&rsquo;t know when you add a new property directly.
If you need to add a new property after the component&rsquo;s definition Vue provides Vue.set or vm.$set
this.$set(this.someObject, &#39;b&#39;, 2) But this not work with the root element, I mean we cannot add a new variable to data
Object.defineProperty works since IE9, and in all modern browsers https://caniuse.com/?search=DefineProperty
How does it for arrays? It doesn&rsquo;t!. If you try to repeat the previous example with an array property:
const myObject = {} let myProperyValue = [] Object.defineProperty(myObject, &#39;myProperty&#39;, { get: () =&gt; { console.log(&#39;getter&#39;) return myProperyValue }, set: (newValue) =&gt; { console.log(&#39;setter&#39;) myProperyValue = newValue } }) myObject.myProperty = [1, 2, 3, 4] console.log(myObject.myProperty) // setter // getter // [1, 2, 3, 4] setter works because, is a direct assignation, but we usually don&rsquo;t work with arrays in that way, we use .push, slice, etc..
myObject.myProperty.push(5) console.log(myObject.myProperty) // getter // getter // [1, 2, 3, 4, 5] Run in PlayCode
We can see getter has been called twice, but the setter has not been called
How does Vue resolve it? Simple, patching vanilla JS array methods
Vue stores the original method, and create a new that notifies the change and execute the original method
You can see how it does in detail on: https://github.com/vuejs/vue/blob/bb253db0b3e17124b6d1fe93fbf2db35470a1347/packages/vue-template-compiler/build.js#L1087
ES6 Proxies There is another way to know when a value change (and other things) in JS since ES6: Proxies
Vue 3 uses Proxies instead of Object.defineProperty to make the reactivity under the hood. I will write a post about Proxies soon.
`,url:"https://sergiocarracedo.es/how-vue-reactivity-works-i/",image:"/how-vue-reactivity-works-i/object-define-property-pb-2361801_hu_82ab13e84c64834d.jpg",tags:["vue","reactivity","javascript"],readingTime:"4 minutes read",date:"Sep 8, 2020"},"https://sergiocarracedo.es/is-search-in-google-a-skill/":{title:"Is know how to search on Google a skill?",content:`Most of us do search on Google every day, maybe looking for a website, for news, for an address, for a cooking recipe, for information about a topic we are interested in, for technical information or anything.
All developers I know use Google, another search engine, or search on webpages like stack overflow every day to find the best way to do a task.
So is it a skill?, Should you know how to resolve a task without using Google or another knowledge source?
I don&rsquo;t think so, part of our skill should be know how to find information about the task. This has been done for centuries, we have just updated (added) the knowledge sources, in the past, sources were physical books, oral knowledge, etc. But now we have these sources and more, like online sources, and Google and other search engines are the way to reach the knowledge we need at some point.
Imagine you have the task of creating something like this:
Do you know how to do that? If you do, nice, lucky you, but I don&rsquo;t know how to do it.
I guess the next step is to search for how to do it, but, what&rsquo;s this thing name? If you don&rsquo;t know it, you need to find the name of the &ldquo;thing&rdquo; you should develop.
How do you do it? In that case (we will suppose that you have this image as a task&rsquo;s reference) I guess a good idea could be use Google Image Search.
If we upload that image, Google interprets the image as a circle. WTF? Perhaps is because it reads the words in the image and believes they are circle&rsquo;s attributes. Who knows ¬Ø\\(„ÉÑ)/¬Ø
So in a rapid view, Google doesn&rsquo;t return relevant information about the name of this thing.
But if you look closely the &ldquo;Visually similar images&rdquo; section, most of them are beautiful images with similar colors, but the third one is similar to our image: It has words too, and if you look even closer, the title of the image is WordMap
I will not go into detail, but, it&rsquo;s easy to find the relation with the term Word Cloud that is the &ldquo;thing&rdquo; we must create, and we could search for the theory and maths behind that, find a library which does the task or create from the scratch and finally do our task.
You will probably agree that use others&rsquo; experience it part of the developer&rsquo;s job (is part of any job), we are doing it all the time, and find the way to reach that experience and knowledge I guess is a skill.
I think employers should appreciate it as another skill, perhaps it&rsquo;s just a soft skill, but if your employee hasn&rsquo;t it, she/he could spend a lot of time just trying to know how to find the way to do some task or find information and knowledge.
Some people even consider that know how to search on Google is the most important skill a developer must have. I would not dare to say that, but I think is an important skill.
What do you think? Should be a soft or a hard skill?
`,url:"https://sergiocarracedo.es/is-search-in-google-a-skill/",image:"/is-search-in-google-a-skill/google-skill-218717_hu_e683e638743364c8.jpg",tags:["google","search"],readingTime:"3 minutes read",date:"Aug 31, 2020"},"https://sergiocarracedo.es/vuepress/":{title:"What is VuePress and why you should use it to document your project?",content:`VuePress defines itself as a Vue-powered Static Site Generator. In other words, is a tool to create static sites. A static site is a website where nothing runs on the server. The server only takes care of return the file (HTML file) as is stored in the server.
Is the opposite of a dynamic site, like for example a PHP webpage, in that case, your browser makes a request to the server, and the server executes PHP code that, for example, get a post from a database, process it, put that post in an HTML template, etc and returns the result to the browser.
What&rsquo;s better dynamic or static? This is not the goal of this blog post, but I will say: &lsquo;It depends&rsquo;. There is no &ldquo;magic&rdquo; answer: the best type depends on the use case.
What does VuePress? With VuePress you can create content in markdown files and when you generate the website every markdown file will be converted to an HTML page.
VuePress also provides you other content-relates features:
Menus Search box (yes, works even as static website) Markdown extensions (that will make your life easier) etc. We will come back to these elements in a moment, but first, we will learn how to start a VuePress website
Creating a VuePress website It&rsquo;s very easy, just:
yarn create vuepress [project-name] cd [project-name] And then, to start the dev server yarn docs:dev.
(This command starts a local dev server, by default, on http://localhost:8080
At this point, you can create content just creating markdown files in docs folder
If you create a file named my-content.md you could access it in http://localhost:8080/my-content.html
(If you want to serve a default page a.k.a http://localhost:8080/ the filename should be \`README.md)
You also could create folders in docs/ and folder name will be in the URL of that content. For example: docs/blog/README.md will be served at http://localhost:8080/blog/
Markdown extensions VuePress provides markdown extensions add more features than &ldquo;standard&rdquo; markdown provides.
For example:
Write Github-styled tables: You can create tables in markdown just writing something like: | Col 1 | Col 2 | Col 3 | | ------------- |:-------------:| -----:| | Content col 1 | Content col 2 | 1234 | Frontmatter support: Frontmatter is a way to add YAML content in a markdown file, to set content metadata, for example, the title, the language, etc ) Emoji support: &#x1f602; Nothing more to say ToC (Table of contents): A very useful extension, you only need to add [[toc]] in your markdown, and it will be rendered as a table of contents (a tree of document headings) Vue components: You can add Vue components directly in the markdown. That is very useful for main the VuePress&rsquo;s use. Go to https://vuepress.vuejs.org/guide/markdown.html for further information
VuePress as documentation generator Applications and uses are infinite, but VuePress is a very simple but powerful tool to create technical documentation.
Most of Vue ecosystem uses VuePress to create their documentation websites: Vue.js, Vuex, Vue Apollo, Portal Vue, Vue ChartJs, etc&hellip;.
I think this simplicity makes easy the task of creating your project documentation.
Using in an existing project I think is a good idea to keep your project and its documentation together, and with VuePress is possible to do it.
In your project, you only need to add Vuepress as development dependency, that&rsquo;s all
yarn add vuepress -D And edit your package.json to add the following items in the scripts section to start dev server and build documentation.
... &#34;scripts&#34;: { ... &#34;docs:dev&#34;: &#34;vuepress dev docs&#34;, &#34;docs:build&#34;: &#34;export NODE_ENV=production &amp;&amp; vuepress build docs&#34; }, ... Advantages Keep the documentation together with the code makes it easier for developers to read the project documentation and update it, because it is near to the code. As documentation is markdown, you can still be reading it even without use VuePress, for example in your IDE or in GitHub. You could insert your project&rsquo;s Vue components in the documentation to create a &ldquo;playground&rdquo;, for example, to demonstrate how your component change if you change some property. Like Buefy does in its documentation VuePress is themable and highly configurable, you can do advanced things, but you can start to write and serve docs in a few minutes. Other VuePress uses Create technical documentation website isn&rsquo;t the only VuePress use case, you could use VuePress to create a blog Example, or a simple webpage but, unless it is something very simple, i think VuePress is not the best tool.
VitePress At this moment, Evan You, VuePress and Vue.js creator is working on Vite, a build tool that uses native ES Module imports and promises be very fast, and over Vite is creating too VitePress, a VuePress brother built on top of Vite, that will have some improvements over VuePress, to highlight Vue 3 usage and faster dev server and build and with lighter page weight.
Summary If you need to serve your project documentation VuePress is a good option to do that.
`,url:"https://sergiocarracedo.es/vuepress/",image:"/vuepress/vuepress_hu_192e5caad144e954.jpg",tags:["vue","vuepress","markdown","documentation"],readingTime:"4 minutes read",date:"Aug 22, 2020"},"https://sergiocarracedo.es/2020/07/22/export-and-import-vuex-state/":{title:"Export and import Vuex state",content:`If you are familiarized with Vuex, you must know that Vuex is a state management pattern library for Vue applications. Vuex centralizes the application&rsquo;s state and how components, and other code parts, change it.
You can find a lot of articles talking about Vuex, I even wrote 2 articles 3 year ago talking about it: Vuex el redux de VueJS I and Vuex el redux de VueJS II (Spanish)
But today I will write about an edge case related to vuex, as you could read in the title, about how to export and import Vuex state.
Is a very easy process, and is not necessary for most of the applications, but I think is useful to know how to do it.
Export Export is very simple, you only need to get the state, for example from a component
const savedState = this.$store.state And voil√† you have the store state, you could save in Local Storage to keep the state even if the user closes or reloads browser&rsquo;s tab. Really you don&rsquo;t need to write code for that, exists an awesome library that does that: vuex-persistedstate
Import You can think that import the state is similar to export: this.$store.state = savedState If you try, you will get an Exception
Uncaught Error: [vuex] use store.replaceState() to explicit replace store state.
And if you read carefully, the exception message gives to you the solution: Use store.replaceState()
This store method replaces all the store state (the root state). For example, Vue Dev Tools uses it to do the time travels.
Applications I can think of a few applications for this, as I mentioned before, save the state to Local Storage, or export and save state to a JSON file to save your application settings, etc&hellip;
`,url:"https://sergiocarracedo.es/2020/07/22/export-and-import-vuex-state/",image:"/2020/07/22/export-and-import-vuex-state/vuex-export-import_hu_8d11f7de0eda86e9.jpg",tags:["js","vue","store"],readingTime:"2 minutes read",date:"Jul 22, 2020"},"https://sergiocarracedo.es/2020/07/17/sharing-variables-between-scss-and-typescript/":{title:"Sharing variables between SCSS and Typescript",content:`Sometimes you need to share variables between CSS (or SCSS) and Typescript, for example, if you have a list of colors in your SCSS file and need to check the variable names in typescript to be sure is an available color.
Imagine a Vue component have a property to set the background color:
&lt;template&gt; &lt;div :class=&#34;[&#39;component&#39;, colorClass]&#34;&gt; My component &lt;/div&gt; &lt;/template&gt; &lt;script&gt; const availableColors = [&#39;primary&#39;, &#39;alert&#39;, &#39;my-custom-color&#39;] export default { name: &#39;my-component&#39;, props: { color: String }, computed: { colorClass () { if (availableColors.indexOf(this.color) !== -1) { return \`color-\${this.color}\` } return null } } } &lt;/script&gt; In this component, if you set the property color, and the value is an available color, adds a class for that color, for example, if the color prop is primary adds the class .color-primary, but if the prop&rsquo;s value is red doesn&rsquo;t add any class related to color because red is not an available color.
Bearing in mind that, we probably have a scss file where we define that classes, something like:
$primary: #333; $alert: #900; $custom: #090; .color-primary { background: $primary; } .color-alert { background: $alert; } .color-custom { background: $custom; } We could improve this file to generate the classes programmaticaly using the power of SCSS:
// colors.scss $colors: ( &#39;primary&#39;: &#39;#333&#39;, &#39;alert&#39;: &#39;#900&#39;, &#39;custom&#39;: &#39;#090&#39; ); @each $name, $color in $colors { .color-#{$name} { background: $color; } } This way of generating the color classes allow us to simplify how we add a new color. We just should add the new color to $colors and we will have the color class
// colors.scss $colors: ( &#39;primary&#39;: &#39;#333&#39;, &#39;alert&#39;: &#39;#900&#39;, &#39;custom&#39;: &#39;#090&#39;, &#39;new-color&#39;: &#39;#00a&#39; ); @each $name, $color in $colors { .color-#{$name} { background: $color; } } But if you remember in our component we had an array width the list of available colors, if we don&rsquo;t add the new colors to component too, we can not use it &#x1f614;
But there is a way to only need to add colors in SCSS and also use the list in typescript: :export.
:export is brought to us by Webpack&rsquo;s scss loader and allows us to make scss variable exposed to Javascript / Typescript. We will add an :export stament to our colors.scss file
// colors.scss $colors: ( &#39;primary&#39;: &#39;#333&#39;, &#39;alert&#39;: &#39;#900&#39;, &#39;custom&#39;: &#39;#090&#39;, &#39;new-color&#39;: &#39;#00a&#39; ); @each $name, $color in $colors { .color-#{$name} { background: $color; } } :export { @each $name, $color in $colors { #{$name}: $color } } Notice we don&rsquo;t need semicolon (;) or comma (,) at the end of each line.
Then, we will refactor our compoment like this:
&lt;template&gt; &lt;div :class=&#34;[&#39;component&#39;, colorClass]&#34;&gt; My component &lt;/div&gt; &lt;/template&gt; &lt;script&gt; import availableColors from &#39;./colors.scss&#39; export default { name: &#39;my-component&#39;, props: { color: String }, computed: { colorClass () { if (availableColors.indexOf(this.color) !== -1) { return \`color-\${this.color}\` } return null } } } &lt;/script&gt; But in Typescript we must declare the module to available its contents, we just add a declaration file (with the same name as scss file plus .d.ts)
// colors.scss.d.ts export const colors : any export default colors And now we only need to add a new color in one place (the scss file) and it will be available everywhere.
`,url:"https://sergiocarracedo.es/2020/07/17/sharing-variables-between-scss-and-typescript/",image:"/2020/07/17/sharing-variables-between-scss-and-typescript/share-scss-variables-ts-3951901_hu_871656d8ff07250b.jpg",tags:[],readingTime:"3 minutes read",date:"Jul 17, 2020"},"https://sergiocarracedo.es/2020/07/15/why-i-write-a-blog/":{title:"Why I write a blog",content:`If you are here (Welcome!) you are reading my blog, this is obvious &#x1f602;, and I want to share with you why I write it.
Writing a blog is in some ways a hard task:
You should find an interesting topic to write about. Look for information related to the chosen topic. Read a lot of references about it. Write your blog post. Review it. Correct it. Find a beautiful and related picture for the header and home page. And finally publish it. You could also want to share on Twitter or LinkedIn. Ok, but why do you do this &ldquo;effort&rdquo;? In somehow I do this thinking of myself:
To Learn All this effort you do reading about the topic and looking references makes you learn a lot about the topic and about other things you didn&rsquo;t know previously. Well, is not the same write opinions or feelings (like this post) or write a technical article.
Going in deep on the topic Very related to the previous one is to go in deep in the topic. For example when you write about some technical topic you know a lot about, probably you could think that you don&rsquo;t need read anything, but is interesting to check your arguments and that forces you to go deep in the topic, more than you expect.
To Practice English When I created this webpage I started to write in Spanish because is my native language (the order one is Galician), but six months ago I joined an English speaking team, and I need to improve my English.
One way to do that is to write my blog articles in English to practice it. (I know I make a lot of mistakes, but this is part of the way to improve &#x1f613;)
To share knowledge I like to learn, and one source of (technical) knowledge is to read other people&rsquo;s blogs, and I want to return a little of what I have received.
As own reference This might sound stupid, but I came back to my old blog post to find information about some things. Things I don&rsquo;t do for a long time and I forget how I resolved it, but I know that I wrote a post about that. So I just need to visit my blog and find the article.
These are my motivations. What are yours?
`,url:"https://sergiocarracedo.es/2020/07/15/why-i-write-a-blog/",image:"/2020/07/15/why-i-write-a-blog/why-i-write-a-blog-977931_hu_6cdb659252f2c78b.jpg",tags:[],readingTime:"2 minutes read",date:"Jul 15, 2020"},"https://sergiocarracedo.es/2020/06/21/Improving-your-JS-workflow/":{title:"Improving your Vue (and JS) coding quality and workflow",content:`To start coding Javascript (and other languages), you almost need nothing, just a simple, text editor. That is good to start coding when you are learning the language, just code without distractions.
But when you develop bigger projects or/and with others, some problems appears: code organization, coding style, and other. That is because all languages have at a least clean code style guide.
Things like use semi-colon or not at the end of the line, the number of spaces indenting code, etc.
In Javascript, we have several code styles
Google Javascript Style Guide AirBnb Javascript Style Guide Javascript Standard Style &hellip; Personally I prefer StandardJS (Javascript Standard Style), despite this style bans semicolons, and I come from PHP where semicolons are mandatory.
And to complicate it a little more if you use a JS framework like Vue, Angular, or React have their coding styles.
And also if your project uses Typescript, it has is own coding style guide
Then, how could we ensure all our code complains chosen style guides?
Lint A linter is a tool that analyzes statically our code to ensure that complaints the rules of our coding style(s).
One of the most used tools in the Javascript world is ESLint. This tool allows you to find coding style errors, and fix them automatically if is possible.
For example when you create a Vue project using vue-cli the setup wizard ask you if you want to use a Linter / Formatter (you should) and allows you to choose the coding style, and when you want the linter runs: on file save and/or when you commit your files.
{% asciinema 34AKksPvMuJ7qpUk6K5gBruyS %}
Please notice I choose lint on commit. When your project is small, there is no problem in use &ldquo;lint on save&rdquo;, but when the project makes bigger check the files on save would be very slow and unproductive. Even with lint on saving, when you are testing things in your code and for example, you comment lines and have an unused import, or remove last value of a list keeping the trailing comma, lint returns an error when you save, and make the tests slower. When you are experimenting you should be concentrated on the experiment, not in the code, when all works ok, then is time to refactor and take care of the coding style.
Let&rsquo;s make an example adding some end line semicolons ;, spaces between lines, etc
{% asciinema ghtOPskHMcmtMmj8q7RUQZb47 %}
As you can see, before commiting the code, a git hook runs the linter and fix the code (not always is able to fix the code)
This is the package.json section related to githooks and linter
{ &#34;gitHooks&#34;: { &#34;pre-commit&#34;: &#34;lint-staged&#34; }, &#34;lint-staged&#34;: { &#34;*.{js,jsx,vue}&#34;: [ &#34;vue-cli-service lint&#34;, &#34;git add&#34; ] } } Vue uses a tool called Lint-staged that allows the linter to only check staged files. I can assume all files in the repo (not modified) are ok because they were linted before commiting to the repo.
If you want, you could manage git hooks using Husky
{ &#34;husky&#34;: { &#34;hooks&#34;: { &#34;pre-commit&#34;: &#34;lint-staged&#34;, &#34;pre-push&#34;: &#34;vue-cli-service test:unit&#34;, &#34;...&#34;: &#34;...&#34; } } } For example, you can force run tests before push.
Optimizing If you are using Webpack, another interesting tool to know the optimization of your project is Speed Measure Pluing
This tool shows the time elapsed by each webpack plugin when you build (even using HRM) your project using webpack. It&rsquo;s very useful to detect if some loader is taking too much time to run.
If you use vue-cli in your project, you can take advantage of using the integrated analyzer. That allows you to see every imported package, css, or library and check the sizes, and put the focus on heavier ones and try to optimize imports (not importing whole package, just the necessary libraries)
`,url:"https://sergiocarracedo.es/2020/06/21/Improving-your-JS-workflow/",image:"/2020/06/21/Improving-your-JS-workflow/js-workflow-38537_hu_93fc20d470266989.jpeg",tags:[],readingTime:"3 minutes read",date:"Jun 21, 2020"},"https://sergiocarracedo.es/2020/06/02/my-favourite-youtube-channels-2020/":{title:"My favorite YouTube Channels 2020",content:`In 2018, I did a list of my favourite YouTube Channels (Spanish), almost 2 years pass after this post and on this time I start to watch new channels and remove or watch less other channels.
This year I classified the videos by topic (as I did last year), and I added an emoji code:
&#x2b50; Favorite channel. I usually watch most videos. &#x2757; Very interesting &#x1f525; Some videos are very interesting
This is the updated list:
Travel / Culture / World: &#x2b50; Nekojitablog [ES] This channel is hosted by a Japanise woman an his Spanish husband. Thay talk about Japan culture, places, etc. An interesting point of view &#x2757; Jabiertzo [ES] A couple of Chinese and Spanish talking about china. Ter Sometimes talks about architecture. Engineering / Science &#x2b50; QuantumFracture [ES] Physics!! &#x2b50; La Hiperactina [ES] Biomedicine &#x2b50; Antroporama [ES] Neuroscience! &#x2b50; Dot CSV [ES] AI &#x2b50; Derivando [ES] Maths! &#x2757; The Slow Mo Guys [EN] Slow motion videos about &ldquo;science&rdquo; &#x2757; La gata de Schr√∂dinget [ES] &#x2b50; CienciaDeSofa [ES] Ra√≠z de Pi [ES] Maths ;) &#x2757; Hrom [ES] Spanish MacGyver SmarterEveryDay [EN] Very detailed videos about science topics. Deborahciencia [ES] Chemistry &#x2757; Physics Girl [EN] Experiments, demonstrations, etc. Vsauce [EN] &#x1f525; Engineerguy [EN] This guy make videos talking about engineering processes, my favourite one https://www.youtube.com/watch?v=RMjtmsr3CqA SizeMatters [ES] Nanoscience using pixelart. &#x1f525; Carolina Jim√©nez G. [ES] Cinema FX CdeCiencia [ES] Weekly videos about scientific news Date un Vlog [ES] Instituto de f√≠sica te√≥rica [ES] &#x2757; LockPickingLawyer [EN] Lockpicking explained Hablando en data [ES] Data science Music &#x2757; Jaime Altozano [ES] Talking about music Animals &#x2b50; I_am_puma [RU] Mugumogu [ES/JP] Mara &amp; Hana cats OwlKitty [EN] Cat and special effects Cole and Marmalade [EN] Cats &#x1f525; The Pet Collective [EN] Hrzysztof Smejlis [EN] Cats Sho Ko [EN] Cats Gaming &#x2b50; Buck Fernandex [ES] Menos Trece [ES] Luzugames [ES] &#x2b50; Auron [ES] aLexBy11 [ES] Vicio ONE MORE TIME [ES] &#x1f525; Ibai [ES] Magma [EN] Minecraft Racing / Simracing / Cars &#x1f525; Heikki360ES [ES] This Youtuber make videos of his races on iRacing and other simracing games) &#x2b50; Saul Lopez [ES] Talks about Tesla, green energies, sustainable transport, etc. &#x2b50; Efeuno [ES] Talks about F1 un deep and with a litle humor sense. Proudly creator of Shittyflaute F1 theme F1 Visualized Show info about F1 races, times, etc using pixel art Chain Bear [EN/ES] Motorsport explained. &#x2b50; Formula 1 [EN] Oficial F1 channel. Race highlights, full races, radios, onboards, etc. Juan Francisco Calero [ES] Lando Norris [EN] McLaren F1 Driver channel Humor / Entertainment &#x2b50; La Vida Moderna [ES] Best humor channel ever. &#x2b50; La Lengua moderna [ES] La Resistencia [ES] &#x2b50; Fortfast WTF [ES] Interviews &#x2b50; Pantomima Full [ES] Weekly videos about human contradictions. &#x2757; AuronPlay [ES] Video Buck [ES] Movies (new and old) reviews &#x1f525; F√°cil Nejo [ES] Movie reviews Agujeros de gui√≥n [ES] Movies and scripts reviews No te metas en pol√≠tica [ES] FaceToFake [ES] Humor using deepfakes Vicisitud &amp; Sordidez [ES] Loulogio [ES] Movies reviews, humor, etc Simon&rsquo;s Cat [EN] &#x1f525; Aleix Sal√≥ [ES] Programming / Communities A Industriosa [ES/EN] &#x2757; CodelyTV [ES] &#x2b50; Alva Majo [ES] Very sarcastic indie game developer Guinxu [ES] Indie game developer talking about game develop Makigas [ES] Win / Fails / Zapping Koreous [FR/EN] &#x2b50; Coka√Øn.fr [FR/EN] WinCompilation [EN] FailArmy [EN] This is Happening [EN] `,url:"https://sergiocarracedo.es/2020/06/02/my-favourite-youtube-channels-2020/",image:"/2020/06/02/my-favourite-youtube-channels-2020/youtube2020_hu_870d139f085020d.jpg",tags:["lifestyle","youtube"],readingTime:"3 minutes read",date:"Jun 2, 2020"},"https://sergiocarracedo.es/2020/05/18/Creating-a-simple-donut-chart/":{title:"Creating a simple donut progress chart",content:`One of more simpler charts you can create is a circular chart to represent the percent of completion of something.
First approximation We could create a SVG document to draw our donut. SVG allow us to represent graphical elements, for example, cicles, rectangles, polygons, etc&hellip; using XML (and therefore accesing to the elements via DOM). SVG is vertorial and is perfect for responsive.
Figure 1 represent that kind of chart, if the value is 100% the donut is perfect, if is 50% donut is a half donut, and 0% means no donut.
As you can see in the figure 2 this chart is the addition of 3 elements: 1 circle with the colors, a smaller circle to create the illusion of the void center, and a triangle to create the illusion of the missing part of the donut to be completed (If the completion value if less than 50% you will need to add a rect to hide half of circles).
This approximation has an issue, the small circle and triangle have a color if your background has different color or is a non-solid background color, this approximation fails (figure 3)
Second approximation To avoid the non solid background color, problem, you could use SVG Clip Path that limits the part of the element (or group of elements) are visible. In this case you should use the smaller circle and the triangle as clip path of the bigger circle.
Problem solved!
Wait, calculations necessary to draw the clip path can be complex, there are a simple way to draw a donut.
Using Stroke We were drawing the donuts as the difference between to circles, the donut&rsquo;s width was the radius difference (R1 - R2) are there another way to draw a donut? Yes, a fat stroke circumference.
Oh, our donut is drawing partially outside SVG element and is cropped. That is because the stroke grows from the center of the line, if the stroke is 10px, 5px will be drawn in a side of the line and other 5px in the other side. So we need to subtract the half of donut width of our circumference radius, and instead of using the radius we will use the svg size (in the case the diameter)
radius = size / 2 - width / 2 Cool! We have our donut, now we must remove part of it to represent the percent. Instead of use clip path we will use stroke-dasharray attribute. This attribute allows us to define how is drawn the stroke, for example stroke-dasharray=&quot;1 1&quot; means line will draw 1px solid and 1px transparent (in this context px is not a screen pixel, is a pixel in the SVG viewbox context). So we could use this behavior to draw a number of pixels and leave the rest transparent. We should calculate the numbers.
First we need to know what is the circle length, this is basic math: 2œÄR
const circumferenceLength = 2 * Math.PI * radius Then the solid part is circumferenceLength * percent / 100 and the transparent is circumferenceLength * (100 - percent) / 100, and this is the result.
Using this technique we could create a component with the properties: width, color, size, percent and we can create easy progress circle charts, absolutely responsive.
Also, you can animate the dash array to make a fluid change when value changes, or add a text label to show the value, or anything you can imagine.
Thanks for reading I hope you like it!
`,url:"https://sergiocarracedo.es/2020/05/18/Creating-a-simple-donut-chart/",image:"/2020/05/18/Creating-a-simple-donut-chart/donut-chart-179907_hu_6833bc8ae2b4f6d6.jpg",tags:["js","vue","data visualization"],readingTime:"3 minutes read",date:"May 18, 2020"},"https://sergiocarracedo.es/2020/05/09/js-unary-operators-and-taking-advantage/":{title:"Javascript unary operators: Taking advantage using them",content:`What&rsquo;s an operator? An operator is a symbol that define the operation to do between 1 o more operands.
We use a lot of operators: sum (+), subtraction (-), multiply (*), division (/), logic and (&amp;&amp;), negation (!),&hellip;
What&rsquo;s a unary operator? A unary operator is an operator that only needs one operand to work. For example
i++ In this example we have an operand (i) and an operator (++), we don&rsquo;t need more to increment the i variable value.
Unary operators in Javascript here is a list with more common unary operators in Javascript
++ Increment. Increments the value of operand in one unit. -- Decrement. Decrements the value of operand in one unit. ! Logical not. Negate the boolean operand value. - Negation. Negate the numeric operand value. typeof Returns the type of the operand in a string. delete Deletes the index of an array or an object. Hacking the system There is another unary operator that I haven&rsquo;t talked about, the unary addition (+).
let a = 10 console.log(+a) What do you think will be the result? You probably right: 10.
So what is this operator for if returns the same value? Here is another example.
let a = &#39;10&#39; console.log(+a) In this one, what do you think will be the result? Yes, it&rsquo;s 10 but not the same 10 we had. What?
Replay, but now let&rsquo;s look at the types
let a = 10 console.log(type +a) let b = &#39;10&#39; console.log(type +a) Both results are number. Here is the point, this operator (tries to) convert the value to number. So we can use it to cast values to number.
console.log(+&#39;10a&#39;) // NaN console.log(+&#39;0x10&#39;) // 16 console.log(+&#39;0o10&#39;) // 8 console.log(+&#39;0.1&#39;) // 0.1 console.log(+&#39;1e12&#39;) // 1000000000000 console.log(+&#39;Infinity&#39;) // Infinity console.log(+undefined) // NaN console.log(+true) // 1 console.log(+[]) // 0 console.log(+[2]) // 2 console.log(+[2, 3]) // NaN console.log(+{}) // NaN console.log(+null) // 0 &lt;= Take care Take care with the last result because it&rsquo;s less obvious
Unary minus do the same, but negate the result.
The ! operator behaves similarly: It tries to convert the value to boolean, but negate the result of the cast, but if we double negate the value we found a way to cast values to boolean
console.log(!!&#39;10a&#39;) // true console.log(!!&#39;&#39;) // false console.log(!!0) // false console.log(!!0) // false console.log(!!null) // false console.log(!!undefined) // false console.log(!!NaN) // false console.log(!!Infinity) // true console.log(!!{}) // true &lt;= Take care console.log(!![]) // true &lt;= Take care console.log(!!&#39;0&#39;) // false &lt;= Take care I marked the results that for me are counterintuitive
This two unary operators, + and !, using as !! are very useful to cast to number and boolean respectively.
Enjoy them! &#x1f609;
`,url:"https://sergiocarracedo.es/2020/05/09/js-unary-operators-and-taking-advantage/",image:"/2020/05/09/js-unary-operators-and-taking-advantage/unary-operators-1364700_hu_71a70c65cc9fd26d.jpg",tags:["js"],readingTime:"3 minutes read",date:"May 9, 2020"},"https://sergiocarracedo.es/2020/04/27/js-loop/":{title:"Understanding Javascript's event loop: Macrotasks and microtasks",content:`Despite what it may seem, Javascript execution in a browser is synchronous. It is a similar thing as a multitask OS running in a mono-core processor, in that case multitasking is &ldquo;fake&rdquo;, because the processor can only execute one instruction at a time, but the OS controls the execution and distribute the processor time between each app, making it looks like multitasking.
In Javascript we have a similar approximation in a way to distribute the execution time. It&rsquo;s not exactly the same. Lets see how it works.
Event loop When you write a program in JS, every script you add or every instruction will be added to a queue in the event loop
The engine starts to run every instruction in the same order you wrote, running all tasks, and when finish, it waits for more tasks and then starts again.
Every task in this queue is a macrotask and queue is the macrotask queue. Some examples of macrotasks are:
Every &lt;script&gt; loaded and all the instructions on them setInterval or setTimeout command DOM event: window.load, mouseout, etc In example below, we run some task: calculate the 6th element on Fibonacci series and get a string with the English alphabet. I put some console.log as code execution checkpoint to show the execution order.
The result is
Checkpoint 1 13 Checkpoint 2: 0.5600000149570405ms ABCDEFGHIJKLMNOPQRSTUVWXYZ Checkpoint 3 As you can see, the tasks run in order, string generation doesn&rsquo;t start until fibonacci function finish, even if the function takes long time to be completed.
Let&rsquo;s see what happens when you use setTimeout. When you set a timeout, you expect your function will be executed after the timeout you set. For example setTimeout(() =&gt; console.log('here'), 1000) should print in console &lsquo;here&rsquo; after 1s. You expect this always happens, no cares about next task or instructions. If you expect that, you are wrong. Check next code:
We have a long task after the timeout. Timeout should be executed after 1s, but next task takes 5s to be completed, and your timeout not executed yet. Ok let&rsquo;s wait long task end, and&hellip; nothing, your timeout still missing. Next macrotask is our string generator, this task is executed and, finally, after all macrotask, our timeout is executed.
That occurs because, when you use a timeout, you are moving the task at the end of the queue, an at this point is when js engine checks if your timeout should be executed.
Taking advantage You can take advantage of that event loop behavior, for example, you can put a high CPU usage task inside a setTimeout, even with 0ms of dispatch time, and this huge task will be executed after following tasks. That is a fast solution, but if you have more than 1 or 2 long tasks, you are only moving the problem to the end of the queue, but problem is still there.
Using Promises You could think of using promises, for example
But the promise starts to run the code inside just after call the promise&rsquo;s constructor, and your program is stucked again. You can resolve it using setTimeout again, but in this case we are creating a microtask:
Microtasks Microtasks are tasks created in promises (then, catch, finally). The microtask queue run immediately after every macrotask, before render or before the next macrotask
In that example, setTimeout creates a new macrotask, so it will run after macrotask loop, Promise creates a microtask that will be executed after next macrotrask ends, in this case after all script instructions but before the queued macrostask. And finally will run the setTimeout macrotask
There are another way to create a microtask, using queueMicrotask that is a recent addition to the standard. it&rsquo;s supported by most modern browsers (https://caniuse.com/#search=queueMicrotask) but if not, you can use this polyfill:
if (typeof window.queueMicrotask !== &#39;function&#39;) { window.queueMicrotask = function (callback) { Promise.resolve() .then(callback) .catch(e =&gt; setTimeout(() =&gt; { throw e })) }; } And works exactly as the previous example but syntax is clearer
Vue If you are using Vue, maybe you can show a loader or some kind of indicator when you go to execute a long task.
... data: { return { loading: false } }, methods: { ... longTask () { this.loading = true // doing your long task stuff this.loading = false } } ... You could think that the loader will be showed before start the long task and hidden after it, but that not happens, you never see the loader, because the reactive properties are &ldquo;checked&rdquo; after the loop and at this point this.loading is false.
You can try to use $.nextTick but the result it&rsquo;s the same, you need to get out of the event loop with your long task
... data: { return { loading: false } }, methods: { ... longTask () { this.loading = true queueMicrotask(() =&gt; { // doing your long task stuff) this.loading = false } } } ... Above I wrote that Javascript is synchronous, but I lied &#x1f605;. You can use WebWorkers, this is topic for another post&hellip; :simple_smile:
More info: https://javascript.info/event-loop
`,url:"https://sergiocarracedo.es/2020/04/27/js-loop/",image:"/2020/04/27/js-loop/js-event-loop-106155_hu_22526f13a32b976d.jpg",tags:["js"],readingTime:"4 minutes read",date:"Apr 27, 2020"},"https://sergiocarracedo.es/2020/04/05/slimbook-prox15-first-impressions/":{title:"Slimbook ProX 15: First Impressions",content:`As you probably know, I started a new job this year, and my company provided me a laptop. They asked me which laptop I would like to use to work, most people would answer: MacBook Pro or something like that, but I don&rsquo;t like Apple (sorry, those are my likes, maybe you have different ones), and I prefer Linux as my OS.
My first choice was a Dell XPS 15 with 16GB of RAM, but after several delays in delivery, my company and I were tired of waiting and we decided to order another laptop.
I was mulling over which model to choose, and I asked my friend F√©lix, who wrote a post about what laptop you can buy to work as a developer (Spanish) and he told me: &ldquo;Slimbook PROX&rdquo; a 13.3&quot; powerful laptop made by a Spanish company specialized in Linux laptops.
It was a great option, but most the time I work on a desktop computer and I prefer a laptop with a larger screen. Finally, I chose the 15&quot; version of that laptop: the PROX15. But not just change the screen size, there are more differences from PROX: PROX15 is a high performance oriented laptop.
I configured my order with 32GB of RAM, 512GB NVMe SSD and an Intel Dual Band 9560AC Wifi Card.
One thing I like of that company is that you can configure your computer: you can choose memory, 1 or 2 SSD storage (with RAID 0 or 1, if you chose 2), WIFI Card, Keyboard layout, and OS. And component are not welded to motherboard, that means you can upgrade the computer in the future.
But let&rsquo;s go to the point.
Technical Specs Display: 15.6&quot; FullHD IPS CPU: Intel i7-9750H (6 Core), no low-power or Ultra-power processor like series U GPU: NVIDIA Geforce GTX 1650 4GB and Intel¬Æ 630 HD Graphics RAM: 32GB Storage: SSD M.2 of 500GB Battery: ~92 Wh Others: Webcam 720 and an infrared cam to facial recognition, Bluetooth, etc&hellip; Weight: 1.5kg - Magnesium and aluminum case After 3-4 week using the laptop (non-intensive use), these are my thoughts.
Things I love Size The size is perfect for me. Big screen in a size contained case. Screen borders are very narrow (about 5mm) so laptop size is similar to 14&quot; one
Powerful CPU and GPU CPU it&rsquo;s a 9th generation&rsquo;s Intel i7-9750 H series, it&rsquo;s a mobility oriented processor, but focused in performance. 2.60GHz, 6 cores, (12 threads). &#x2764;&#xfe0f; Processor has an integrated Intel UHD Graphics 630, but also has a dedicated NVIDIA Geforce GTX 1650 with 4GB of non-shared RAM. If your system supports PRIME, you be able to choose what graphics card use. Intel to save battery or NVIDIA for performance.
I benchmarked both graphics card (Intel and Nvidia) and my desktop computer (Nvidia GTX 950) using Unigine Valley Benchmark and this were the results.
Using integrated Intel card
Using Nvidia GTX1650
Desktop computer. Nvidia GTX950
It&rsquo;s amazing!
Weight and case&rsquo;s materials It&rsquo;s a very light laptop considering its size, only 1.5Kg. The case is made in magnesium-aluminum alloy, very pleasant to the touch, at first view it could look like plastic, but it is not, the touch it&rsquo;s very different, it&rsquo;s not cold like only aluminum.
Connections It has many of them: HMDI (full size), USB 2.1, USB 3.0, USB-C, RJ45, Jack 3.5mm, MicroSD. Using a dock station, you can connect laptop to 3 external displays.
Keyboard I love keyboard layout, keys are a bit bigger than my previous laptop (Asus ZenBook UX330UA) Directions keys are complete and have Start, End, PageStart, PageEnd keys
Upgradable Components: Storage (2xM2 slots) and memory banks are upgradable. It&rsquo;s a feature that I really appreciate. Maybe in the future, I will add other SSD. It&rsquo;s possible to do this, and it does not force you to choose oversized components at the price of gold (cough, cough Apple) just in case.
Price That is work computer, and it was bought by my company. But the price was around 1500‚Ç¨ (VAT included). I think is a low price considering the specifications.
Other considerations Touchpad I&rsquo;m not a big fan of the touchpad in general, I like mice, but I think the Slimbook&rsquo;s touchpad is very good. More than enough for me.
Battery As I said before, and due to the quarantine, I didn&rsquo;t use the laptop intensively. Maker said up to 12h of life, but they are honest and talk about 5-8 with office use, obviously, playing games or using CPU and GPU intensive apps, battery life will be short as using &ldquo;normal&rdquo; apps
Refrigeration While Benchmarking graphics I guess fans were at maximum speed, they are a little noisy, but at acceptable levels bearing in mind that the GPU was at 80¬∫C with maximum performance. I just felt the temperature in a small area on the keyboard.
Things that I don&rsquo;t like Keyboard backlight It is not very bright, even raising the level to the maximum. The problem is that the keyboard is metallic gray, and for me, it is difficult to see the symbols on the keys. It&rsquo;s worse with backlight on, because the contrast between the keys color and symbol color it is very low. You can see it in the picture.
Windows key I know, it&rsquo;s a &ldquo;standard&rdquo; but I don&rsquo;t like it.
Summary In the future, after using it longer, I will write a more complete review, but right now I think I have a very good laptop, high quality materials, nice display, powerful components that can be upgraded, no problem to install linux. I&rsquo;m very happy with it.
I cannot forget good care of the Slimbook company. When I have any doubt about assembly process or something else, they respond very quickly on Twitter.
Images by Slimbook. CC-BY-SA
`,url:"https://sergiocarracedo.es/2020/04/05/slimbook-prox15-first-impressions/",image:"/2020/04/05/slimbook-prox15-first-impressions/cover_hu_a74e54e80a3f6420.jpg",tags:["hardware"],readingTime:"5 minutes read",date:"Apr 5, 2020"},"https://sergiocarracedo.es/2020/02/27/start-using-typescript-in-vue-easy-way/":{title:"Start using Typescript in Vue. The easy way.",content:`If you program in JavaScript probably somebody told you about the advantages of using Typescript or you thought about start using it.
As you can see in this chart, more than 50% Javascript developers are using Typescript
Source: https://2019.stateofjs.com/javascript-flavors/
Start to use a new technology, new paradigm, new framework or anything could be hard and challenging, this is why is important start integrating new technologies avoiding friction with previous one. Today I&rsquo;ll try to show you the easiest way to start using Typescript in your Vue projects.
In case of Typescript we &ldquo;just&rdquo; will add a layer over Javascript then I&rsquo;ll try to explain easier way to use Typescript with Vue even in existing projects.
Before start, you should know Typescript using benefits. You have thousands blog&rsquo;s post talking about that:
https://apiumhub.com/tech-blog-barcelona/top-typescript-advantages/ https://ionicframework.com/docs/v3/developer-resources/typescript/ https://medium.com/swlh/the-major-benefits-of-using-typescript-aa8553f5e2ed https://alligator.io/typescript/typescript-benefits/ &hellip; Most repeated benefit is &ldquo;types&rdquo;. Javascript is a weakly typed language (variables and consts have types, but you can not set its type. Only by means of value)
Prerequisites I assume you are using vue-cli. You must be sure you are using version 4.0.0+
Step 1: Adding Typescript First step is add Typescript using vue-cli
vue add typescript Vue cli starts to add typescript package and necessary dependencies. Also create and update configuration files like tsconfig.json, It needs ask you some questions to configure properly.
Let view each question in detail and the &ldquo;correct&rdquo; answer.
? Use class-style component syntax? (Y/n) If you want start to use class-style component syntax you should answer &ldquo;yes&rdquo;. You can answer &ldquo;no&rdquo; and you can not use it, but anything else changes.
? Use Babel alongside TypeScript (required for modern mode, auto-detected polyfills, transpiling JSX)? (Y/n) &ldquo;Yes&rdquo;. You were probably already using it.
The next two questions are the most important
? Convert all .js files to .ts? (Y/n) &#x26a0;&#xfe0f; You must answer &ldquo;No&rdquo; to avoid Vue cli rename all your files to .ts. If you do that you will have to refactor all your project files to Typescript
? Allow .js files to be compiled? (y/N) &#x26a0;&#xfe0f; You must answer &ldquo;Yes&rdquo;. It&rsquo;s the same as, in tsconfig.json, set the value allowJs to true. If you don&rsquo;t anwser &ldquo;yes&rdquo; you will receive a lot of error messages, because the typescript transpiler tries to transpilate .js
Step 2 No step 2. You just added Typescript support to your project and everything should work fine as usual.
Writing your first Vue component in typescript When you write a Vue component in Javascript, you probably write something similar to:
&lt;template&gt; .... &lt;/template&gt; &lt;script&gt; export default { props: ...., data () ...., methods: .... } &lt;/script&gt; To write a Vue compoment using Typescript you need to set the scripting language using the attribute lang in the script tag. Also, as Typescript needs to infer types, you must define your components with Vue.component or Vue.extend (I prefer second one)
Then our component look like:
&lt;template&gt; .... &lt;/template&gt; &lt;script lang=&#34;ts&#34;&gt; import Vue from &#39;vue&#39; export default Vue.extend({ props: ...., data () ...., methods: .... }) &lt;/script&gt; &#x1f389; Done! You have your first Typescript Vue component.
The point is you can start using Typescript and keep your previous code. You don&rsquo;t need to rewrite all components, or you can do step by step.
I recommend you to read Vue documentation about Typescript support
In future blog posts I show how use multiple mixins in Typescript and other tips about typescript
`,url:"https://sergiocarracedo.es/2020/02/27/start-using-typescript-in-vue-easy-way/",image:"/2020/02/27/start-using-typescript-in-vue-easy-way/pexels-photo-1272328_hu_f55ed52abd8187db.jpeg",tags:["vue","typescript","js"],readingTime:"3 minutes read",date:"Feb 27, 2020"},"https://sergiocarracedo.es/2020/02/26/same-avatar-in-diferent-account/":{title:"Using same avatar in different accounts and avoid confusion",content:`Probably you have several internet services accounts, and some in the same service, for example two gmail accounts, one for work and a personal, or multiple Instagram accounts as me
I like to use same picture as avatar in each account, but the problem is difficult to differentiate the account you are logged in only watching the avatar.
I&rsquo;m using a simple tip to differentiate them, add a color band at the bottom of the image, the point is to use same color for every single account or role, for example you could use black for work, and green for personal.
So when you are logged in and see the avatar, you will be able to know which account are you logged in.
Next picture shows my avatars in 2015:
Be sure that the color band has height enough when the avatar is a circle.
Using this tip is easier to differentiate the account you are using, in the example below, in gmail
If you are using OAuth, for example &ldquo;Login with Google&rdquo; button, to login into third party services, the avatar is the same as the source account and this tip still working.
I&rsquo;m sorry for forcing you to see my face so many times &#x1f605;&#x1f605;&#x1f605;
`,url:"https://sergiocarracedo.es/2020/02/26/same-avatar-in-diferent-account/",image:"/2020/02/26/same-avatar-in-diferent-account/pexels-photo-267350_hu_f2fb68d6b26cddc1.jpeg",tags:[],readingTime:"1 minute read",date:"Feb 26, 2020"},"https://sergiocarracedo.es/deep-cloning-objects-in-javascript/":{title:"(Deep) Cloning objects in Javascript",content:`Cloning objects in Javascript (and in other languages) is a tricky task. JS doesn‚Äôt store the object value in your variable or in your constant, instead, stores a pointer to the object value (the object reference).
Even when you pass an object to a function or method, you are passing this object by reference, not the value.
If you pass (or copy) an object by reference and then change any property, the ‚Äòsource‚Äô object‚Äôs property also changes.
In any example, I‚Äôll use the object below
const sourceObject = { l1_1: { l2_1: 123, l2_2: [1, 2, 3, 4], l2_3: { l3_1: &#39;l3_3&#39;, l3_3: () =&gt; &#39;l3_3&#39; } }, l1_2: &#39;My original object&#39; } &lsquo;Standard&rsquo; cloning We‚Äôll use a &lsquo;standard&rsquo; cloning by assigning the source value to another constant
const copiedObject = sourceObject console.log(&#39;sourceObject&#39;, sourceObject.l1_2) // My original object --&gt; ‚úîÔ∏è clonedObject.l1_2 = &#39;My cloned object&#39; console.log(&#39;clonedObject&#39;, clonedObject.l1_2) // My original object --&gt; ‚úîÔ∏è console.log(&#39;sourceObject&#39;, sourceObject.l1_2) // My original object --&gt; ‚ùå As I said before, when I change the property l1_2 on the cloned object, the value also changes on the source object.
Using this strategy, you are not copying the object at all.
Using spread operator This time I&rsquo;ll use the spread operator, that &lsquo;returns&rsquo; every element in the object individually.
console.log(&#39;sourceObject l1_2&#39;, sourceObject.l1_2) // My original object --&gt; ‚úîÔ∏è console.log(&#39;sourceObject l1_1.l2_1&#39;, sourceObject.l1_1.l2_1) // 123 --&gt; ‚úîÔ∏è const clonedObject = { ...sourceObject } clonedObject.l1_2 = &#39;My cloned object&#39; console.log(&#39;clonedObject&#39;, clonedObject.l1_2) // My cloned object --&gt; ‚úîÔ∏è console.log(&#39;sourceObject&#39;, sourceObject.l1_2) // My original object --&gt; ‚úîÔ∏è clonedObject.l1_1.l2_1 = &#39;321&#39; console.log(&#39;clonedObject l1_1.l2_1&#39;, clonedObject.l1_1.l2_1) // 321 --&gt; ‚úîÔ∏è console.log(&#39;sourceObject l1_1.l2_1&#39;, sourceObject.l1_1.l2_1) // 321 --&gt; ‚ùåÔ∏è // Should keep returning 123 if the clone was complete Now the property l2_1 is copied by value, we can change it, and the original object l2_1 keeps its original value, but if when I changed l1_1.l2_1 (2th depth property) we get the same as the first attempt.
The spread operator does a shallow copy of the object. Only first-level depth properties are copied by value, the nested ones keep copying by reference.
Using Object.assign Like the spread operator, do a shallow copy, then I will not create the example, trust me, you will get the same result.
const clonedObject = Object.assign({}, sourceObject) Using JSON.parse and JSON.stringify This is a simple and fast way to deep clone an object, the point is to convert the object to a string JSON.stringify and then get an object from the string using JSON.parse
Let&rsquo;s do it
const clonedObject = JSON.parse(JSON.stringify(sourceObject)) clonedObject.l1_1.l2_1 = &#39;321&#39; console.log(&#39;clonedObject l1_1.l2_1&#39;, clonedObject.l1_1.l2_1) // 321 --&gt; ‚úîÔ∏è console.log(&#39;sourceObject l1_1.l2_1&#39;, sourceObject.l1_1.l2_1) // 123 --&gt; ‚úî Everything seems fine! &#x1f389; But, did you notice l1_1.l2_3.l3_3 property is a function? &#x1f622;
console.log(&#39;clonedObject l1_1.l2_3.l3_3&#39;, clonedObject.l1_1.l2_3.l3_3) // undefined --&gt; ‚ùåÔ∏è console.log(&#39;sourceObject l1_1.l2_3.l3_3&#39;, sourceObject.l1_1.l2_3.l3_3) // function l3_3() {} --&gt; ‚úîÔ∏è Oh, oh, functions are not copied using that method, then, what could we do? The solution is to iterate every nested property in the object and use, for example, the spread operator method. It&rsquo;s hard and dirty work.
Lodash to the rescue Lodash is a modular utility library that adds many funcionalities, and one of them is cloneDeep which does exactly what we need to clone (deep) an object through nested properties, keeping all value types, even functions.
import { cloneDeep } from &#39;lodash&#39; const clonedObject = cloneDeep(sourceObject) console.log(&#39;clonedObject l1_1.l2_3.l3_3&#39;, clonedObject.l1_1.l2_3.l3_3) // function l3_3() {} --&gt; ‚úîÔ∏è console.log(&#39;sourceObject l1_1.l2_3.l3_3&#39;, sourceObject.l1_1.l2_3.l3_3) // function l3_3() {} --&gt; ‚úîÔ∏è Performance We‚Äôll copy the source object 10.000 times using each method to compare the time elapsed. Compare memory usage is no sense because Object.assign and Spread Operator method is not copying nested property by value.
The results in my browser are the following:
Object.assign clone elapsed time: 4ms Spread operator clone elapsed time: 22ms JSON clone elapsed time: 47ms Lodash clone elapsed time: 92ms As you can see, if you only need to do a shallow clone Object.assign is the faster solution, and if you only need to clone values in nested properties (not functions or symbols), JSON.parse(JSON.stringify()) could be a faster solution. But if you want to make sure that all values are copied you must use lodash or a similar solution.
Get your own results by trying it in codesandbox!
Header picture: unsplash-logoKaren Lau
`,url:"https://sergiocarracedo.es/deep-cloning-objects-in-javascript/",image:"/deep-cloning-objects-in-javascript/karen-lau-tRR3w-S2A3Y-unsplash_hu_11523fcdde3a3518.jpg",tags:["js"],readingTime:"4 minutes read",date:"Feb 22, 2020"},"https://sergiocarracedo.es/2019/12/29/vue-router-lazy-loading-and-chunking/":{title:"Vue Router lazy loading and chunking",content:`When you start to create SPA (Single page application) you must bear in mind that SPA doesn&rsquo;t mean Single JavaScript file.
You normally use Webpack to handle your app builds, by default, Webpack create one file for all assets, even CSS.
The first step, maybe, is separate styles from app.js in their own CSS files.
To do this, we&rsquo;ll use the Webpack plugin MiniCssExtractPlugin which we&rsquo;ll configure like this:
// webpack.config.js const MiniCssExtractPlugin = require(&#39;mini-css-extract-plugin&#39;); module.exports = { plugins: [ new MiniCssExtractPlugin({ filename: &#39;[name].css&#39; }) ], ... module: { rules: [ { test: /\\.css$/, use: [ { loader: MiniCssExtractPlugin.loader, options: { publicPath: &#39;/public/path/to/&#39;, }, }, &#39;css-loader&#39;, ... // Other loaders like sass-loader or postcss-loader ], } ... ] }, This forces Webpack to extract CSS into separated files, for example app.css
If you use vue-cli, this is the default config for Webpack.
Going forward For simple apps it is a good idea keep all your built code into a single file, because client&rsquo;s browser loads app.css the first time user accesses your app and keep it in cache, next access the file will be served from local browser&rsquo;s cache (until cache expire).
But when your application starts to grow the app.js will be huge, slowing down the page loading. There will even be parts of the app that are never used, for example &ldquo;pages&rdquo; (in this context think pages as Vue page component, not static pages) forbidden for regular users.
In this case a good solution is chunking your app.js using async components for page components. You can split every page into different files which will be loaded when user navigates to route.
This strategy uses the Webpack&rsquo;s code splitting feature.
In Vue router configuration you just do
const routes = [ { path: &#39;/&#39;, name: &#39;home&#39;, component: () =&gt; import(&#39;./views/HomeComponent&#39;) }, ... ] Webpack now will create a separated file for your page components. But this way the user receives no feedback about the loading process. We could improve the router using a loading component.
An async component must provide a Promise.resolve. When you write () =&gt; import('./views/HomeComponent') implicit you return a Promise that resolves the component. But if you want to use the handling loading state you need to return an explicit Promise
Like this:
import LoadingComponent from &#39;./LoadingComponent&#39; const routes = [ { path: &#39;/&#39;, name: &#39;home&#39;, component: lazyLoading(import(&#39;./views/HomeComponent&#39;)) }, ... ] function lazyLoadView (AsyncPageComponent) { const AsyncHandler = () =&gt; ({ component: AsyncPageComponent, loading: LoadingComponent, ... }) return Promise.resolve({ functional: true, render: (h, { data, children }) =&gt; h(AsyncHandler, data, children) }) } As you can see, we use a Promise.resolve that returns component render function.
data and children are necessary to pass props, attributes and events to component More info
With these changes, when the user navigate to / firstly, the app shows the LoadingComponent and then, when the component is fully loaded, shows it. Finally, say that you can group the components in the same chunk using the following notation.
import(/* webpackChunkName: &#34;group-main&#34; */ &#39;./HomeComponent.vue&#39;) import(/* webpackChunkName: &#34;group-main&#34; */ &#39;./LoginComponent.vue&#39;) import(/* webpackChunkName: &#34;group-admin&#34; */ &#39;./AdminPageComponent.vue&#39;) `,url:"https://sergiocarracedo.es/2019/12/29/vue-router-lazy-loading-and-chunking/",image:"/2019/12/29/vue-router-lazy-loading-and-chunking/pexels-photo-64782_hu_9c27eb2364913fe7.jpg",tags:["vue"],readingTime:"3 minutes read",date:"Dec 29, 2019"},"https://sergiocarracedo.es/2019/12/23/instagram-rbg/":{title:"Instagram RGB Project",content:`This is my first post in English. I joined a team whose main language is English and I need to improve it &#x1f61e;
I will start writing about non-technical topics, in this case about my 3 public Instagram accounts.
Red About 3 years ago, in 2016, I decided to create an Instagram account only for red things pictures because I thought in the global visual effect in the account profile page. This is the effect I am talking about:
This is that account https://www.instagram.com/sergiocarracedo.red/
Green In August of this year I created another account with same concept, but for green things: https://www.instagram.com/sergiocarracedogreen/
Blue And a few days later, I completed the RGB project creating the blue one: https://www.instagram.com/sergiocarracedo.blue/
In the beginning I thought that taking pictures based on a single color would be easy but I was wrong. Taking pictures is complicated, maybe the red are the easiest, and blue ones the hardest, always with the constraint of not repeating topic.
There are a lot of blue ones if you think of the sea and the skies, but all those pictures would be similar.
If you like this idea (different Instagram accounts based in color), please follow them:
Red: https://www.instagram.com/sergiocarracedo.red/ Green: https://www.instagram.com/sergiocarracedogreen/ Blue: https://www.instagram.com/sergiocarracedo.blue/ `,url:"https://sergiocarracedo.es/2019/12/23/instagram-rbg/",image:"/2019/12/23/instagram-rbg/ig-cover_hu_f87d3f3a686e3119.jpg",tags:[],readingTime:"1 minute read",date:"Dec 23, 2019"},"https://sergiocarracedo.es/2019/12/11/simracing-mas-real-que-virtual/":{title:"Simracing: Muy real y mucho real",content:`Llevo mucho tiempo jugando a simuladores de conducci√≥n, especialmente de F1, y cuando digo mucho tiempo, es mucho tiempo, m√°s de 20 a√±os &#x1f613;, desde el Microprose Grand Prix hasta uno de los m√°s actuales que he jugado: Live for Speed
N√≥tese que hablo de simuladores no de juegos de coches en general. Y por simulador podemos entender que el juego trata de imitar de la formas m√°s realista posible la experiencia de conducci√≥n y competici√≥n: Principalmente las f√≠sicas del veh√≠culo: Amortiguaciones, Ruedas (temperaturas, desgaste, suciedad), aerodin√°mica, √°ngulos de ca√≠da, motor, inercias, etc.
Obviamente los primeros simuladores no eran totalmente fidedignos a la realidad f√≠sica, pero el paso del tiempo y el aumento de la potencia de los equipos ha permitido que los c√°lculos de f√≠sicas lleguen a unos niveles de realismo muy alto.
Tambi√©n otras tecnolog√≠as como el LIDAR han permitido el escaneo 3D de los circuitos, pudiendo llevar al simulador hasta el m√°s m√≠nimo bache o irregularidad del asfalto. En este video de iRacing se muestra el proceso de escaneo de un circuito: Desde hace ya a√±os los equipos de F√≥rmula 1 disponen de simuladores propios, para que sus pilotos puedan entrenar y probar distintos setups del coche antes de realmente a pista o incluso antes de fabricar alguna pieza. De hecho la mayor√≠a de ellos tienen actualmente la figura del piloto de desarrollo que es el encargado de pasarse, literalmente, horas y horas en el simulador, para que los setups de partida en cada una de las carreras sean lo mejores posibles.
Esto es una de las pruebas de que el mundo f√≠sico (Real) y el virtual (Simuladores) cada d√≠a tiene las fronteras m√°s difuminadas.
Otra pieza importante son las mejora de las interfaces de interacci√≥n con el simulador, los volantes, pedaleras, cambios, sillas, etc. Un volante para PC o consola de buena calidad, con un force feedback de calidad, y tacto bastante realista ronda los 1000-1500‚Ç¨, aunque hay volante y pedales bastante decentes por 200‚Ç¨ (Yo tengo actualmente un Logitech Driving Force GT que es m√°s que suficiente para un uso ocasional)
Como dec√≠a cada vez est√°n m√°s difuminadas las fronteras entre el Racing y el Simracing, en 2008 Nissan, PlayStation(r) y Polyphony Digital (creados de Gran Turismo) crearon la GT Academy un concurso que permite a los ganadores del concurso (que compiten en la PS4) la oportunidad de conseguir una plaza en el equipo de carreras de Nissan (equipo de competici√≥n en el mundo Real). Es decir esta habiendo un transvase de talento del mundo virtual al real.
Hay que pensar que competir en carreras de coches es algo muy caro que no esta al alcance de todo el mundo, adem√°s de los riesgos que supone, el simracing permite reducir esa barrera, reduciendo los costes a la m√≠nima expresi√≥n y pudiendo &ldquo;entrenar&rdquo; y practicar con la seguridad de estar sentado en un simulador.
Y es m√°s, las compa√±√≠as y federaciones han apostado por fomentar la competici√≥n oficial virtual, por ejemplo la F√≥rmula 1 con F1esports apoyado por los propios equipos de la competici√≥n &ldquo;real&rdquo;. Logitech patrocinando el equipo McLaren de F1
Pero si hay un &ldquo;juego&rdquo; que ha hecho mucho por el Simracing creo que puedo decir sin temor a equivocarme que ha sido iRacing.
iRacing iRacing tiene un modelo bastante distinto a lo habitual:
por un lado, en lugar pagar por un juego, se paga por una subscripci√≥n mensual que da acceso al juego y al uso de ciertos coches y pistas (se puede acceder a m√°s pagando por ellas y ellos) Est√° totalmente orientado a la competici√≥n online, y aunque se pueden hacer carreras privadas, est√° orientado a competiciones &ldquo;oficiales&rdquo; que organiza la propia iRacing en determinados d√≠as a determinadas horas. Y lo que, para m√≠, es m√°s interesante. El irating que es una valoraci√≥n de cada piloto seg√∫n las posiciones que consigues en cada carrera. Las carreras se dividen en splits de 20-30 coches con pilotos con irating similar. Esto hace que en cada carrera compitas con pilotos de un nivel similar al tuyo y que vayas progresando poco a poco. Tambi√©n se valora el Safety rating que penaliza los incidentes de carrera, evitando que los &ldquo;Maldonado&rdquo; de turno fastidien la carrera de los dem√°s en la medida de lo posible. El nivel que he visto en esta plataforma es bestial y prueba de ello, es que pilotos profesionales est√°n &ldquo;jugando&rdquo; en iRacing, no s√© si para entrenar o simplemente para entretenerse.
Por ejemplo en esta carrera que pude ver en directo hace unos d√≠as del streamer espa√±ol Keny500
En esta carrera los 4 primeros clasificadores eran:
Max Verstappen piloto oficial de Red Bull Racing de Formula 1, que 2 d√≠as antes de esa carrera virtual hab√≠a ganado el Gran Premio do Brasil de F1 (en el mundo real) Lando Norris piloto oficial de McLaren F1 (compa√±ero del espa√±ol Carlos Sainz) Bruno Spengler piloto canadiense de la DTM Dani Juncadella piloto de DTM y ganador del Gran Premio de Macao en 2011 De hecho Lando Norris streamea en Twitch sus carreras en iRacing (y otros juegos)
Y parece que recientemente Carlos Sainz, piloto oficial de McLaren F1, tambi√©n se ha unido a este grupo: Aprovecho para dejar un lista de youtubers / streamers que emiten habitualmente carreras que en muchas ocasiones son igual o m√°s entretenidas (y competidas) que las reales.
Heikki360 YouTube | Twitch Kenny500 YouTube | Twitch `,url:"https://sergiocarracedo.es/2019/12/11/simracing-mas-real-que-virtual/",image:"/2019/12/11/simracing-mas-real-que-virtual/simracing-12795_hu_76d1653be237be1a.jpg",tags:["esports","lifestyle"],readingTime:"5 minutes read",date:"Dec 11, 2019"},"https://sergiocarracedo.es/2019/12/05/tu-propio-v-model-en-un-componente-vue/":{title:"Tu propio v-model en un componente Vue (The right way)",content:`Si has usado componentes de Vue (o incluso un input b√°sico) habr√°s usado v-model para enlazar un valor en el componente padre y en el componente hijo.
&lt;input v-model=&#34;message&#34;&gt; // o &lt;datepicker v-model=&#34;date&#34;&gt;&lt;/datepicker&gt; Si en el componente padre (aquel donde incluimos datepicker) modificamos el valor de date autom√°ticamente se modificar√° el valor dentro del componente, de igual forma si el componente modifica el valor de date el valor se modificar√° en el padre. Esto lo que permite, por ejemplo, es que cuando en un campo input escribimos algo en el componente padre se pueda mostrar lo que escribimos usando {% raw %}{{ date }}{% endraw %}
Pues bien, realmente v-model es la combinaci√≥n de la prop value y el evento input, es decir para que nuestro componente disponga de su v-model debe tener un aspecto similar a este
&lt;template&gt; &lt;label&gt;Fecha: &lt;/label&gt; &lt;input v-model=&#34;value&#34; @change=&#34;$emit(&#39;input&#39;, value)&#34;&gt; &lt;/template&gt; &lt;script&gt; export default { props: { value: { required: true } } } &lt;/script&gt; ¬øY ya est√°? Pues no, no est√°. Si recuerdas, las propiedades de un componente deben ser inmutables (One-way data flow) desde el propio componente, y si te fijas meti√©ndolo dentro del v-model del input cada vez que se escriba algo en el, el valor de la propiedad cambiar√°, y esto (aunque funciona) genera un error como el siguiente en la consola:
Error message: Avoid mutating a prop directly since the value will be overwritten whenever the parent component re-renders. Instead, use a data or computed property based on the prop&rsquo;s value.
Para resolver esto nuestro componente debe usar una variable interna intermedia, que tome el valor de la propiedad de inicio y que cuando cambie emita el evento, tambi√©n debemos tener en cuanta que cuando el valor de la propiedad cambie la variable local tome el nuevo valor.
La forma m√°s simple de hacerlo es con una variable interna, a la que llamaremos localValue y dos watchers, uno para emitir el evento input con el valor de localValue y otro para modificar el valor de localValue si el valor de value cambia desde el exterior.
Y el componente nos quedar√° as√≠:
&lt;template&gt; &lt;label&gt;Fecha: &lt;/label&gt; &lt;input v-model=&#34;localValue&#34;&gt; &lt;/template&gt; &lt;script&gt; export default { props: { value: { required: true } }, data () { return { localValue: this.value } }, watch: { localValue (newValue) { this.$emit(&#39;input&#39;, newValue) }, value (newValue) { this.localValue = value } } } &lt;/script&gt; Como ves eliminamos el @change para que el cambio en el &lt;input&gt; pase a ser controlado en uno de los watchers.
A primera vista podr√≠a parecer que esto provocar√≠an un bucle infinito, ya que al modificar localValue emitimos el evento que modifica el valor de value en el componente padre y este a su vez modifica de nuevo localValue repiti√©ndose otra vez, pero los watchers solo se ejecutan cuando el valor cambia, es decir como el valor de localValue y value coincide una vez emitido el evento input el segundo watcher no se dispara deteniendo el bucle.
Podemos hacerlo usando propiedades calculadas (computed), empleando los getters y setters de estas:
... computed: { computedValue: { set(value) { this.localValue = value }, get() { return this.localValue } } } ... pero seguimos necesitando la variable localValue y el watcher de value, con lo cual para mi gusto, a parte de no ahorrar c√≥digo, complicamos la legibilidad.
De sencilla esta forma cumplimos con la regla de One-Way Data Flow sin complicar demasiado el c√≥digo.
Como √∫ltimo apunte podr√≠amos llevar esto a un mixin:
// custom.vmodel.mixin.js export default { props: { value: {} }, data () { return { localValue: this.value } }, watch: { localValue (value) { this.$emit(&#39;input&#39;, value) }, value (value) { this.localValue = value } } } y solo lo tendr√≠amos que importar en nuestro componente que quedar√≠a as√≠:
&lt;template&gt; &lt;label&gt;Fecha: &lt;/label&gt; &lt;input v-model=&#34;localValue&#34;&gt; &lt;/template&gt; &lt;script&gt; import customvmodelMixin from &#39;./custom.vmodel.mixin.js&#39; export default { mixins: [customvmodelMixin] props: { //Otras propiedades }, data () { return { // M√°s variables } }, } &lt;/script&gt; `,url:"https://sergiocarracedo.es/2019/12/05/tu-propio-v-model-en-un-componente-vue/",image:"/2019/12/05/tu-propio-v-model-en-un-componente-vue/vmodel-256302_hu_a590fe4626716d73.jpg",tags:["vue","js"],readingTime:"4 minutes read",date:"Dec 5, 2019"},"https://sergiocarracedo.es/2019/12/03/programacion-funcional-en-php/":{title:"Programaci√≥n funcional en PHP: Un poco de teor√≠a y Lambdish/phunctional",content:`Hace unas semanas @felixgomexlopez me habl√≥ de una librer√≠a en PHP que te ofrec√≠a un mont√≥n de funciones para manejar iterables y funciones de primera clase (u orden superior) en programaci√≥n funcional, as√≠ que la he probado y (spoiler) me ha encantado.
Antes de nada veamos algunos conceptos importantes de la programaci√≥n funcional:
Funciones de primera clase o de orden superior Son aquellas que pueden recibir como par√°metros otra funci√≥n, por ejemplo la funci√≥n &lsquo;array_map&rsquo; de PHP, recibe dos argumentos, un callable y un array, devolviendo una matriz con el resultado de aplicar el callable (normalmente una funci√≥n) a cada elemento de la matriz original.
Funciones puras Son aquellas que no tienen efectos secundarios, es decir aplican el concepto matem√°tico de funci√≥n en el que dado un valor/es de entrada aplicamos un conjunto de operaciones y obtenemos un resultado. Y ese resultado es siempre el mismo para la misma entrada, el resultado no puede depender de un estado interno o de una lectura interna de datos.
Esto nos da ventajas como poder cachear sin miedo el valor del resultado para un conjunto de par√°metros de entrada ya que el resultado siempre ser√° el mismo.
// Funci√≥n pura function suma($a, $b) { return $a + $b } //Funci√≥n impura function sumaFactor($a, $b) { return ($a + $b) * $_ENV[&#39;factor&#39;] } La segunda funci√≥n es impura por que cada vez que la llamamos devuelve un valor que depende de algo externo que podria cambiar.
Clausuras o closures Otra de las piezas importantes de la programaci√≥n funcional en PHP (y otros lenguajes) las closures. Las closures permiten a una funci√≥n acceder a las variables del √°mbito donde es ejecutada.
Por ejemplo:
$text = &#39;El n√∫mero es: &#39;; $function = function($number) use ($text) { return $text . $number; } $function(12); //Prints &#39;El n√∫mero es: 12&#39; Donde el valor de $text es el del contexto de ejecuci√≥n. (Este es un ejemplo chorra). En PHP las closures se representan internamente como una clase.
Funciones Œª (lambda) Tambi√©n llamadas funciones an√≥nimas, son aquellas que no tienen un nombre y se invocan, o bien directamente o almacenando su referencia en una variable.
(function($a) { echo &#39;El n√∫mero es &#39;. $a * 2; })(12); // Prints &#39;El n√∫mero es 24 Vale ya de teor√≠a! Vamos Despu√©s de esta peque√±a introducci√≥n a la programaci√≥n funcional seguro que has visto que la has usado m√°s de una vez, a lo mejor sin darte cuenta.
Estoy convencido de que lo has hecho, y si no deber√≠as.
Como dec√≠a al principio, @felixgomexlopez me habl√≥ de una librer√≠a PHP que nos da herramientas para trabajar de forma m√°s c√≥moda con programaci√≥n funcional en PHP.
Esta librer√≠a es Phunctional y se instala de una forma tan simple como:
$ composer require Lambdish/phunctional Nos aporta en muchos casos funcionalidades ya presentes de alguna forma en PHP pero de forma que homogeniza la forma de pasar par√°metros.
Ya habl√© hace en unos a√±os en una lightning en un PHPVigo de las verg√ºenzas de PHP, con respecto a algunas de las funciones de manejo de arrays, por ejemplo
array_map ( callable $callback , array $array1 [, array $... ] ) : array array_reduce ( array $array , callable $callback [, mixed $initial = NULL ] ) : mixed Que como veis, no mantiene una consistencia en el orden de los par√°metros.
Por lo contrario, Lambdish\\Phunctional si la mantiene
Lambdish\\Phunctional\\map( callable $fn, $coll ) : array Lambdish\\Phunctional\\reduce( callable $fn, $coll, $initial = null ) : array S√≥lo por esto vale la pena usarlo &#x1f602;
Ya en serio nos provee de mogoll√≥n de funciones chulas que nos simplifican la vida, por ejemplo:
&lt;?php use function Lambdish\\Phunctional\\group_by; group_by(&#39;strlen&#39;, [&#39;manzana&#39;, &#39;patata&#39;, &#39;mel√≥n&#39;, &#39;jam√≥n&#39;, &#39;fresa&#39;, &#39;mandarina&#39;]); // [5 =&gt; [&#39;mel√≥n&#39;, &#39;jam√≥n&#39;, &#39;fresa&#39;], 6 =&gt; [&#39;patata&#39;], 7 =&gt; [&#39;manzana&#39;], 9 =&gt; [&#39;mandarina&#39;] Que nos devuelve un nuevo array con los elementos agrupados por el valor resultado de aplicar a cada elemento la funci√≥n, en este caso strlen
Tambi√©n podemos encadenar varias funciones:
&lt;?php use function Lambdish\\Phunctional\\do_if; use function Lambdish\\Phunctional\\each; each( do_if( function ($item) { print $item . PHP_EOL; }, [ function($item) { return $item &gt; 10; }, function($item) { return is_numeric($item) &amp;&amp; !is_string($item); } ]), [ 11, 3, &#39;14&#39;, &#39;a&#39;, 40] ); En este caso, recorremos el array y aplicamos a cada elemento do_if que lo que hace es ejecutar la primera funci√≥n si el resultado de todas las funciones indicadas en el array es true
Como veis las posibilidades son muchas sobre todo para trabajar con Colecciones (arrays, objectos o generadores).
No voy a hacer una descripci√≥n de cada funci√≥n una a una, porque no tiene demasiado sentido ya que ten√©is la documentaci√≥n completa de todas las funciones disponibles en https://github.com/Lambdish/phunctional/blob/master/docs/docs.md y el objetivo de este post es dar a conocer un poco la programaci√≥n funcional en PHP y esta fant√°stica librer√≠a.
P.D. Uno de los principales contribuidores del proyecto es Rafa G√≥mez, al que he tenido el placer de conocer en persona en la PulpoCon 19
`,url:"https://sergiocarracedo.es/2019/12/03/programacion-funcional-en-php/",image:"/2019/12/03/programacion-funcional-en-php/phunctional-33317_hu_b1ca4dd19a926ea6.jpg",tags:["php"],readingTime:"4 minutes read",date:"Dec 3, 2019"},"https://sergiocarracedo.es/2019/11/01/php-reflection/":{title:"PHP Reflection: Anotaciones, PHPDoc y mucho m√°s",content:`La reflex√≠√≥n en PHP es algo que nunca he usado, y que posiblemente no usar√© m√°s all√° de curiosear su funcionamiento como en este post (como otras muchas veces es posible que me trague estas palabras &#x1f604;), y creo que la mayor√≠a de vosotros tampoco ha usado.
Esta este conjunto de clases est√° con nosotros desde PHP 5.0, es decir desde julio de 2004 (hace ahora m√°s de 15 a√±os &#x1f613;) y nos proveen un sistema para hacer ingenier√≠a inversa de clases, interfaces, m√©todos, propiedades, generadores, incluso extensiones del lenguaje.
Para obtener informaci√≥n de cada una de las partes de nuestro c√≥digo debemos usar una de las clases definidas:
ReflectionFunction: Para obtener informaci√≥n de una funci√≥n. ReflectionGenerator: Para obtener informaci√≥n de un generador. ReflectionExtension y ReflectionZendExtension: Para obtener informaci√≥n de una extensi√≥n. ReflectionClass y ReflectionObject: Para obtener informaci√≥n de una clase y un objecto instanciado respectivamente. Vamos a centrarnos en ReflectionClass y veamos una clase muy sencilla
define(&#39;DEFAULT_RADIUS&#39;, 20); interface Shape { public function area():float; } /** * Clase que representa un Circulo */ final class Circle implements Shape { // Constante PI const PI = 3.1415; /** * Radio del circulo */ private $radius; /** * @param $radius Radio del circulo */ public function __construct(float $radius = DEFAULT_RADIUS){ $this-&gt;radius = $radius; } /* Devuelve el radio del circulo */ public function area(): float { return pow($this-&gt;r, 2) * $this-&gt;PI; } } Para obtener informaci√≥n de esta clase en primer lugar debemos instanciar ReflectionClass pasando el nombre de la clase a &ldquo;investigar&rdquo; en el constructor
$reflection = new ReflectionClass(Circle::class); Y usando los m√©todos de esta clase podremos obtener distintos datos de la clase, por si queremos obtener la lista de todos los m√©todos.
var_dump($refection-&gt;getMethods()); array(2) { [0]=&gt; object(ReflectionMethod)#2 (2) { [&#34;name&#34;]=&gt; string(11) &#34;__construct&#34; [&#34;class&#34;]=&gt; string(6) &#34;Circle&#34; } [1]=&gt; object(ReflectionMethod)#3 (2) { [&#34;name&#34;]=&gt; string(4) &#34;area&#34; [&#34;class&#34;]=&gt; string(6) &#34;Circle&#34; } } Como vemos nos devuelve un array de instancias de la clase ReflectionMethod que a vez tambi√©n nos devolver√° informaci√≥n sobre la cada m√©todo, por ejemplo le pedimos que nos de la lista de p√°rametros del primer m√©todo de la lista (en este caso el constructor)
var_dump($reflection-&gt;getMethods()[0]-&gt;getParameters()); array(1) { [0]=&gt; object(ReflectionParameter)#2 (1) { [&#34;name&#34;]=&gt; string(6) &#34;radius&#34; } } De nuevo tenemos otro array pero en este caso de instancias de ReflectionParameter. Sigamos jugando, Vamos a consultar:
el tipo de dato del par√°metro si tiene un valor por defecto si es opcional (esta petici√≥n puede ser un poco redundante, ya que si tiene valor por defecto es obviamente un par√°metro opcional) si tiene valor por defecto que nos diga si este est√° asignado por una constante (y su nombre). $parameter = $reflection-&gt;getMethods()[0]-&gt;getParameters()[0]; var_dump((string)$parameter-&gt;getType()); // string(5) &#34;float&#34; var_dump($parameter-&gt;getDefaultValue()); // int(20) var_dump($parameter-&gt;isOptional()); // bool(true) var_dump($parameter-&gt;getDefaultValueConstantName()); // string(14) &#34;DEFAULT_RADIUS&#34; Como veis podemos obtener una informaci√≥n exhaustiva de la clase, m√©todos, par√°metros, etc. Cosas como por ejemplo en que l√≠nea empieza (getStartLine()) o acaba (getEndLine()) la definici√≥n de la clase o el nombre del fichero (getFileName()).
Recomiendo echar un vistazo a la lista completa de m√©todos disponibles: https://www.php.net/manual/es/class.reflectionclass.php
Los comentarios: PHPDoc y las anotaciones Entre esa lista de m√©todos que nos devuelven informaci√≥n de la clase (o funci√≥n o m√©todo) est√° getDocComment que nos devuelve el comentario de documentaci√≥n del elemento, &#x26a0;&#xfe0f; y ojo que lo he puesto en negrita por que nos devuelve exactamente eso, no nos va a devolver cualquier comentario.
PHP considera que un bloque de comentario es de documentaci√≥n cuando comienza por /**
Veamos un ejemplo:
// Comentarios de documentaci√≥n del constructor var_dump($reflection-&gt;getMethods()[0]-&gt;getDocComment()); // string(49) &#34;/** // * @param $radius Radio del circulo // */&#34; // Comentarios de documentaci√≥n del m√©todo area() var_dump($reflection-&gt;getMethods()[0]-&gt;getDocComment()); // bool(false) Si os fij√°is en el m√©todo area el bloque de comentario comienza por /* en lugar de /** y el m√©todo lo ignora.
Como os pod√©is imaginar este m√©todo es el que ha permitido la creaci√≥n de &ldquo;cosas&rdquo; como PHPDoc o traer las anotaciones como tienen de forma nativa otros lenguajes, pero tened en cuenta que el m√©todo getDocComment devuelve el comentario tal cual, no lo interpreta o parsea, ah√≠ es donde entran en juego las clases como doctrine/annotations que se encargan de realizar dicho parseo.
Como curiosidad indicar que hay varias propuestas de RFC para llevar las anotaciones de forma nativa al int√©rprete del lenguaje, algunas de ellas bastante longevas, que a√∫n siguen en Draft y no han sido votadas para llevarlas realmente al int√©rprete.
https://wiki.php.net/rfc/annotations https://wiki.php.net/rfc/attributes https://wiki.php.net/rfc/annotations_v2 En resumen, las clases de reflexi√≥n son √∫tiles y muy interesantes, pero como dec√≠a al principio, y a riesgo de tragarme mis palabras, son algo que la mayor√≠a, posiblemente, nunca use en un proyecto real, aunque nunca se sabe y siempre es bueno conocer las herramientas que nos da el lenguaje.
`,url:"https://sergiocarracedo.es/2019/11/01/php-reflection/",image:"/2019/11/01/php-reflection/php-reflection-949215_hu_987b0cb7c3b1d696.jpg",tags:[],readingTime:"4 minutes read",date:"Nov 1, 2019"},"https://sergiocarracedo.es/2019/10/26/composition-api-las-novedades-de-vue-3-en-vue-2-x/":{title:"Composition API: Las novedades de Vue 3 en Vue 2.x",content:`Hace ahora casi un a√±o, Evan You, creador de Vue, durante la VueConf present√≥ una preview de lo que ser√° Vue 3 cuyas principales lineas maestras son:
M√°s r√°pido M√°s peque√±o Un c√≥digo m√°s mantenible M√°s amistoso con entornos nativos (Android, iOS, Desktop, etc.) M√°s f√°cil de usar Una de las novedades, que nos traer√° es lo que en ese momento se llam√≥ Hooks API (inspirado en React Hooks), y que ha pasado a llamarse Composition API.
Esta nueva API nos permite simplificar los componentes y facilitar reusar c√≥digo, sobre todo tendr√° repercusi√≥n en componentes de tama√±o medio y grande ya que permitir√° extraer parte de la l√≥gica de forma sencilla en varios ficheros, reusarla y organizar mejor el c√≥digo por conceptos l√≥gicos en lugar de por opciones. Actualmente en Vue 2.x tenemos que organizar el c√≥digo de un componente por opciones, de la siguente manera:
export default { name: &#39;component-name&#39;, props: { ... }, components: { }, data () { return { ... } }, computed: { .... }, watch () { .... }, methods: { .... }, mounted () { } .... } Esto, en componentes de cierta complejidad, hace que, por ejemplo, las variables y los m√©todos que las gestionan esten separados.
Veamos con un ejemplo simple a que me refiero, un componente que muestra lo que escribimos en un input, lo muestra en may√∫sculas y adem√°s tenemos un bot√≥n para a√±adir un emoji:
Si os fijais tenemos por un lado la variable text, en otro la variable computada uppercase y en otroel m√©todo addEmoji. En este componente estan todos bastante &ldquo;cerca&rdquo;, pero si seguimos a√±adiendo metodos y variables empiezan a separarse.
Con Composition API podemos tener &ldquo;cerca&rdquo; todos estos elementos por l√≥gica o funcionalidad (En el video que enlazo al final se ve esto de forma muy gr√°fica)
Antes de continuar, debemos saber que en Vue 2.x, para poder emplear Composition API necesitamos instalar el paquete vue/composition-api con un simple:
yarn add @vue/composition-api Una vez hecho esto refactorizamos el componente de la siguiente forma
En el &lt;template&gt; no ha cambiado nada significativo, as√≠ que vamos al javascript que es donde est√° la chicha.
En primer lugar vemos un import que nos trae: reactive, computed y ref, en seguida vamos a ver que es cada uno
A continuaci√≥n vemos un metodo setup(), nada de las opciones &ldquo;tradicionales&rdquo; (data, methods, computed, etc), y aqu√≠ es donde haremos la magia del composition API. En lugar de tener una opci√≥n en el objeto para cada cosa: data, computed, methods los definimos directamente en el m√©todo setup() y los devolvemos. Vamos paso por paso:
const text = ref(&#34;&#34;) Vue necesita que el elemento reactivo sea pasado por referencia, cosa que javascript no hace con los tipos primitivos como en este caso (un String), para ello Composition API nos provee el ref().
En el caso de que us√°semos un objeto que quisi√©semos que fuese reactivo, usar√≠amos &lsquo;reactive&rsquo;
const state = reactive({ text: &#39;&#39; }) Lo siguiente que nos encontramos es la variable computada:
const uppercase = computed(() =&gt; text.value.toUpperCase()); Sencillo, usamos el computed y le pasamos la funci√≥n computada y transformamos el valor de la variable text a may√∫sculas.
Aqu√≠ ya podemos ver un par de detalles:
el .value: esto es por qu√© ref convierte el tipo primitivo en un objeto plano, guardando el valor real en la propiedad value. N√≥tese que en los templates no es necesario usar el .value ya que se realiza un ref unwrapping Otro detalle importante es ¬ød√≥nde esta el this?, pues b√°sicamente no est√°, setup() es llamado antes de montar el componente, por lo que en ese contexto this no funciona como lo conoc√≠amos. Un poco m√°s adelante tenemos lo que antes era el m√©todo addEmoji que ahora es una funci√≥n:
function addEmoji() { text.value += &#34;üòÇ&#34;; } y finalmente devolvemos todos los elementos que vamos a usar:
return { text, uppercase, addEmoji }; A primera vista parece todo un poco m√°s complicado y engorroso, y seguro que lo es para componentes peque√±os, pero en cuanto el componente empieza a crecer esto nos permite agrupar la l√≥gica de cada parte del componente en lugar de tenerla separada en las distintas propiedades del objeto, adem√°s nos simplifica extraer tanto las variables reactivas y computadas, como los m√©todos a un fichero externo:
import compositionFunction from &#34;./demoCompositionApi.js&#34;; export default { setup() { return { ...compositionFunction() }; } }; Se√±alar que el m√©todo tradicional seguir√° estando disponible y no hay planes a largo plazo de eliminarlo, por lo que podremos usar uno u otro m√©todo de desarrollo. Esto es algo que Evan You ha dejado muy claro despues de el denominado &ldquo;Vue Darkest Day&rdquo;, en el que, con el anuncio del Composition API la comunidad interpret√≥ que se eliminar√≠a el sistema &ldquo;tradicional&rdquo; y gener√≥ bastante pol√©mica.
Recomiendo ver este video para entender de una forma muy gr√°fica las ventajas del Composition API
Tambien consultar el RFC de Composition API
Os dejo el proyecto demo en codesandbox:
`,url:"https://sergiocarracedo.es/2019/10/26/composition-api-las-novedades-de-vue-3-en-vue-2-x/",image:"/2019/10/26/composition-api-las-novedades-de-vue-3-en-vue-2-x/composition-api-414579_hu_5183ac040da890b8.jpg",tags:[],readingTime:"4 minutes read",date:"Oct 26, 2019"},"https://sergiocarracedo.es/2019/10/06/mi-entorno-de-trabajo-hardware-software/":{title:"Mi entorno de trabajo: Hardware & Software",content:`Es raro que, tras alg√∫n meetup, en las ca√±as de despu√©s, no se hable de el hardware y software que cada uno usa, normalmente por que alguien est√° pensando en cambiar un dispositivo o por que se comentan las ventajas de tal o cual IDE.
As√≠ que me he decidido a hacer un peque√±o inventario de hardware y software de mi entorno de desarrollo para no olvidarme de nada.
Hardware Aunque dispongo de un port√°til, normalmente desarrollo en un PC de escritorio que uso a veces tambien para jugar:
Procesador: Intel i3 de 4¬∫ generaci√≥n. Memoria: 32GB de RAM. Grafica: Nvidia GeForce GTX 950 2GB Almacenamiento: 1 SSD de 120GB para el sistema, 1 HDD de 1TB para las webs y el directorio home, 1 HDD de 500GB para los backups (tanto locales como remotos) y un SSD de 80GB para los juegos Monitores: Uso dos monitores de 24&quot; con resoluci√≥n FullHD (1920x1080px) Teclado: Krom Kernel (un teclado m√©canico del que ya he hablado por aqui Rat√≥n: Un Logitech MX Master Mesa: Ikea Malm Silla: Silla 1337 Indutries Flexo: Ikea FORS√Ö &ldquo;Tuneada&rdquo; para que se encienda y apague automaticamente Microfono: Micr√≥fono de mesa Plantronics Webcam: No tengo Software Sistema operativo: Ubuntu LTS IDE: PHPStorm / SublimeText (para cosas puntuales) Terminal: Terminator Shell: OhMyZSH Con el tema Agnoster Correo: Wavebox Comunicaci√≥n: Slack, Telegram Desktop Editor gr√°fico: Firework (mediante Wine) Navegadores: Chrome, Firefox Gestor SQL: SQLYog (mediante Wine) Almacenamiento en la nube: pCloud y Dropbox Herramienta de despliegue favorita: Deployer En el tema de software no he mencionado las herramientas t√≠picas: git, npm, composer, etc. por que son bastante est√°ndar, y seguro que me olvido de alguna.
¬øEs muy diferente al que tu usas?
`,url:"https://sergiocarracedo.es/2019/10/06/mi-entorno-de-trabajo-hardware-software/",image:"/2019/10/06/mi-entorno-de-trabajo-hardware-software/entorno-trabajo-1432673_hu_68a354844aab17c5.jpg",tags:[],readingTime:"2 minutes read",date:"Oct 6, 2019"},"https://sergiocarracedo.es/2019/10/02/simplificando-la-gestion-del-router-de-vue/":{title:"Simplificando la gesti√≥n del router de Vue",content:`Cuando creamos nuestra aplicaci√≥n Vue una de las cosas que debemos configurar, si nuestra app dispone de varias p√°ginas, es el router.
Vue dispone de un router propio lo suficientemente bueno para que no necesites usar ning√∫n otro y que se configura de una forma similar a esta:
import Vue from &#39;vue&#39; import Router from &#39;vue-router&#39; import Login from &#39;../pages/Login&#39; import Home from &#39;../pages/Home&#39; import Article from &#39;../pages/Article&#39; Vue.use(Router) new Router({ routes: [ { path: &#39;/login&#39;, name: &#39;login&#39;, component: Login }, { path: &#39;/&#39;, name: &#39;home&#39;, component: Home, meta: { auth: true } }, { path: &#39;/articulo/:slug&#39;, name: &#39;home&#39;, props: true, component: Article, meta: { auth: true } }, ] }) Como vemos las cada ruta se define en un Objeto dentro de un Array, indicando el path al que responder√°, el nombre y el componente que mostrar√° dicha p√°gina o vista, adem√°s de otras muchas configuraciones que podr√≠a tener.
Si nuestra aplicaci√≥n tiene muchas p√°ginas este array se volver√° un poco complicado de manejar.
Si habeis trabajado con Nuxt.js conocer√©is que en su caso, con colocar los componentes en la carpeta pages, ya se genera de forma autom√°tica la ruta con el nombre del componente, que adem√°s podemos meter en distintos niveles de carpetas.
Pues ese comportamiento es lo que vamos a replicar de una forma m√°s b√°sica para cualquier aplicaci√≥n Vue que no necesite emplear Nuxt.js
En primer lugar modificamos el router.js, vamos a explicarlo por partes:
Importamos Vue y Vue router y le indicamos a Vue que lo use, nada extra√±o aqu√≠:
import Vue from &#39;vue&#39; import Router from &#39;vue-router&#39; Vue.use(Router) Haciendo uso de la gesti√≥n de dependencias de Webpack, en concreto de require.context(), que nos permite, en nuestro caso, obtenemos un contexto con todos los archivos *.vue en la carpeta /pages que es donde guardamos las p√°ginas de nuestra App.
Esto s√≥lo funciona en un entorno webpack si usamos Vue directamente en el navegador require.context no estar√° definido.
const files = require.context(&#39;../pages/&#39;, false, /\\.vue$/) const pages = [] files.keys().forEach((key) =&gt; { if (key === &#39;./index.js&#39;) return pages[key.replace(/(\\.\\/|\\.vue)/g, &#39;&#39;)] = files(key).default }) A continuaci√≥n vamos a√±adiendo rutas de vue-router en funci√≥n de los archivos que tenemos en nuestro contexto, usando el nombre del fichero como ruta (en min√∫sculas) y para el nombre de la ruta el nombre indicado en el componente o el nombre del fichero, y, obviamente el propio componente como componente que gestionar√° esa ruta.
const routes = [] Object.keys(pages).forEach(page =&gt; { const route = { path: &#39;/&#39; + page.toLowerCase() name: pages[page].name.toLowerCase || page.toLowerCase() component: pages[page] } routes.push(route) }) Ahora s√≥lo tenemos que instanciar el router
const router = new Router({ routes: routes }) Y voil√† cada componente que a√±adamos a la carpeta /pages tendr√° su propia ruta sin necesidad de hacer nada m√°s.
Pero, ¬øy si quiero personalizar la ruta, a√±adirle par√°metros, o metas?. Un paso m√°s Lo que acabamos de hacer es c√≥modo pero poco flexible, ya que no podr√≠amos configurar por ejemplo una ruta que contenga par√°metros o indicarle un meta.requireAuth para que el router mediante un guard compruebe si el usuario esta sogueado.
Para solucionarlo lo que podemos hacer es definir en nuestro componente una key llamada &lsquo;route&rsquo; que contenga esa configuraci√≥n y que si esta existe nuestro generador de rutas lo lea y aplique, y si no que use el procedimiento por defecto.
Modificamos la parte de generaci√≥n de rutas
const routes = [] Object.keys(pages).forEach(page =&gt; { let route = {} if (pages[page].route !== undefined) { route = pages[page].route } else { route = { path: &#39;/&#39; + page.toLowerCase() } } route.component = pages[page] routes.push(route) }) Y en nuestro componente, por ejemplo Articule.vue a√±adimos la key de esta forma:
&lt;template&gt; .... &lt;/template&gt; &lt;script&gt; export default { .... route: { path: &#39;/articulo/:slug&#39;, name: &#39;article&#39;, props: true, meta: { auth: true } } ... } &lt;/script&gt; Y as√≠ el generador de rutas que hemos creado usar√° esa configuraci√≥n.
De esta forma adem√°s tenemos la configuraci√≥n de la p√°gina en el propio componente.
Esto podr√≠amos ir iterandolo para permitir, por ejemplo, como hace Nuxt.js crear rutas a partir de las carpetas y dem√°s, pero creo que esta ya fuera del objetivo y ¬øpor qu√© no usar Nuxt.js directamente?
`,url:"https://sergiocarracedo.es/2019/10/02/simplificando-la-gestion-del-router-de-vue/",image:"/2019/10/02/simplificando-la-gestion-del-router-de-vue/vue-router-2742048_hu_e7632aae7d5baed0.jpg",tags:[],readingTime:"4 minutes read",date:"Oct 2, 2019"},"https://sergiocarracedo.es/2019/09/26/pulpocon-19/":{title:"Mi edificante experiencia ayudando a organizar la PulpoCon19",content:`El pasado 7 de septiembre, se celebr√≥ la primera PulpoCon una conferencia para programadores que he tenido el honor de ayudar a organizar.
Poco puedo a√±adir a la historia de como se forj√≥ el evento a lo descrito en este estupendo post de Rolando Caldas, que os recomiendo encarecidamente leer ‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è
La batalla de organizar la pulpoCon
S√≥lo puedo corroborar todo lo ah√≠ dicho y ahondar a√∫n m√°s en los agradecimientos a todas las personas que nos han ayudado a montar esto.
Por a√±adir algo, me gustar√≠a contar un poco de mi experiencia a nivel personal de lo que ha significado la PulpoCon19.
Los que me conocen saben que no estoy pasando por un buen momento, ha sido un a√±o especialmente complicado en lo que respecta a salud.
Cuando a principios de a√±o surgi√≥ la idea organizar un evento en Vigo para traer a grandes de este mundillo, yo estaba recuper√°ndome de un linfoma y no sabia como me iba a encontrar para poder participar en &ldquo;todo esto&rdquo;, pero tenia claro que intentar√≠a hacer lo que estuviese en mi mano.
Y as√≠ fue: lo poco que pudo estar en mi mano lo fui m√°s o menos haciendo, todo ello siendo consciente, y es algo que quiero decir alto y claro, de que la PulpoCon19 no ser√≠a posible sin el trabajo de F√©lix G√≥mez y Rolando Caldas, tambi√©n co-organizadores de PHPVigo.
As√≠ fueron pasando las semanas y los meses preparando cosas (lo que cuenta Rolando de forma magistral en su post)
Leg√≥ julio y yo &ldquo;explot√©&rdquo; a nivel mental con varias crisis de ansiedad, lo que no me permit√≠a ser capaz de sobrellevar cualquier tensi√≥n o tarea que implicase tensi√≥n.
Cuento esto para poder agradecer de nuevo a F√©lix y Rolando que me permitiesen continuar &ldquo;dentro&rdquo; de esto, a pesar de la poca ayuda que yo pod√≠a aportar en esos momentos.
Esto me lleva a lo que me lleva a lo siguiente, a que el hecho de poder seguir organizando el evento es ese momento de &ldquo;baj√≥n&rdquo; me hizo vivir una edificante experiencia desde el punto de vista personal, que de otra forma me hubiese perdido.
A pesar de lo intensos que fueron los d√≠as previos, llevando cajas de un lado para otro, preparando los welcome packs (recuerdo el sal√≥n de mi casa lleno de cajas, camisetas, bolsas, etc), yendo de un lado para otro, casi con la lengua fuera, el recuerdo que guardar√© es fant√°stico.
Como dec√≠a en el t√≠tulo, la experiencia ha sido muy edificante, he podido aprender muchas cosas: como se organiza un evento de este tipo, de lo que cuesta cuadrar las cuentas, conseguir patrocinios, de como la comunidad se vuelca y ayuda m√°s de lo imaginable, de la colaboraci√≥n desinteresada de otras comunidades.
Tengo que mencionar aqu√≠ a mi mujer, Patricia, que se encarg√≥ de decorar la sala de recepciones y el jard√≠n, que aunque eso era parte del patrocinio de la empresa en la que trabaja PartyFiesta, su esfuerzo y dedicaci√≥n con el evento fue mucho m√°s all√° de lo normal, dandonos ideas como usar uno de los globo-pulpo para recibir a los ponentes en el aeropuerto en lugar de el t√≠pico cartel, o usar el photocall como donde hacer fotos divertidas.
Pero guardo como una lecci√≥n el ejemplo que han sido para m√≠ los ponentes, personas que han dedicado un fin de semana de sus vidas a compartir sus conocimientos de forma desinteresada con todos nosotros
Por que si lo piensas fr√≠amente, ¬øqu√© necesidad tiene cualquiera de estar personas, que son referentes en sus empresas y en la profesi√≥n, de venir hasta Vigo a compartir su conocimiento y experiencia profesional? Bueno, podr√©is decirme que el pulpo &#x1f419; y ese desde luego es un gran motivo &#x1f602;, para m√≠ ese esfuerzo es algo que valoro mucho.
Mencionar lo cercanos que han sido todos, su inter√©s por conocer la ciudad, las peque√±as diferencias culturales que hay entre todos los lugares, etc.
Aunque la mayor√≠a ya hab√≠a o√≠do hablar de nuestro amado lider, les hemos contado un poco m√°s, y lo m√°s importante se han podido hacer fotos con el Dinoseto &#x1f602;
Yo espero que a todos ellos les gustase la ciudad, por que era parte de nuestros objetivos dar a conocer la ciudad en todo su contexto, a nivel de comunidad tecnol√≥gica, tejido empresarial, turismo y gastronom√≠a, que eso en Galicia no puede faltar.
Me llevo muchos peque√±os recuerdos y sensaciones: Como cuando llev√© a Javier y Rafa (CodelyTV) y a Jose Armesto en coche a la cena de ponentes y al oirlos hablar parec√≠a que estaba en uno de sus videos. Tambi√©n las risas que nos echamos a cuenta del gorro pulpo y de todos los complementos para hacernos fotos divertidas en el photocall.
Creo que con el tiempo podr√© valorar a√∫n m√°s el haber sido participe de la primera PulpoCon
Muchas gracias a todos: ponentes, voluntarios, asistentes, patrocinadores, personal del recinto, personal del catering, a todos los que participasen de alguna forma en el evento.
`,url:"https://sergiocarracedo.es/2019/09/26/pulpocon-19/",image:"/2019/09/26/pulpocon-19/pulpocon_hu_448a4739fccd4015.jpg",tags:[],readingTime:"4 minutes read",date:"Sep 26, 2019"},"https://sergiocarracedo.es/2019/09/09/desacoplando-la-interaccion-con-una-api-en-vue/":{title:"Desacoplando la interacci√≥n con una API en Vue: vue.$api",content:`Durante el desarrollo de una web, una SPA en Vue es muy habitual que ese desarrollo implique interactuar con una API y lo habitual ser√° que uses axios para ello (aunque lo que voy a explicar valdr√≠a para cualquiera otra librer√≠a)
Es muy habitual que cuando necesitamos interactuar con un endpoint de la API, lo hagamos directamente desde el controlador, entendiendo por controlador el componente que se encarga de responder a una ruta.
Por ejemplo:
.... data() { return { users: [] }, ... methods: ... getUsers(page) { axios({ method: &#39;get&#39;, url: &#39;https://reqres.in/api/users&#39;, data: { page } }).then(res =&gt; { this.users = res.data.data }).catch(res =&gt; { // Do something on error }) } ... Esto cumple su funci√≥n b√°sica que es la de pedirle a la API la lista de usuarios de la p√°gina indicada y guardarlos en users para que este disponible para renderizar.
Se me ocurren varios problemas que nos podemos encontrar a botepronto:
Si necesitamos pedir los usuarios desde varios controladores tendr√≠amos que duplicar este c√≥digo. Si para hacer la llamada necesitamos a√±adir alguna cabecera de autorizaci√≥n, token o similar, tendr√≠amos que saber en todas las llamadas a la API que usen esa autentificaci√≥n aplicarlo y usarlo desde una variable de entorno que deber√≠amos conocer. La gesti√≥n de errores puede ser tediosa, si por ejemplo siempre que la API responda un error lo vamos a mostrar en pantalla de la misma forma tendr√≠amos que programarlo en cada llamada. Lo mismo si usamos loaders en pantalla, tenemos que acordarnos de a√±adirlos cada vez que usemos una llamada a la API. Una soluci√≥n m√°s elegante seria mover las llamadas a la API a funciones en un fichero externo que importamos en cada controlador y de las que hacemos uso:
import Api from &#39;./api.js&#39; .... data() { return { users: [] }, ... methods: ... getUsers(page) { Api.getUsers(page) .then(res =&gt; { this.users = res.data.data }) } ... Y el contenido de api.js ser√≠a:
export default { getUsers: function (page) { return axios({ method: &#39;get&#39;, url: &#39;https://reqres.in/api/users&#39;, data: { page } }) } } Con esto adem√°s de simplificar el c√≥digo ya podr√≠amos reusar la llamada a la API en varios controladores.
Si queremos que cada vez que hagamos una llamada a la API se muestre un cargador en el front (y este desaparezca al terminar la carga), seria razonable usar vuex o similar para esto, entonces nuestro controlador quedar√≠a as√≠
import Api from &#39;./api.js&#39; .... data() { return { users: [] }, ... methods: ... getUsers(page) { this.$store.commit(&#39;loading&#39;, true) Api.getUsers(page) .then(res =&gt; { this.$store.commit(&#39;loading&#39;, false) this.users = res.data.data }) .catch(error =&gt; { this.$store.commit(&#39;loading&#39;, false) }) } ... (Doy por supuesto que hay un store que tiene una mutaci√≥n encargada de mostrar un componente loading en la UI)
Pero, de esta forma volvemos a tener el mismo problema, para cada llamada a la funci√≥n que gestiona la API tenemos que acordarnos de hacer el commit al store tanto al principio de la llamada como cuando devuelve datos como, importante, cuando hay un error.
Vale, llevemos esto a nuestro api.js
import store from &#39;./store.js&#39; // Donde tengamos nuestra store definida export default { getUsers: function (page) { store.commit(&#39;loading&#39;, true) return axios({ method: &#39;get&#39;, url: &#39;https://reqres.in/api/users&#39;, data: { page } }) .then(res =&gt; { store.commit(&#39;loading&#39;, false) }) .catch(error =&gt; { store.commit(&#39;loading&#39;, false) }) } } OJO, pero si dejamos esto as√≠, al capturar la resoluci√≥n de la promesa (con el then y el catch), nuestro controlador no recibir√° dicha resoluci√≥n, por lo que tenemos que reenviarselas de la siguiente forma:
import store from &#39;./store.js&#39; // Donde tengamos nuestra store definida export default { getUsers: function (page) { store.commit(&#39;loading&#39;, true) return axios({ method: &#39;get&#39;, url: &#39;https://reqres.in/api/users&#39;, data: { page } }) .then(res =&gt; { store.commit(&#39;loading&#39;, false) return res // &lt;--- Devolvemos la misma promesa ya resuelta (propagamos) }) .catch(error =&gt; { store.commit(&#39;loading&#39;, false) throw error // &lt;-- Lanzamos de nuevo la misma excepci√≥n }) } } De esta forma el propagar la resoluci√≥n de la promesa el controlador ni se ha enterado que hemos lanzado el loader y lo hemos desactivado.
Vale, pues ahora, porqu√©, en lugar de tener que importar api.js en cada controlador, no hacemos que est√© disponible en toda la app, de la misma forma que lo esta, por ejemplo la store de vuex: &lsquo;vue.$store&rsquo; es decir: &lsquo;vue.$api.getUsers()&rsquo;
Para ello creamos un plugin de vue, que es tan sencillo como lo siguiente en nuestro api.js
import store from &#39;./store.js&#39; // Donde tengamos nuestra store definida import axios from &#39;axios&#39; export default { install (Vue) { Vue.prototype.$api = Api } } const Api = { getUsers: function (page) { store.commit(&#39;loading&#39;, true) return axios({ method: &#39;get&#39;, url: &#39;https://reqres.in/api/users&#39;, data: { page } }) .then(res =&gt; { store.commit(&#39;loading&#39;, false) return res // &lt;--- Devolvemos la misma promesa ya resuelta (propagamos) }) .catch(error =&gt; { store.commit(&#39;loading&#39;, false) throw error // &lt;-- Lanzamos de nuevo la misma excepci√≥n }) } } Y nuestro entrypoint, que suele ser main.js, lo dejamos como algo as√≠:
import Vue from &#34;vue&#34;; import App from &#34;./App.vue&#34;; import Api from &#34;./api.js&#34;; // &lt;---------------- Vue.config.productionTip = false; Vue.use(Api) // &lt;---------------- new Vue({ render: h =&gt; h(App) }).$mount(&#34;#app&#34;); Ahora ya tenemos disponible el acceso al objecto Api en cualquier controlador con solo usar this.$api.getUser(), y en el caso de nuestro ejemplo quedar√≠a
... data() { return { users: [] }, ... methods: ... getUsers(page) { this.$api.getUsers(page) .then(res =&gt; { this.users = res.data.data }) } ... Obviamente nuestro objeto Api puede tener m√°s funciones que hagan llamadas a otros endpoints, tener una forma com√∫n de mostrar errores, etc. Eso se lo dejo a vuestra imaginaci√≥n
Por √∫ltimo, y como spoiler de un pr√≥ximo post, decir que si hacemos uso de muchos endpoints distintos, el fichero api.js puede ser enorme y lo razonable ser√≠a trocearlo, e incluso separar las el objecto de la entidad a la que acceda, por ejemplo this.$api.user.get o this.$api.user.create o this.$api.billing.list o cualquier otro ejemplo que se os ocurra, pero como digo eso da para otro post.
`,url:"https://sergiocarracedo.es/2019/09/09/desacoplando-la-interaccion-con-una-api-en-vue/",image:"/2019/09/09/desacoplando-la-interaccion-con-una-api-en-vue/vue-api_hu_88678f11f83d49f0.jpg",tags:[],readingTime:"5 minutes read",date:"Sep 9, 2019"},"https://sergiocarracedo.es/2019/09/02/Unificando-la-informacion-de-eventos-de-una-comunidad-vigotech-json/":{title:"Unificando la informaci√≥n de eventos de una comunidad: vigotech.json",content:`VigoTech Alliance la comunidad que aglutina a los grupos de tecnolog√≠a de Vigo y su √°rea de influencia, tiene entre uno de sus objetivos coordinar y divulgar las fechas de los eventos de cada uno de los grupos miembros.
En el momento de escribir esta entrada tiene 18 grupos miembros, que funcionan de forma aut√≥noma a la hora de organizar meetups, eventos, charlas, etc, lo que hace que a la hora de mantener la informaci√≥n de los eventos actualizada de forma manual sea una tarea complicada.
Teniendo en cuanta que la mayor√≠a de los grupos usan plataformas para crear y publicitar sus eventos (mayoritariamente Meetup) surgi√≥ la idea de automatizar la recogida de informaci√≥n de los eventos y servirla en lugar √∫nico para que pudiese ser consultada de forma sencilla por cualquiera, y ya puestos, ¬øpor qu√© excluir a las m√°quinas de esto?
De aqu√≠ naci√≥ la idea de crear un archivo (vigotech.json) colgado en el servidor de la web, que sirviese como fuente de informaci√≥n de VigoTech, de sus miembros, sus eventos, sus videos, etc.
Pero por hacerlo un poco mejor se dot√≥ a dicho fichero de un esquema (JSON Schema) que permitiese validarlo.
Resumiendo la estructura, partimos de un nodo ra√≠z que es el propio meta grupo VigoTech y que tiene como propiedades cosas como, el logo, los links a web y redes sociales y sus propios eventos (si, eventos que son de todo el meta grupo y no de un grupo concreto) y lo m√°s importante los miembros.
Los miembros son tambi√©n objectos cuyas propiedades son similares a la anterior, a√±adiendo los videos.
&#34;aindustriosa&#34;: { &#34;name&#34;: &#34;A Industriosa&#34;, &#34;logo&#34;: &#34;https://vigotech.org/images/aindustriosa.png&#34;, &#34;links&#34;: { &#34;web&#34;: &#34;https://aindustriosa.org/&#34;, &#34;twitter&#34;: &#34;https://twitter.com/aindustriosa&#34;, &#34;meetup&#34;: &#34;https://www.meetup.com/es-ES/AIndustriosa/&#34;, &#34;youtube&#34;: &#34;https://www.youtube.com/channel/UC9DPKfcLiNd7SEU-QLlIG7A&#34; }, &#34;events&#34;: [ { &#34;type&#34;: &#34;meetup&#34;, &#34;meetupid&#34;: &#34;AIndustriosa&#34; } ], &#34;videos&#34;: [ { &#34;type&#34;: &#34;youtube&#34;, &#34;channel_id&#34;: &#34;UC9DPKfcLiNd7SEU-QLlIG7A&#34; } ] }, Como pod√©is observar en los eventos y videos, no se indican realmente los eventos, sino la o las fuentes de donde extraer los eventos. En el ejemplo vemos que solo tenemos una fuente de eventos de tipo &ldquo;meetup&rdquo; y que el &ldquo;meetupid&rdquo; es &ldquo;AIndustriosa&rdquo;, lo mismo para los videos.
Esta sistema nos permite flexibilidad de ir a√±adiendo &ldquo;fuentes&rdquo; de eventos, como realmente hemos hecho.
De lo abstracto a lo real Si bien, esta definici√≥n resuelve en parte el problema planteado de unificar la informaci√≥n, pero lo que realmente queremos es obtener los eventos de cada grupo y sus videos, no solo el sitio de donde obtenerlos.
Para ello solo tendr√≠amos que crear un script o app en cualquier lenguaje que se encargue de procesar vigotech.json para obtener esa informaci√≥n.
Pues bien, eso ya est√° hecho: Metagroup schema tools , una herramienta escrita en JS (y tambi√©n disponible como paquete NPM) que se encarga de validar y procesar vigotech.json para generar un nuevo json con toda la informaci√≥n ya &ldquo;mascada&rdquo;, generando algo como lo que hace la web de vigotech: vigotech-generated.json
La herramienta actualmente admite como fuentes de eventos: Meetup, Eventbrite y otro json, como en este ejemplo)
La principal ventaja de esta infraestructura es que una vez que tenemos un centro de informaci√≥n podemos usarla de mucha formas, en los ejemplos he mencionado la web de VigoTech, pero tenemos otra herramienta vigotech-event-bot que es un sencillo bot que se encarga de publicar en twitter los eventos y que obtiene toda la informaci√≥n a partir del vigotech.json
Por ejemplo actualmente s√© est√° desarrollando un nuevo bot desarrollado en PHP que publica los eventos en Twitter, Slack y Telegram. La posibilidades son casi infinitas.
Pensad el esfuerzo que supondr√≠a mantener un calendario com√∫n, una web actualizada al minuto, las publicaciones en redes sociales, si tuviese que hacerse de forma manual. Ser√≠a un esfuerzo tit√°nico.
Para cerrar el post solo dar las gracias a VigoTech por lo que supone para Vigo y Galicia en cuanto difusi√≥n de la tecnolog√≠a y por permitirme ser miembro y participar de forma activa en esta comunidad.
`,url:"https://sergiocarracedo.es/2019/09/02/Unificando-la-informacion-de-eventos-de-una-comunidad-vigotech-json/",image:"/2019/09/02/Unificando-la-informacion-de-eventos-de-una-comunidad-vigotech-json/vigotech_hu_2c02c1f714c60cfc.jpg",tags:[],readingTime:"4 minutes read",date:"Sep 2, 2019"},"https://sergiocarracedo.es/2019/08/26/SirenoGrid-Un-sistema-ligero-de-Grid-CS-basado-en-Grid-Layout/":{title:"Sireno Grid: Un sistema ligero de Grid CSS, basado en Grid Layout",content:`CSS Grid Layout es un est√°ndar del W3C que fue llevado a webkit y blink por Igalia una empresa asentada en Galicia con programadores por todo el mundo y adem√°s tuvimos el honor de organizar una charla [Video] de uno de los desarrolladores encargado de implementar CSS Grid Layout en Chromium/Blink y Safari/Webkit: Manuel Rego
Los que llevamos mucho tiempo maquetando (pero mucho, mucho), recordaremos una √©poca en la que se maquetaba usando tablas, ¬øpor qu√©?, pues porque la mayor√≠a de los dise√±os de web, se pueden trocear en √°reas o zonas cuadradas o rectangulares, algo que un su momento permit√≠an las tablas. Con la llegada de la web sem√°ntica y las mejoras de CSS la maquetaci√≥n con tablas fue marcada como una mala pr√°ctica (que lo era). CSS Grid Layout es &ldquo;como&rdquo; volver a maquetar con tablas pero bien. No voy a entrar en muchos detalles, pero la maquetaci√≥n con tablas se defin√≠a con las etiquetas y aqu√≠ las definimos con las hojas de estilo.
¬øPor qu√©? Un buen d√≠a, hace ya m√°s de un a√±o, cay√≥ en mis manos un proyecto de maquetaci√≥n web, pero en este caso tenia un requisito nada habitual en ese momento, y este era que la maquetaci√≥n estuviese basada en CSS Grid Layout. Yo llevaba tiempo usando Bootstrap para el layout de columnas, ya que me parece muy c√≥modo el hecho de poder definir unos tama√±os para cada &ldquo;bloque&rdquo; seg√∫n el tama√±o de pantalla, y esto no lo quer√≠a perder, as√≠ que decid√≠ a hacer un sistema de grid similar / inspirado en el de Bootstrap, pero en lugar de usar flotantes, como en el caso de Bootstrap 3, o flexbox, como en Bootstrap 4, usar√≠a CSS Grid Layout.
Una vez realizado este proyecto y viendo que esta herramienta funcionaba razonablemente bien, me decid√≠ a usarla en m√°s proyectos y finalmente a compartirla con la comunidad, liber√°ndola a principios de este a√±o.
As√≠ naci√≥ Sireno Grid con un nombre inspirado en una de las estatuas, para mi gusto, m√°s feas de mi ciudad (Vigo), pero a la vez con m√°s personalidad.
Objetivo Sireno Grid tiene un simple objetivo: Servir de estructura de grid responsive para que puedas crear tu layout de forma simple, nada de botones, badgets, etc&hellip; solo el grid, los &ldquo;margenes&rdquo; y el embed de elementos de forma responsive
Como ya coment√© Sireno Grid esta inspirado en Bootstrap y por ello he usado m√°s o menos los mismos nombres de clases y con la misma funci√≥n, para que cambiar de uno a otro sea sencillo. El cambio principal viene en usar la clase .grid-row en lugar de .row para definir la fila, esto lo he hecho pensando en que se puedan usar los dos &ldquo;frameworks&rdquo; si fuese necesario.
Breakpoints Los breakpoints definidos son estos (definidos en una variable SCSS)
$grid-breakpoints: ( xs: 0, lxs: 576px, sm: 768px, md: 992px, lg: 1200px ) !default;
Los containers Una cosa que siempre tenia que modificar o a√±adir sobre Bootstrap eran los containers, por defecto Bootstrap 4 solo tiene dos container uno fluid que ocupa el 100% del ancho del navegador y el fijo que adquiere distintos tama√±os seg√∫n el brakpoint usado.
En ninguno de los proyectos esto me encajaba al 100%, el que m√°s encajaba es el de un container fluido, pero con un tama√±o m√°ximo, pero ello Sireno Grid dispone de un &lsquo;.container-fluid&rsquo; que se ajusta al 100% del viewport y un &lsquo;.container-fluid-1920&rsquo; y un &lsquo;.container-fluid-1440&rsquo; que se ajustan al 100% del viewport con un m√°ximo de 1920px y 1440px respectivamente.
Los gutters o grid-gap Por defecto el .grid-row establece un espacio entre columnas o &ldquo;gutter&rdquo; de 15px, pero en ocasiones necesitamos que ese gap no exista, para ellos he creado una colecci√≥n de clases aplicables a cada columna que desactivan ese gap de la forma:
.no-gutters =&gt; para eliminar los espacios en los dos lados en todos los breakpoints. .no-gutter-[xs|lxs|sm|md|lg] =&gt; para eliminar los espacios a los dos lados del breakpoint indicado en adelante. .no-gutter-[left|right] =&gt; para eliminar el espacio en el lado indicado en todos los breakpoints. .no-gutter-[left|right]-[xs|lxs|sm|md|lg] =&gt; para eliminar el espacio en el lado indicado del breakpoint indicado en adelante.
Como nota un poco t√©cnica, indicar que los gap entre columnas no hacen uso de &lsquo;grid-column-gap&rsquo; como pudiera parecer l√≥gico, ya que este establece un gap entre columnas fijo para todas, no pudi√©ndose indicar uno para el gap entre dos columnas y otro distintos entre otras dos.
Rellenador de columnas o Col filler Seguramente en alguna ocasi√≥n os ha sucedido que alrededor de un container has tenido que poner un fondo distinto a cada lado (ya sea un color o una imagen), si esto te ha ocurrido el .grid-filler est√° pensado para ti.
Como veras en la p√°gina de ejemplos de Sireno Grid la maquetaci√≥n de esto es muy sencilla y permite un comportamiento responsive.
Flexbox fallback Por desgracia no todos los usuarios actualizan sus navegadores (ya bien por que no pueden por limitaciones t√©cnicas o de pol√≠ticas corporativas o por que simplemente no lo hacen) as√≠ que era requisito establecer un fallback para las funcionalidades de Sireno Grid a flexbox para aquellos navegadores que no implementen el est√°ndar Grid Layout. Esperemos poder eliminar este fallback cuando Grid Layout est√© m√°s extendido
Otras caracter√≠sticas Me he dejado otras caracter√≠sticas para que las consulteis en la propia web de Sireno Grid donde est√°n explicadas y con ejemplos de uso.
Adem√°s en la propia web se indica como usar Sireno Grid en tu proyecto (usando por ejemplo NPM) y se muestran ejemplos (c√≥digo incluido) de las funcionalidades ya comentadas.
Dejo tambi√©n aqu√≠ enlazada una lightning talk que hice sobre Sireno Grid en un meetup de PHPVigo
Agradecimientos Agradecer a Pedro Figueras el dise√±o del logo y a BrowserStack la cesi√≥n de una licencia de uso de su herramienta para realizar el testeo en m√∫ltiples navegadores.
https://sirenogrid.com/
`,url:"https://sergiocarracedo.es/2019/08/26/SirenoGrid-Un-sistema-ligero-de-Grid-CS-basado-en-Grid-Layout/",image:"/2019/08/26/SirenoGrid-Un-sistema-ligero-de-Grid-CS-basado-en-Grid-Layout/sireno-grid_hu_f98c26c82e530bc1.jpg",tags:[],readingTime:"5 minutes read",date:"Aug 26, 2019"},"https://sergiocarracedo.es/2019/08/18/Juegos-Sandbox-Minecraft-y-7-Days-to-die-7D2D/":{title:"Juegos Sandbox: Minecraft y 7 Days to Die (7D2D)",content:`Siempre me han gustado los videojuegos desde mi √©poca del Amstrad CPC464.
Como todo el mundo tengo mis g√©neros favoritos, y uno de ellos son los juegos de tipo Sandbox o mundo abierto, para quien no los conozca son juegos en los que el camino del jugador no est√° definido por la histor√≠a, o por el programador, es decir puedes moverte por todo el mundo del juego sin m√°s limitaci√≥n que los &ldquo;bordes de mapa&rdquo; y puedes hacer lo que quieras (son el l√≠mite de las acciones programadas en el juego).
Ejemplos de este tipo hay muchos: Fallout, GTA V, No man&rsquo;s Sky, The Forest, Red Dead Redemtion, Rust, etc&hellip; solo hay que ver en Steam lo etiquetado bajo Sandbox para hacerse una idea.
En esa lista no he mencionado los dos que el objeto de este post: Minecraft y 7 Days to Die. Ambos tienen algo distinto al resto y es que el mundo virtual es totalmente modificable.
Minecraft Empecemos por Minecraft un videojuego que puede parecer infantil pero que tiene mucho potencial para el desarrollo de la imaginaci√≥n y la construcci√≥n de &ldquo;mundos&rdquo;.
En Minecraft no hay un objetivo como tal, el mundo est√° compuesto por bloques de 1x1x1m de distintos tipos: Desde los m√°s b√°sicos: piedra, tierra, madera, agua, arena, etc. Hasta hornos, railes, luces, puertas, etc.
Gr√°fico simplificado de craftings disponible en Minecraft - Via : Reddit
El jugador puede ir obteniendo recursos a partir de esos bloques para ir creando herramientas (que le permitan extraer nuevos recursos o los mismos de forma m√°s eficiente), u otros bloques, por ejemplo: con madera podemos crear palos y estos palos y piedra podemos crear un pico de piedra que a su vez nos permite picar bloques de mineral de hierro con el que podemos hacer otras herramientas.
Este concepto sencillo de crafting genera un potente mecanismo de juego y ense√±a al usuario a optimizar los recursos y el esfuerzo necesario para obtener un determinado objeto.
El nivel de libertad de construcci√≥n es tal que en internet se pueden encontrar partidas de personas que han construido verdaderas maravillas virtuales: grandes edificios, ciudades, etc. Por ejemplo este server online que recrea el mundo de Juego de tronos. Y todo sin olvidar que estos &ldquo;decorados&rdquo; son totalmente jugables y visitables.
Via : https://westeroscraft.com
Via : Reddit
Screenshot de mi partida
Screenshot de mi partida
7 Days to Die 7 Days to Die es para muchos un Minecraft para adultos, que a√±ade al genero sandbox el de supervivencia y terror.
Parte de la misma base de mundo totalmente &ldquo;rompible&rdquo; y de la obtenci√≥n de recursos para la construcci√≥n con un objetivo, y este es sobrevivir en un mundo apocal√≠ptico lleno de zombis (o infectados) cuyo objetivo es acabar con nosotros y que para complicarlo m√°s cada 7 d√≠as recibiremos la visita de una horda de zombis que arrasar√°n lo que se ponga a su paso.
En este juego comenzamos con una lata de comida, un vaso de agua (si tendremos que preocuparnos mucho de obtener comida y bebida), una antorcha, &hellip; Poca cosa y tendremos que buscar todo en el mundo ya sea minando al estilo Minecraft o saqueando bolsas de basura, en los muebles de las casas, los coches averiados, etc.
En este juego hay objetos que no son crafteables (es decir no podemos construirlos a partir de otros) y que solo se pueden conseguir looteando (saqueando), lo que obliga al jugador a explorar un mundo en el que tenemos ciudades y pueblos.
Tambi√©n a√±ade un concepto de juego sobre Minecraft que son los perks (o habilidades) que nos permiten ir mejorando el perfil del jugador, por ejemplo haciendo m√°s da√±o con un tipo de arma a los zombis, o pudiendo cargar m√°s objetos sin ir m√°s lento.
Otro concepto que a√±ade dificultad y madurez al juego es el de stamina, que impide podamos correr indefinidamente (al menos el principio del juego) o hacer tareas &ldquo;f√≠sicas&rdquo; sin limite.
Tenemos sigilo para intentar pasar desapercibidos de los zombis.
Otros elementos sobre los que no voy a entrar en detalle son los veh√≠culos, armas de fuego, armaduras (protecci√≥n), modificadores de herramientas y armas.
En mi caso este juego es capaz de meterme en esa atm√≥sfera de mundo apocal√≠ptico lleno de peligros, en el que tienes que ir consiguiendo recursos para construir tu base con el principal objetivo de defenderte de los zombis y en especial de las hordas que llegan cada 7 d√≠as (como dice el t√≠tulo del juego) y que cada vez con m√°s complicadas de pasar.
Si ten√©is m√°s inter√©s en el juego, os dejo este video de un Youtuber (BuckFernandez) que forma parte de una de las series en las que juega a la √∫ltima versi√≥n del juego.
Linux for gamming Como pincelada t√©cnica he de mencionar que ambos juegos son multiplataforma, y funcionan en Linux de forma nativa (que es la que yo uso principalmente), tambi√©n en Windows y Max (y en el caso de Minecraft en consolas, m√≥viles, etc.)
Y como curiosidad final decir que 7 Days to Die, est√° en Early Access lo que quiere decir que es un juego inacabado en el sentido de que, aunque es totalmente jugable, cada X tiempo, los desarrolladores van a√±adiendo nuevas funcionalidades al juego, como ejemplo, a d√≠a de hoy est√° en el Alpha 17 y en el paso del Alpha 16 al 17 se a√±adieron nuevas armas, mods, zombis, nuevos edificios, etc. Para m√≠ esto es un plus, ya que pagas por un juego que va mejorando poco a poco y sabes que con tu compra estas ayudando a los desarrolladores a crear un juego a√∫n mejor.
`,url:"https://sergiocarracedo.es/2019/08/18/Juegos-Sandbox-Minecraft-y-7-Days-to-die-7D2D/",image:"/2019/08/18/Juegos-Sandbox-Minecraft-y-7-Days-to-die-7D2D/cover_hu_b751639fad12e63c.jpg",tags:[],readingTime:"5 minutes read",date:"Aug 18, 2019"},"https://sergiocarracedo.es/2019/06/30/Mapas-interactivos-a-partir-de-un-SVG/":{title:"Mapas interactivos SVG con Vue",content:`En un reciente proyecto me ha surgido la necesidad de crear un mapa de Espa√±a interactivo en el que cada provincia fuese un elemento sobre le que poder hacer hover y click.
El proyecto lo estaba realizando el Nuxt.js y por lo tanto en Vue, por lo que decid√≠ crear un componente que encapsulara la generaci√≥n gr√°fica del mapa y emitiera los correspondientes eventos.
Lo primero que necesitamos es el mapa en formato SVG, no nos vale cualquiera debe ser un mapa en el que cada provincia est√© en un path y este est√© correctamente etiquetado, por ejemplo podemos usar este que es de libre tanto para uso comercial como personal.
Si lo abrimos vemos que cumple la condici√≥n de que cada provincia est√© en un path, y adem√°s este tiene metadatos como el nombre de la provincia.
&lt;path id=&#34;ESP5840&#34; name=&#34;Pontevedra&#34; d=&#34;M451.9 ....&#34; &gt; El siguiente paso es convertir ese SVG a JSON para facilitar el manejo con JS, yo he usado esta herramienta https://www.freeformatter.com/xml-to-json-converter.html#ad-output, aunque hay librer√≠a para hacer la conversi√≥n al vuelo desde JS.
Lo que nos dejar√° algo como esto:
Para volver a convertir esta informaci√≥n a un SVG que podamos manipular us√© svg.js
La estrategia consiste en generar de nuevo el SVG desde el componente de Vue pero a√±adiendo los eventos y necesarios, para ello una vez montado el componente, y usando svg.js creamos un path por cada provincia, este path viene definido por la key @d del json que generamos a partir del SVG.
generateMap() { const svgContainer = svg(this.id) .size(&#34;100%&#34;, &#34;100%&#34;) .viewbox(0, 0, 1000, 891); provinces.forEach(pathObj =&gt; { this.generatePath(svgContainer, pathObj); }); }, generatePath(svgCont, pathObj) { const attrs = { fill: &#34;transparent&#34;, stroke: &#34;#28586c&#34;, &#34;stroke-width&#34;: 1, title: pathObj[&#34;@name&#34;], &#34;map-id&#34;: pathObj[&#34;@id&#34;] }; const province = svgCont.path(pathObj[&#34;@d&#34;]).attr(attrs); } Si os fij√°is, indicamos al SVG que creamos que ocupe el 100% (tanto en ancho como en alto) de un viewbox cuyo tama√±o viene definido por el SVG original, esto define el tama√±o real del SVG si que parte ser√° visible, es decir si dibujamos un elemento m√°s all√° de 1000x891 este no se ver√° por que esta fuera del viewbox, pero el SVG es totalmente responsive, por decirlo de otra forma el tama√±o del viewbox no tiene una relaci√≥n 1:1 con la visualizaci√≥n real.
Como se ve en el ejemplo, creamos un path para cada provincia, ahora necesitamos dotarlo de interactividad para que pueda responder a un click.
... const province = svgCont.path(pathObj[&#34;@d&#34;]).attr(attrs); province.click(e =&gt; { const mapId = e.target.attributes[&#34;map-id&#34;].value; const title = e.target.attributes.title.value; this.$emit(&#34;mapClick&#34;, { mapId, title }); }); ... Simplemente emitimos un evento de componente en el que pasamos el ID y el nombre de la provincia, para que fuera del componente podamos gestionar ese click.
Y este el resultado final.
El ejemplo completo lo he dejado en codesandbox.io
`,url:"https://sergiocarracedo.es/2019/06/30/Mapas-interactivos-a-partir-de-un-SVG/",image:"/2019/06/30/Mapas-interactivos-a-partir-de-un-SVG/vuemaps_hu_c930c6c63e3289a8.jpg",tags:[],readingTime:"3 minutes read",date:"Jun 30, 2019"},"https://sergiocarracedo.es/2019/05/25/Impresiones-tras-un-mes-de-uso-de-un-teclado-mecanico/":{title:"Impresiones tras un mes de uso de un teclado mec√°nico",content:`Ya no se si como deformaci√≥n profesional o como &ldquo;tara mental&rdquo; propia, siempre he sido bastante quisquilloso con los perif√©ricos, tanto teclado como rat√≥n, porque al fin y al cabo paso al menos 8 horas diarias us√°ndolos y es importante sentirse c√≥modo con ellos.
Yo trabajo principalmente en un equipo de sobremesa y hasta hace poco estaba usando el teclado de Apple, como el que se muestra en la foto (pero con layout en Espa√±ol), no el &ldquo;moderno&rdquo; inal√°mbrico, sino el anterior, con cable.
Foto : Wikimedia
Sobre el cable podemos abrir un debate, he probado de los 2 tipos (con cable e inal√°mbrico) y bajo mi punto de vista, para el uso que yo le doy, un teclado debe ser cableado por varios motivos:
No tener que depender de pilas, bater√≠as, etc. Quedarte sin pilas o bater√≠a mientras estas haciendo algo es muy molesto, y tenerlo enchufado para que cargue es b√°sicamente lo mismo que que sea cableado. Respuesta: por muy bien que funcione un inal√°mbrico (que actualmente lo hacen), uno cableado lo hace mejor. Este teclado lleva conmigo 10 a√±os, y con pocos problemas, el √∫nico es que la tecla &ldquo;¬°¬ø&rdquo; se ha roto, pero en cuanto a funcionamiento y est√©tica estaba muy contento.
Te preguntar√°s por que me he decidido a cambiarlo, pues principalmente, por el tema de la tecla rota y por el mero hecho de cambiar.
Muchos programadores que conozco estaban usando teclados mec√°nicos y me quer√≠a probar si realmente era tan ventajoso y c√≥modo como comentaban.
Como casi siempre que hago una compra de algo que espero que me dure cierto tiempo, hice una peque√±a investigaci√≥n de las distintas tecnolog√≠as de los teclados y simplific√°ndolo mucho est√°n los cl√°sicos (como el de Apple que ten√≠a hasta el momento) y los mec√°nicos, los hay tambi√©n de accionamiento √≥ptico y capacitivo, pero no quiero perderme en los detalles ya que pretendo que esto sea m√°s una valoraci√≥n de la experiencia que un desglose t√©cnico.
As√≠ tras ver muchos teclados y reviews de los mismos, como, en ese momento, no estaba completamente seguro de que el paso al mec√°nico fuese definitivo me decant√© por un teclado con un precio contenido, y buena relaci√≥n calidad precio.
El &ldquo;escogido&rdquo;: Krom Kernel Foto: kromgaming.com
Finalmente el escogido ha sido el que ves en la foto superior, el Krom Kernel un teclado mec√°nico con switches Outemu Red, unos clones de los Cherry Mx Red y con iluminaci√≥n RGB configurable desde el propio teclado (sin software externo) que cuesta actualmente unos 55‚Ç¨ en Amazon y del que adem√°s tienes una versi√≥n sin teclado num√©rico (las llamadas TLK, Ten Key Less) por un poco menos (50‚Ç¨)
En este mes de uso que llevo con √©l, la adaptaci√≥n ha sido muy r√°pida, desde el primer momento me ha parecido c√≥modo al escribir sin necesidad de hacer una fuerza excesiva en las teclas (de hecho parece que menos que el con teclado de Apple), la iluminaci√≥n que la ve√≠a como algo &ldquo;superfluo&rdquo; aunque visualmente bonito, es algo a lo que se le puede sacar partido, por ejemplo, yo he configurado un layout (se pueden configurar varios con cambio r√°pido entre ellos) &ldquo;imitando&rdquo;, salvando las distancias, al m√≠tico teclado del CPC464.
Foto : Wikimedia
En este video de la propia marca se pueden ver todos los modos de iluminaci√≥n (me gusta mucho uno muy al estilo Ghost in the shell que ilumina toda la l√≠nea a partir de la tecla pulsada, minuto 3:30)
Esos modos de iluminaci√≥n, la verdad es que son muy espectaculares, pero siendo realistas, son muy poco √∫tiles en un uso real.
Por ponerle algunas pegas al teclado, la iluminaci√≥n, como se ve en las fotos, se &ldquo;fuga&rdquo; por debajo de las teclas, no es tan exagerado como se ve en las fotos. Tambi√©n hay que decir que no es homog√©nea, por ejemplo en las teclas &ldquo;grandes&rdquo;, como el backspace el color no es igual de luminoso que en las otras teclas, y me ha sucedido que el blanco de las teclas &ldquo;0&rdquo; y &ldquo;.&rdquo; del teclado num√©rico no se ve blanco del todo en la configuraci√≥n que uso, pero si cuando se coloca todo el teclado en blanco.
En cuanto a la sonoridad, es cierto que como casi todos los teclados mec√°nicos es m√°s ruidoso, pero no es un sonido que me moleste, al contrario, y por lo que me han comentado, en una video llamada, no suena mucho m√°s que el antiguo teclado de membrana que tenia.
En definitiva, por el precio que tiene este teclado cumple perfectamente, si dura como el anterior (10 a√±os) ya ser√≠a la bomba, y viendo lo c√≥modo que me siento tras un mes de uso, creo que he hecho una compra de la que, al menos de momento, no me arrepiento.
Aqu√≠ dejo unas cuantas fotos con distintos layouts:
`,url:"https://sergiocarracedo.es/2019/05/25/Impresiones-tras-un-mes-de-uso-de-un-teclado-mecanico/",image:"/2019/05/25/Impresiones-tras-un-mes-de-uso-de-un-teclado-mecanico/kromKernel_hu_54c0164ebff1b342.jpg",tags:[],readingTime:"4 minutes read",date:"May 25, 2019"},"https://sergiocarracedo.es/2019/05/12/Mi-historico-de-telefonos-moviles/":{title:"Mi hist√≥rico de tel√©fonos m√≥viles",content:`Esta semana he decidido &ldquo;jubilar&rdquo; mi tel√©fono m√≥vil y reemplazalo por uno m√°s nuevo, lo que me ha hecho pensar en todos los tel√©fonos que he tenido y en el tiempo que me &ldquo;duraban&rdquo; lo me llevado a escribir este post
He de hacer notar lo &ldquo;se√±or mayor&rdquo; que me ha hecho sentir recordar conceptos ya olvidados, como el prepago, los sms, los tonos, etc.
El primer m√≥vil (Foto: Wikimedia)
Mi primer tel√©fono m√≥vil lleg√≥ all√° por 1999, un flamante Nokia 5510, un tel√©fono cuya principal caracter√≠stica eran las carcasas intercambiables.
Fue muy recordado en su momento el anuncio de dos chicos intercambiando las carcasas usando un tendal de ropa (Todo muy noventero en el anuncio).
Desgraciadamente este tel√©fono, que era de segunda mano poco dur√≥ conmigo ya que solo soportaba la banda de 900MHz y en cuanto el operador que tenia por entonces, Amena (la Amena original), empez√≥ a dar servicio en la banda de 1800Mhz el tel√©fono dejo de ser √∫til.
Muy &ldquo;se√±or mayor&rdquo; me ha hecho recordar la verg√ºenza que nos daba cuando sonaba el m√≥vil en un sitio p√∫blico, lo normal que era que todos sonasen igual &ldquo;Ring Ring&rdquo; y nadie supiese de quien era el que sonaba (en aquella √©poca la vibraci√≥n no era para nada algo de serie en un m√≥vil)
Motorola M3688 (Foto: https://www.milanuncios.com/moviles-motorola/motorola-m3688-296180854.htm)
De este recuerdo especialmente la tapa, y pero que a√∫n as√≠ era necesario bloquear el teclado (se hac√≠a pulsando durante 2-3s una tecla) para evitar llamadas por error.
Hay que hacer notar que en ese momento una llamada pod√≠a costar cerca de un 1‚Ç¨ el minuto + establecimiento. &#x1f631;
Erisson A1018s Foto: https://www.milanuncios.com/otros-moviles/movil-antiguo-247647249.htm
Creo que fue all√° por el a√±o 2000-2001 cuando compr√© este m√≥vil.
El que tuve era justo como el de la imagen, con la &ldquo;carcasa&rdquo; serigrafiada con la mascota de Amena, aunque tra√≠a otra m√°s azul si no recuerdo mal.
De esta compra me acuerdo muy bien, era un pack llamado &ldquo;D√∫o&rdquo; (una oferta del operador) en el que te &ldquo;daban&rdquo; dos tel√©fonos iguales con dos n√∫mero de tel√©fono consecutivos. De este es el n√∫mero que a√∫n conservo actualmente
Recuerdo tambi√©n que era especialmente resistente, ya que me cay√≥ bastantes veces y lo √∫nico que le pasaba era que se desmontaba en 3 piezas: La carcasa, la bater√≠a y el cuerpo del tel√©fono. Solo hab√≠a que volver a montarlo y a funcionar.
Nokia 3210 (Foto: Wikimedia)
S√≠ amigos, tuve uno de esos tel√©fonos que los memes recuerdan como un tel√©fono indestructible, y la verdad es que lo era.
Tenia cosas avanzadas para su √©poca, como la gesti√≥n de la agenda, el poder conectarlo a un ordenador (mediante cable) y cambiar el logo de la operadora, lo &ldquo;politonos&rdquo;
Alcatel One Touch Easy (Foto: Wikimedia)
Este ha sido uno de los peores tel√©fonos que he tenido, sin duda, lo recuerdo muy lento, se colgaba y no te notificaba las llamadas, recuerdo que a veces al sentarme se mov√≠a la bater√≠a y se apagaba, era un quiero y no puedo.
Nokia 6210 (Foto: Wikimedia)
A√±o 2002. Este fue el caso contrario, uno de los mejores tel√©fonos que he tenido. Era de la gama &ldquo;profesional&rdquo; de Nokia. Tenia una gesti√≥n de la agenda espectacular para su √©poca, permit√≠a tener varios tel√©fonos en el mismo contacto, algo que en la mayor√≠a de tel√©fonos de la √©poca ni so√±aban, la pantalla era grande en su momento, la bater√≠a duraba casi una semana (us√°ndolo).
Tenia puerto de infrarrojos para cargarle politonos, gestionar la agenda (y hacer backup de ella), y como se ve en la siguiente imagen (de mi propio tel√©fono), cargar im√°genes como logotipo de operador.
Me dio mucha pena haberlo vendido para adquirir el siguiente tel√©fono, me hubiese gustado conservarlo, por que adem√°s como se ve en la imagen la chapita inferior se pod√≠a personalizar y de hecho lo estaba &#x1f602;.
Nokia 6610 (Foto: Wikimedia)
A√±o 2003 Mi primer tel√©fono en color y con c√°mara, eso si la c√°mara era un perif√©rico externo &#x1f602; y tenia una resoluci√≥n de 640x480px, es decir 0,3 Mega p√≠xeles. En la siguiente foto, ya se puede observar que la calidad no era especialmente buena.
Nokia 3650 (Foto: Wikimedia) A√±o 2003-2004. Este curioso tel√©fono al que llamaba cari√±osamente &ldquo;la lavadora&rdquo; por la forma del teclado, que a pesar de lo que pudiese parecer era f√°cil y r√°pido de manejar, fue mi primer smartphone.
Era un tel√©fono con Symbian OS, en concreto la versi√≥n 6.1, que permit√≠a instalar aplicaciones, juegos, comunicaci√≥n con una suite muy potente de gesti√≥n del tel√©fono mediante el PC. Ya tenia c√°mara integrada. Todo muy primitivo comparado con los m√≥viles que llegar. La velocidad era bastante r√°pida para lo que hab√≠a en su momento eso si, marc√≥ la tendencia que seguir√≠a despu√©s, la duraci√≥n de la bater√≠a era con suerte de 1 d√≠a de uso.
Qtek S100 (Foto: Wikimedia)
A√±o 2005. Este Smartphone o mejor dicho PDA con m√≥vil, que realmente era una HTC Magician, fue otro salto al disponer de pantalla t√°ctil (resistiva) manejada con l√°piz, aunque si te acostumbrabas pod√≠as llegar a manejarla con los dedos. Fue tambi√©n mi primer tel√©fono con wifi, pero nunca lo llegu√© a usar demasiado
A pesar de ser un buen aparato en global, el sistema operativo Window Mobile 2003, era terrible, continuos cuelgues, errores de apps, me llevaron a dar un paso atr√°s e irme a un tel√©fono lo m√°s sencillo posible.
Samsung L760v (Foto: Wikimedia)
A√±o 2006.Est√© fue un paso atr√°s en cuanto a &ldquo;smart&rdquo;, pero fue un paso adelante en cuanto a duraci√≥n de la bater√≠a, ligereza, rapidez, y comodidad para llevar en el bolsillo.
Un tel√©fono que a√∫n conservo y cuya bater√≠a apagado se mantuvo durante varios a√±os.
HTC Touch 3g (Foto: Wikimedia)
Este ya en el a√±o 2007 fue una vuelta a los smartphones de nuevo con Window Mobile, pero con una capa de HTC (muy chula, por cierto) que permit√≠a menejarla f√°cilmente con la mano (sin necesidad del puntero)
Con este tel√©fono aprovechando contrat√© mi primera tarifa de datos, entendiendo que era la forma m√°s l√≥gica de usar un smartphone.
HTC Magic (Foto: Wikimedia)
Hace 10 a√±os ya (2009) de este tel√©fono, fue mi primera incursi√≥n en el mundo Android. Con una versi√≥n Android 1.5 (luego actualizada a 1.6, pro cierto haciendo que el tel√©fono funcionase terriblemente lento) era un paso adelante respecto a los anteriores smartphones que hab√≠a tenido: Multitarea, pensado completamente para el manejo con dedos, tienda de aplicaciones incluida, navegador web decente.
Me gustaba mucho la &ldquo;bolita&rdquo;, que adem√°s de servir como luz de notificaci√≥n era un trackball, lo que permit√≠a mover el cursor por el texto de una forma muy c√≥moda.
HTC Desire (Foto: Wikimedia)
A√±o 2012. Fue un salto en rendimiento respecto al HTC Magic, un buen procesador y memoria para su √©poca, una buena c√°mara, capaz de grabar video en 720p.
Manten√≠a el manejo del cursor mediante un &ldquo;trackball&rdquo; √≥ptico, para mi gusto no tan bueno en cuanto a control como la bola f√≠sica.
Con este tel√©fono, cuando HTC dej√≥ de lanzar actualizaciones, fue con el que me introduje en el mundo del Rooteo, flaseado e instalaci√≥n de Custom ROMs, usando principalmente CyanogenMod
Nexus 4 (Foto: Wikimedia)
A√±o 2013. Estaba ya cansado de leer que muchos fabricantes faltaban a sus promesas de actualizar la versi√≥n de Android (que ellos personalizaban), y de ver la de la cantidad de bloatware que instalaban, as√≠ que me decante por este tel√©fono fabricado por LG, pero vendido y mantenido por Google.
Todo un acierto, un tel√©fono de gran calidad de acabado, con bastantes novedades tecnol√≥gicas, como el NFC, y que termin√© cambiando por el desgaste de la bater√≠a, pero que a√∫n sigui√≥ usando una amiga 2 a√±os m√°s sin problemas.
Nexus 5X (Foto: Wikimedia)
A√±o 2016. Y este es el tel√©fono que tengo actualmente (hasta que me llegue el nuevo). Un gran tel√©fono con un gran defecto de dise√±o. Al a√±o de tenerlo un d√≠a se reinicio y ya no arrancaba, se reiniciaba durante el propio proceso de reinicio. Leyendo y buscando informaci√≥n encontr√© que era un defecto de dise√±o/fabricaci√≥n, que la composici√≥n de las soldaduras que un√≠an el procesador a la placa, son el tiempo (dependiendo sobre todo del calor), se agrietaban y dejan de hacer contacto ciertas patillas, provocando estos reinicios constantes.
Llam√© al soporte de Google y el trato fue muy bueno, en unos d√≠as tenia otra unidad del tel√©fono (refurbished) en casa.
Esto me volvi√≥ a suceder casi otro a√±o despu√©s, lo que hizo que me volvieran a cambiar el tel√©fono, de esto hace algo m√°s de un a√±o, por lo que la garant√≠a ya no lo cubrir√≠a, y es un poco el motivo que me ha llevado a decidir cambiar de m√≥vil.
¬øY ahora qu√©? Pues, ahora me he decantado (tras mucho valorar opciones) por un Xiaomi A2, b√°sicamente por que tiene pr√°cticamente todo lo que necesito o puedo usar, por precio, por que sigue llevando Android One, que deber√≠a hacer que reciba las actualizaciones del sistema de forma r√°pida y por que mucha gente que lo tiene me lo ha recomendado.
Si me dura 2 a√±os me dar√© con un canto en los dientes.
Personalmente respecto a los m√≥viles estoy en un momento como en 2006, en el que di un paso atr√°s por que ve√≠a el mercado estancado, y veo que ahora, como es l√≥gico, no hay funcionalidades rompedoras que te inviten a cambiar de tel√©fono.
`,url:"https://sergiocarracedo.es/2019/05/12/Mi-historico-de-telefonos-moviles/",image:"/2019/05/12/Mi-historico-de-telefonos-moviles/mobiles_hu_40f26375b308aa47.jpg",tags:[],readingTime:"8 minutes read",date:"May 12, 2019"},"https://sergiocarracedo.es/2019/04/27/haciendo-cafe/":{title:"Haciendo caf√©: simple an√°lisis de costes",content:`Aunque no me puedo considerar un gran &ldquo;cafetero&rdquo;, si que me gusta desayunar con un buen caf√© por las ma√±anas.
Hasta hace unos a√±os las opciones para hacer cas√© no eran muchas: Cafetera de goteo, cafetera italiana y poco m√°s. (Algunas opciones m√°s tendr√≠amos por eso me centrar√© en comentar las m√°s habituales). Pero desde la aparici√≥n en el mercado de las C√°psulas monodosis, estas han copado el mercado, ya sean Nespresso, Senseo, Dolce Gusto, Tassimo, etc.
Las ventajas de las C√°psulas son muchas, pero principalmente es la rapidez en tener una taza de caf√© reci√©n hecho y caliente. Esta ventaja sobre las italianas y de goteo es obvia.
Lo peor es el precio de las C√°psulas, que en algunos casos acercan el coste de un caf√© al de tomarlo en una cafeter√≠a y la cantidad de residuos que generan, no nos olvidemos que por cada caf√© estamos tirando a la basura una capsula, que mayoritariamente son de pl√°stico, adem√°s de su embalaje.
En casa hasta hace unos 2 a√±os usamos en casa una de sistema Senseo que quiz√° es en la que cada dosis es m√°s econ√≥mica, pero actualmente tenemos una cafetera autom√°tica Krups Roma, y aqu√≠ es donde quer√≠a explicar que nos ha llevado a este cambio.
Esta cafetera muele el caf√© en el momento de hacerlo, lo que le hace tener una frescura y aroma igual o superior a las C√°psulas.
Un paquete de caf√© en grano cuesta desde 5‚Ç¨/kg a pr√°cticamente lo que se te ocurra gastar, pero por 6-7‚Ç¨/kg puedes disponer de un caf√© m√°s que decente.
He calculado de forma aproximada que para cada caf√© gasta unos 10gr (el sobrante que suelta al terminar pesa 15gr, pero est√° mojado lo que aumenta √©l pero). Esto nos dar√≠a con un paquete de 1kg podr√≠amos hacer 100 caf√©s y para un coste de 7‚Ç¨/kg, cada caf√©/dosis nos costar√≠a (aproximadamente) 0,07‚Ç¨
Por comparar los costes:
Nespresso: C√°psulas originales (123‚Ç¨/kg) 0,58‚Ç¨/dosis C√°psulas compatibles (25‚Ç¨/kg) 0,29‚Ç¨/dosis Dolce Gusto C√°psulas originales (79‚Ç¨/kg) 0,79‚Ç¨/dosis C√°psulas originales 0,26‚Ç¨/dosis C√°psulas compatibles 0,27‚Ç¨/dosis Senseo C√°psulas 0,11‚Ç¨/dosis Estos precios seguramente var√≠en y se puedan encontrar c√°psulas compatibles a precios inferiores, pero no pretendo hacer un estudio tan exhaustivo, son m√°s unos c√°lculos caseros.
En mi casa hacemos una media a la baja de 120 caf√©s al mes, lo que supondr√≠a en costes (voy a usar los precios m√°s bajos por c√°psula)
Nespresso: 69,6‚Ç¨/mes Dolce gusto: 31,2‚Ç¨/mes Senseo: 13,2‚Ç¨/mes Krups: 8,4‚Ç¨/mes Obviamente para hacer un caf√© necesitamos una cafetera para cada sistema de c√°psulas:
Nespresso: 69‚Ç¨ Dolce gusto: 44‚Ç¨ Senseo: 59‚Ç¨ Krups: 234‚Ç¨ Contando la cafetera como coste fijo (y eliminando de la ecuaci√≥n el coste del agua y electricidad, que suponemos muy similares entre todas las cafeteras), vamos a ver cuanto se tardar√≠a en amortizar la cafetera m√°s cara, y para ello veremos la diferencia entre su coste y el de la cafetera del sistema a comparar y el de las capsulas:
Krups vs Nespresso: Cafetera: 234‚Ç¨ - 69‚Ç¨ = 165‚Ç¨ Diferencia mes: 8,4‚Ç¨ - 69,6‚Ç¨ = ‚àí61,2‚Ç¨ Meses para amortizar: 165‚Ç¨/61,2‚Ç¨ = 2,69 meses Krups vs Dolce gusto: Cafetera: 234‚Ç¨ - 31,2‚Ç¨ = 202,8‚Ç¨ Diferencia mes: 8,4‚Ç¨ - 31,2‚Ç¨ = -22,8‚Ç¨ Meses para amortizar: 202‚Ç¨/22,8‚Ç¨ = 8,86 meses Krups vs Senseo: Cafetera: 234‚Ç¨ - 59‚Ç¨ = 175‚Ç¨ Diferencia mes: 8,4‚Ç¨ - 13,2‚Ç¨ = -4,8‚Ç¨ Meses para amortizar: 175‚Ç¨/4,8‚Ç¨ = 36,46 meses Como se observa, la m√°quina autom√°tica se amortiza, en algunos casos de forma muy r√°pida, todo va a depender la cantidad de caf√© que se haga y del caf√© / c√°psulas usadas.
En el caso de la comparativa con la que us√°bamos hasta hace 2 a√±os (Senseo) el amortizar econ√≥micamente la cafetera autom√°tica, pero os puedo asegurar que el sabor del caf√© (y repito que no soy muy cafetero) es mejor, lo mismo que la comodidad de uso.
Quiz√° con un poco m√°s de tiempo seria interesante a√±adir el an√°lisis las cafeteras italianas, expreso, etc. ¬øAlguien se anima?
`,url:"https://sergiocarracedo.es/2019/04/27/haciendo-cafe/",image:"/2019/04/27/haciendo-cafe/cafe_hu_4bcb8ee87182d400.jpg",tags:[],readingTime:"4 minutes read",date:"Apr 27, 2019"},"https://sergiocarracedo.es/2019/04/20/no-todo-es-trabajar-jugando-con-trenes/":{title:"No todo es trabajar: Jugando con trenes",content:`No todo es siempre trabajar, los momentos de ocio tamb√≠en son importantes.
De peque√±o tuve un maqueta funcional de un tren el√©ctrico, que desgraciadamente, por lo que m√°s la recuerdo es por que nunca lleg√≥ a funcionar correctamente, problemas con los engranajes de transmisi√≥n, que por m√°s que se pidieron recambios nunca se resolvieron.
Con los a√±os y con el desarrollo l√∫dico de la inform√°tica, fueron apareciendo juegos &ldquo;de trenes&rdquo; que suplieron, con creces, ese trauma infantil.
Railroad Tycoon Un poco m√°s mayor, lleg√≥ a mi un juego de MS-DOS llamado Railroad Tycoon, que yo creo que fue uno de los primeros &ldquo;Tycoon&rdquo;. Este juego consist√≠a en crear una red de ferrocarril (poniendo los ra√≠les, estaciones, dep√≥sitos, etc sobre un escenario 2D), crear y programar los trenes, etc&hellip;
A este juego se puede jugar on line en archive.org usando chrome.
Transport Tycoon All√° por 1994 se public√≥ un que para mi lo cambio todo bastante, este juego fue Transport Tycoon, una de las principales diferencias con Railroad Tycoon, es que el escenario es una perspectiva isom√©trica 3D, dividida en &ldquo;cuadrados&rdquo; en los que es posible colocar las v√≠as, carreteras, etc, por que si este juego no se centra solo en los trenes, permite gestionar flotas de veh√≠culos de carretera, aviones y barcos adem√°s de trenes.
El nivel de control de la red es muy potente, pudiendo dise√±ar las v√≠as al detalle, con sem√°foros, t√∫neles, puentes, etc.
En 1995 aparece una revisi√≥n de este juego llamada Transport Tycoon Deluxe o TTD, que entre otras cosas, permit√≠a crear se√±ales unidireccionales para gestionar de forma m√°s eficiente la red.
Tal fue el √©xito de ese juego que la comunidad se interes√≥ en mejorarlo y aportar nuevas funcionalidades creando TTDPatch que entre otras cosas a√±ad√≠a compatibilidad con Window XP al juego original.
OpenTTD En 2003 un programador comienza a programar un clon de TTD escrito en C (a partir del desensamblado del juego original) y en 2004 lo libera bajo licencia GPL y desde ese momento el juego ha seguido creciendo con nuevas funcionalidades de todo tipo, nueva generaci√≥n de terreno, distintos tipos de se√±ales, mejoras en la usabilidad y gesti√≥n de la flota.
Adem√°s incorpora un gestor de plugins llamado newGRF que van desde nuevos tipos de veh√≠culos, de flotas (por ejemplo tranv√≠as), gr√°ficos para el juego, m√∫sicas, etc&hellip;
El juego se puede descargar en https://www.openttd.org/ para Windows, OSX, Linux y otras plataformas.
Este juego es mi juego favorito de trenes. S√© que hay juegos m√°s nuevos con gr√°ficos en 3D &ldquo;real&rdquo;, pero adem√°s de la nostalgia (esos gr√°ficos de 8-bits me enamoran), me gusta por que me permite hacer algo que no tiene otros juegos, que es el pensar en la optimizaci√≥n de las lineas de transporte, dise√±o de mejores cruces (para hacerlos m√°s √≥ptimos y r√°pidos), mejores entradas y salidas en las estaciones, etc.
Dejo una de mis √∫ltimas partidas, la que cree cuando estuve hospitalizado 45 d√≠as, y que me permiti√≥ hacerlos un poco m√°s llevaderos.
Descargar partida
Descargar partida extreme
`,url:"https://sergiocarracedo.es/2019/04/20/no-todo-es-trabajar-jugando-con-trenes/",image:"/2019/04/20/no-todo-es-trabajar-jugando-con-trenes/openttd_cover_hu_87a1cd46706ba15e.jpg",tags:[],readingTime:"3 minutes read",date:"Apr 20, 2019"},"https://sergiocarracedo.es/2019/04/07/Accediendo-al-Sergas-y-al-eSaude-desde-linux/":{title:"Accediendo al Sergas y a eSaude desde linux",content:`Debido mi ya no tan reciente enfermedad , y dedicado tiempo a acceder a la p√°gina web del Sergas (Servizo Galego de Sa√∫de) para usar un servicio que fue presentado en 2016, y que desde finales de 2018 permite acceder a la funci√≥n, que yo creo que √∫til, de descarga de las pruebas diagnosticas de imagen: TACs, resonancias, radiograf√≠as, ecograf√≠as, etc&hellip;
Para acceder a esta herramienta es necesario disponer de eDNI, Chave365 o de un certificado digital, pero en este √∫ltimo caso es necesario pasar antes por el Centro de Salud para autorizar a dicho certificado a darnos acceso a nuestros datos de Salud, un paso que no entiendo del todo, puesto que para obtener un certificado de la FNMT, debemos verificar nuestra identidad.
Como usuario de Linux que soy, por desgracia siempre encuentro alguna peque√±a (o no tan peque√±a) pega para acceder a sitios web y herramientas de la administraci√≥n. He de decir en su descargo que esto sucede cada vez menos.
En el caso de usar eDNI, en primer lugar debemos tener un lector y tenerlo configurado.
Accedemos al portal eSsaude ya sea desde el banner en la web del Sergas o directamente en https://esaude.sergas.es/
Nos logueamos haciendo uso del eDni y ya podremos acceder al portal, y a lo que m√°s nos interesa: La historia cl√≠nica: Prueba diagnosticas de imagen e Informes (tamb√≠en hay otras opciones que por el momento no he usado).
En primer lugar tenemos los Informes que es una lista de documentos en formato PDF, aqu√≠ encontraremos desde Informes de Alta hospitalaria, An√°lisis de Sangre, Informes de radiolog√≠a (de TACs, esc√°neres, PETs, etc), Informes de cirug√≠a, y seguramente otros tipos que en mi caso no hay informaci√≥n.
El acceso a estos informes no suponen ning√∫n reto para un usuario de linux, ya que son documentos en formato PDF, un formato est√°ndar y pueden ser le√≠dos con cualquier visor, como el que trae por defecto tu distro.
El problema llega en el apartado de &ldquo;Pruebas diagn√≥sticas de imagen&rdquo;, aqu√≠ aparece una lista de las pruebas, no aparecen todas, pero no he logrado saber el criterio, en mi caso los PETs no aparecen.
En esta p√°gina si queremos acceder a las im√°genes de una de las pruebas, debemos solicitar la descarga, que pasado cierto tiempo (nos avisan a nuestro email) podremos descargar.
Una vez descargado el archivo ZIP debemos descomprimirlo, en mi caso (Ubuntu 18.04.2) el gestor de archivos comprimidos de Gnome no funcionaba correctamente puesto que no respetaba la estructura de carpetas interna al descomprimir, por lo que tuve que hacerlo usando la l√≠nea de comandos: unzip [normbreDelArchivo.zip] y listo.
Y aqu√≠ nos encontramos con una aplicaci√≥n para Windows que nos da acceso a las im√°genes de la prueba, no hay archivos JPG, ni nada similar a los que acceder.
En un primer intento trat√© de ejecutar el visor con Wine, pero, tras dedicarle unas horas sin resultados, decid√≠ atacar por otro lado.
Viendo los archivos que compon√≠an el descargable se hac√≠a referencia a algo llamado DICOM, que de entrada pens√© que se trataba del nombre del propio visor, pero que no tarde en descubrir que es un est√°ndar para el intercambio de im√°genes m√©dicas. En este est√°ndar se incluye el propio formato de los archivos, as√≠ como el protocolo de comunicaci√≥n para intercambiar datos. Es decir en el formato hay mucha m√°s informaci√≥n que solo las im√°genes, est√°n por ejemplo los datos del paciente, n√∫mero de historia cl√≠nica, datos del aparato que realiza la imagen, etc.
Sabiendo esto √∫ltimo, me lanc√© a buscar alguna herramienta de software libre con soporte para linux que me sirviese para abrir y visualizar las pruebas.
Tras probar varias, la que m√°s me gusto, por funcionalidades y facilidad de uso de entrada fue: WEASIS, con su repo en Github. Est√° escrita en Java, y es compatible con Linux, OSX y Windows.
La aplicaci√≥n parece bastante completa, pero se escapa a mi conocimiento el saber si es potente para un especialista, pero tiene una funcionalidad que me llamo mucho la atenci√≥n:
Un TAC se almacena como multiples im√°genes (400-500) que representan cada uno de los cortes transversales del cuerpo que realiza el TAC (cortes de imagen, obviamente), esta aplicaci√≥n, permite, a partir de esos cortes en un eje, genera los cortes en los otros 2, generando 3 vistas del interior del cuerpo, tal y como muestra la imagen extra√≠da de la p√°gina web de Weasis, donde hay muchas m√°s im√°genes de ejemplo
Imagen de un TAC extraida de la web de Weasis
Obviamente no soy medico, ni radi√≥logo, ni nada que se le parezca, pero lo que si soy es curioso y estas herramientas me permiten visualizar las pruebas e intentar hacerme una imagen mental de lo que me va contando el m√©dico.
`,url:"https://sergiocarracedo.es/2019/04/07/Accediendo-al-Sergas-y-al-eSaude-desde-linux/",image:"/2019/04/07/Accediendo-al-Sergas-y-al-eSaude-desde-linux/esaude_hu_f2665b44645d47b4.jpg",tags:[],readingTime:"4 minutes read",date:"Apr 7, 2019"},"https://sergiocarracedo.es/2019/03/31/Mis-proyectos-Open-Source/":{title:"Mis proyectos Open Source",content:`En todos estos a√±os &ldquo;haciendo cosas con el ordenador&rdquo;, he aprendido que compartir conocimiento y c√≥digo es una muy buena forma de devolver una peque√±a parte de lo recibido, por que al fin y al cabo en mi caso el 90% del software que uso para desarrollar, ya sean herramientas, frameworks, librerias o el propio lenguaje son Open Source.
He aqu√≠ una lista de los proyectos que liberado agrupados por de una forma totalmente arbitraria.
Drupal Drupal es mi CMS de desarrollo favorito, y para el he desarrollado varios m√≥dulos que en muchos casos cubr√≠an necesidades espec√≠ficas de proyectos que no estaban contempladas por otros m√≥dulos:
Commerce Billy Cancel: Permite generar una serie de facturaci√≥n especifica de facturas anuladas.
Follow Font Awesome: Complementa al m√≥dulo Follow permitiendo usar iconos vectoriales de FontAwesome en lugar de im√°genes.
SA-CORE-2018-002 Mitigation: Mitiga la explotaci√≥n del SA-CORE-2018-002 (https://www.drupal.org/sa-core-2018-002)
Site publish countdown: Redireciona a los usuarios an√≥nimos a una p√°gina de cuenta atr√°s para la publicaci√≥n del sitio web. Una vez finalizada esa cuenta atr√°s se publica el sitio.
3Dmol.js field: A√±ade un campo y su widget para permitir mostrar mol√©culas usando 3Dmol.js Otros: Block WoW y Simplelineicons
Deployer Deployer es mi herramienta de despliegue de c√≥digo favorita, y para ella he creado la receta de (Drupal 7)[https://github.com/deployphp/deployer/blob/master/recipe/drupal7.php] y Drupal 8, receta que ha sido mejorada con el tiempo por la comunidad
Full project En esta categor√≠a entran los proyectos completos como:
Sireno Grid, un framework CSS ligero basado en CSS Grid Layout con fallback a flexbox Gracias a Pedro Figueras por el dise√±o del logo
User Group OBS Background, una aplicaci√≥n ElectronJS, que permite gestionar fondos para la grabaci√≥n de eventos usando OBS.
Backup Tasks, una herramienta de creaci√≥n de tareas de backup y checkeo de integridad de ficheros remotos.
Backup tools, una simple herramienta que permite realizar backups contra repositorios git
Colaboraciones Aqu√≠ listo algunas de las colaboraciones en proyectos de otro tipo: websites de la comunidad, herramientas, etc.
Sitio web Vigotech.org
Vigotech-event-bot bot escrito en nodejs que se encarga de publicar los eventos pr√≥ximos de VigoTech Alliance en Twitter
Widget Made with love in Vigo
`,url:"https://sergiocarracedo.es/2019/03/31/Mis-proyectos-Open-Source/",image:"/2019/03/31/Mis-proyectos-Open-Source/opensource_hu_64fdb96317a77.jpg",tags:[],readingTime:"2 minutes read",date:"Mar 31, 2019"},"https://sergiocarracedo.es/2019/03/01/mis-charlas/":{title:"Mis charlas",content:`Hace un par de a√±os, superando mi timidez, empec√© a dar chapas charlas t√©cnicas en publico, obviamente la primera fue un desastre &#x1f613;, pero para eso es la primera, para aprender, y tamb√≠en la segunda y la tercera&hellip;. &#x1f602;
Compartir conocimiento de esta forma, es algo que recomiendo, por muchos motivos, incluso motivos &ldquo;ego√≠stas&rdquo; como el hecho de obligarte a esforzate a preparar ese tema sobre el que vas a hablar.
Tamb√≠en est√° el feedback que recibes de parte de los asistentes, que en muchos casos me han descubierto otras perpectivas o hilos por donde tirar.
He querido aprovechar estos d√≠as de cierta tranquilidad para recopilar las charla que he dado y el material empleado.
Aqu√≠ van:
13/04/2016 - Charla en PHPVigo #3: Drupal pr√°ctico: M√≥dulos, bloques, formularios Ver Presentaci√≥n en Google Docs
17/08/2016 - Charla en PHPVigo #7: Despliegue con Deployer A partir de 1:00:57 Ver Presentaci√≥n en Google Docs
21/12/2016 - Charla en PHPVigo #9: Preprocesadores CSS Ver Presentaci√≥n en Google Docs
22/02/2016 - Charla lightning en PHPVigo #11: Resque: Workers As√≠ncronos Ver Presentaci√≥n en Google Docs
18/09/2017 - Charla lightning en PHPVigo #13: Shame on you PHP!! Ver Presentaci√≥n en Google Docs
18/09/2017 - Charla lightning en PHPVigo #16: Double Extension Attack 24/01/2018 - Charla lightning en PHPVigo #19: Herramienta de backup y seguridad hosting en PHP Ver Presentaci√≥n en Google Docs
31/05/2018 - Charla en (JS + PHP) * Vigo: Introducci√≥n a Vue.js Presentaci√≥n en reveal.js
`,url:"https://sergiocarracedo.es/2019/03/01/mis-charlas/",image:"/2019/03/01/mis-charlas/talks_hu_39dddeebeab1fa3d.jpg",tags:[],readingTime:"2 minutes read",date:"Mar 1, 2019"},"https://sergiocarracedo.es/2019/02/04/lampara-de-escritorio-domotica/":{title:'"Domotizar" una l√°mpara de escritorio por 10‚Ç¨',content:` Hace ya unos cuantos meses hab√≠a comprado este aparatillo: un interruptor wifi, con la intenci√≥n de probar a &ldquo;domotizar&rdquo; algo en casa.
Este modelo concreto es de doble canal por lo que lleva dos interruptores en el mismo aparato, pero para este proyecto con un s√≥lo canal es suficiente.
Como la mayor√≠a de estos aparatos de bajo coste est√°n basado en el chip ESP8266 usado originalmente para dotar de conectividad wifi a los Arduino, pero que ha demostrado su solvencia usado de forma independiente.
Como era mi primera prueba no quise complicarme mucho y decid√≠ no flashearlo ni nada por el estilo usando el software &ldquo;original&rdquo;
Tras darle varias vueltas a que &ldquo;domotizar&rdquo; encontr√© un candidato. Mi lampara de escritorio, esa que siempre me dejaba encendida cuando me levantaba de la mesa de trabajo &#x1f605;.
El proceso de instalaci√≥n del interruptor wifi fue muy sencillo, cortar el cable de alimentaci√≥n y conectarlo al nuevo.
&#x26a0;&#xfe0f; Creo que sobra decir que cualquier manipulaci√≥n de cableado: siempre con la l√°mpara desenchufada &#x26a0;&#xfe0f;
Una vez vuelto a conectar todo, segu√≠ los pasos indicados por la app eWeLink para sincronizar el dispositivo y ya de entrada pod√≠a encender y apagar la l√°mpara desde el m√≥vil, lo que no es nada pr√°ctico.
Mi idea era hacer que la lampara se encendiese al desbloquear el ordenador y que se apagase cuando este entrase en modo ahorro de energ√≠a (apagado del monitor).
Investigando, encontr√© la forma de que dbus lance comandos cuando cambia el estado del salva pantallas de Gnome
lamp.sh
dbus-monitor --session &#34;type=&#39;signal&#39;,interface=&#39;org.gnome.ScreenSaver&#39;&#34; | while read x; do case &#34;$x&#34; in *&#34;boolean true&#34;*) echo &#34;SALVA PANTALLAS ACTIVADO, APAGAR LAMPARA&#34;;; *&#34;boolean false&#34;*) echo &#34;SALVA PANTALLAS DESACTIVADO, ENCENDER LAMPARA&#34;;; esac done Este fichero lamp.sh debe lanzarse al iniciar nuestra sesi√≥n cada vez que arranquemos el equipo, as√≠ que lo m√°s simple es a√±adirlo a la lista de Startup applications de Gnome
Ahora s√≥lo me quedaba ser capaz de poder lanzar esos eventos al servidor de gesti√≥n del interruptor wifi, de la misma forma que lo hace la app m√≥vil.
Lo que hace el interruptor cuando lo sincronizamos es conectarse a un servidor externo (esta parte es la que menos me gusta, pero puede cambiarse) y esperar comandos. Usando IFTTT podemos enlazar un dispositivo eWeLink (o Sonoff) con un evento, en este caso el evento es un webhook que nos permite llamar a una URL para ejecutar el evento.
Este webhook tiene esta forma: https://maker.ifttt.com/trigger/[nombre_del_evento]/with/key/[la_key_que_te_da_ifttt]
Donde el nombre del evento lo definimos al crearlo, y la_key_que_te_da_ifttt te la proporciona IFTTT al crear el primer webhook https://ifttt.com/maker_webhooks
Ya solo queda crear dos Applets en IFTTT (uno para encender y otro para apagar) enlazados a las acciones que nos proporciona respecto a eWeLink y retocar nuestro lamp.sh para que llame a estos webhooks.
lamp.sh
dbus-monitor --session &#34;type=&#39;signal&#39;,interface=&#39;org.gnome.ScreenSaver&#39;&#34; | while read x; do case &#34;$x&#34; in *&#34;boolean true&#34;*) curl -X POST https://maker.ifttt.com/trigger/turn_off_desktop_lamp/with/key/[TU_KEY];; *&#34;boolean false&#34;*) curl -X POST https://maker.ifttt.com/trigger/turn_on_desktop_lamp/with/key/[TU_KEY];; esac done No es la soluci√≥n perfecta, sobre todo en lo que respecta a depender de un servidor externo para algo tan simple, pero es un primer paso sobre el que evolucionar.
Olvidaba comentar que tambi√©n es posible conectar el interruptor a Google Home, Alexa o HomeKit, yo lo he probado con Google Home y funciona correctamente.
`,url:"https://sergiocarracedo.es/2019/02/04/lampara-de-escritorio-domotica/",image:"/2019/02/04/lampara-de-escritorio-domotica/lampara_hu_a5d349d144177f66.jpg",tags:[],readingTime:"3 minutes read",date:"Feb 4, 2019"},"https://sergiocarracedo.es/2019/01/28/mi-experiencia-con-el-trato-medico-paciente/":{title:"Mi experiencia con el trato m√©dico-paciente",content:`Aclarar de antemano que esta entrada no pretende ser una magufada que ponga por delante la parte psicol√≥gica a la m√©dica, si en alg√∫n momento parece eso, lo he escrito muy mal &#x1f602;
Por diversos motivos, ya desde peque√±o he tenido que estar varias veces ingresado en hospitales, y eso me ha permitido ver desde el punto de vista del paciente la evoluci√≥n del funcionamiento de un hospital y del trato al paciente.
Con mi reciente largo ingreso hospitalario (del 31 de agosto al 15 de octubre) debido al linfoma del que me sigo recuperando, he tenido tiempo de sobra para ver como volver a ver el trato al paciente, y compararlo con mis otras experiencias. Durante esta estancia, he estado asignado a 3 servicios: neumolog√≠a, medicina interna y oncolog√≠a, en dos plantas distintas del hospital, y con equipos de enfermer√≠a y m√©dicos distintos.
No puedo nada m√°s que deshacerme en halagos para todas las personas con las que he tenido trato en todo este proceso hospitalario (y extrahospitalario): auxiliares, enfermeras y enfermeros, m√©dicos (incluyendo residentes). Todos ellos han hecho m√°s f√°cil mi estancia en el hospital y el propio proceso m√©dico.
S√≥lo ha habido una excepci√≥n, que me recuerda al trato que se daba antes, pero de ese trato hablar√© un poco m√°s adelante.
En la parte m√©dica, en todo momento los m√©dicos trataban de que yo como paciente entendiese los procesos que me iban a realizar: como iban a ser, la finalidad, los riesgos, etc. Ya no solo por formar el Consentimiento informado sino por que lo entendiese realmente.
De entrada, mis prejuicios me hac√≠an esperar un trato distinto en oncolog√≠a, algo m√°s paternalista, pero nada m√°s lejos de la realidad. 0 paternalismo, 0 drama, llamar a las cosas por su nombre, o al menos sin darles rodeos. Y desde el minuto 0 me dejaron claro que me iban a contar todo y que nadie me iba a mentir en nada, esto me lleva a pensar que el pasado esto se hac√≠a (el no contar la realidad al paciente), supongo que con la intenci√≥n de no a√±adirle presi√≥n psicol√≥gica, ansiedad, o vete tu a saber.
Esto lo quiero dejar muy claro:
Bajo mi punto de vista tienes que tener absoluta confianza en el equipo m√©dico (yo la tengo) y para poder lograr eso no puedes estar pensado que si lo que te dicen esta edulcorado o no se ajusta a la realidad
No puede haber la m√≠nima duda, por que como decimos aqu√≠ en Galicia: a cabeza non para, y si entras ese bucle de dudas, entras en zona peligrosa.
En mi caso, como desde el primer momento yo mostr√© curiosidad por mi enfermedad y por el tratamiento, una de las onc√≥logas, la que me llev√≥ en planta al principio y ahora realiza el seguimiento y programa los tratamientos, me dec√≠a incluso que buscar en internet, y os digo que es un ejercicio maravilloso, por que no es buscar por buscar, si no es una b√∫squeda guiada por m√©dico. En ejemplo, fue en el primer ciclo de quimio (no fu√© un ciclo completo, solo Rituximab y ciclofosfamida) me contaban que ten√≠amos que controlar la cantidad de orina por que me estaban poniendo 4 litros de suero, y tendr√≠a que orinar una cantidad parecida para evitar el S√≠ndrome de lisis tumoral, y ella misma me dijo; &ldquo;tu busca Lisis tumoral&rdquo; y eso fue lo que hice &#x1f61c;
Para m√≠ el saber lo que me est√° pasando es importantisimo, por que me ayuda a hacerme una idea realista de la situaci√≥n sin dejar que el subconsciente se imagine o invente cosas.
Atr√°s quedan los tiempos (que viv√≠ en mis carnes otros ingresos hospitalarios), donde entraba un grupo de gente, que te√≥ricamente eran tu m√©dico con algunos residentes, con suerte te preguntaban como estabas y luego hablaban entre ellos (de ti), y se giraban y se iban. Fin. Nada de explicarte lo que te iban a hacer ni nada por el estilo, con suerte te lo explicaban las enfermeras despu√©s.
En cuanto al equipo de enfermer√≠a y auxiliaries, otro tanto de lo mismo, un trato exquisito que te hac√≠a el d√≠a a d√≠a m√°s f√°cil y c√≥modo, la atenci√≥n a que estuvieses bien, sin dolor es total.
Como ejemplo, una de mis mayores &ldquo;taras&rdquo;: el miedo a las agujas, no es un miedo muy grande, pero si que me genera tensi√≥n saber que me van a pinchar. Desde que inicie este proceso me habr√°n pinchado para extraer sangre o colocar una v√≠a, unas 40-50 veces si no m√°s, en ninguna de ellas me han hecho da√±o y he de reconocer que esto hace que vaya perdiendo ese miedo a las agujas (no del todo).
Seguro que la experiencia de otros no es la misma, o ni siquiera es parecida, pero tambi√©n os puedo decir que si te comportas como un maleducado con la gente que te atiende no esperes que te traten como un rey, no te van a tratar mal, pero desde luego no esperes que hagan m√°s de lo que deben por ti. Eso lo he visto con un compa√±ero de habitaci√≥n que era bastante d√©spota con las enfermeras y auxiliares.
Lo de los compa√±eros de habitaci√≥n en un hospital da para escribir mucho, igual un d√≠a me animo. &#x1f609;
`,url:"https://sergiocarracedo.es/2019/01/28/mi-experiencia-con-el-trato-medico-paciente/",image:"/2019/01/28/mi-experiencia-con-el-trato-medico-paciente/medicina_hu_7324cb551116ad62.jpg",tags:[],readingTime:"5 minutes read",date:"Jan 28, 2019"},"https://sergiocarracedo.es/2018/12/29/Una-prueba-radiologica-PET-CT-un-friki-Yo-y-un-contador-Geiger/":{title:"Una prueba radiol√≥gica (PET-CT), un friki (Yo) y un contador Geiger",content:`El pasado d√≠a 26 de diciembre, tenia programada una prueba radiol√≥gica para hacer un seguimiento del linfoma. Esta prueba era un PET-CT, en castellano: tomograf√≠a por emisi√≥n de positrones es una de las prueba m√°s avanzadas actualmente para detectar las c√©lulas tumorales y su ubicaci√≥n en el cuerpo.
La prueba no invasiva y consiste en inyectar al paciente (yo) un radiof√°rmaco, y mediante un detector observar que partes del cuerpo tienen un metabolismo m√°s r√°pido, estas nos indicar√°n donde est√°n las c√©lulas tumorales.
Con un poco m√°s de detalle, lo que hacen es inyectarme Fl√∫or-18 que es un isotopo inestable del Fl√∫or cuyo periodo de semidesintegraci√≥n es de, aproximadamente, 2h (109,7 minutos). Este isotopo se decae en √ìxigeno-18, que es estable, emitiendo un positr√≥n.
Una vez inyectado el fl√∫or, espero en una sala con paredes emplomadas para contener la radiaci√≥n que empiezas a emitir (esos positrones) durante aprox 1h para que el fl√∫or se reparta por el cuerpo.
Pasado ese tiempo paso a la m√°quina, que a simple vista es como un TAC com√∫n, de hecho el primer &ldquo;barrido&rdquo; que hace es un TAC, esto da al radi√≥logo una imagen base de tu cuerpo, luego, la misma m√°quina comienza un nuevo barrido, ya mucho m√°s lento, que en mi caso (por estatura) tarda 28 minutos, para hacer la detecci√≥n de las zonas con metabolismo acelerado.
El funcionamiento de la detecci√≥n es &ldquo;simple&rdquo;, cuando uno de los positrones emitidos por el radiof√°rmaco (Fl√∫or-18) interact√∫a con un electr√≥n de mi cuerpo se emiten dos fotones de alta energ√≠a, que la m√°quina detecta. En las zonas donde hay c√©lulas tumorales habr√° m√°s de estas interacciones, y por lo tanto m√°s detecciones de fotones, por lo que la imagen superpuesta al TAC tendr√° puntos m√°s brillantes.
Obviamente, tanto los procesos de detecci√≥n como de an√°lisis de los datos, no son tan simples como los planteo, pero la idea es que se entienda lo m√°ximo el proceso.
En todo momento el proceso esta monitorizado por un m√©dico que eval√∫a si la imagen resultante es correcta, y si fuese necesario se repite la detecci√≥n.
Esta es la teor√≠a o la descripci√≥n &ldquo;est√°ndar&rdquo; del proceso, pero ¬øqu√© pasa si a un friki como yo, le prestas un Contador Geiger? &#x1f602;
En primer lugar he de agradecer a Luis Miranda presidente de A industriosa, una asociaci√≥n sin √°nimo de lucro que gestiona y promueve un medialab en Vigo, para que los makers, comunidades tecnol√≥gicas y empresas puedan disponer de equipamiento t√©cnico para llevar a cabo proyectos de todo tipo, el pr√©stamo del contador Geiger.
Te invito a visitar su web y si estas interesado a hacerte socio.
Doy por hecho que sabes que es un contador Geiger, pero por si acaso lo explico r√°pidamente: es un aparato que permite medir la radiactividad emitida por un objeto o lugar. Seguro que lo has visto en alguna pel√≠cula o documental, y sobre todo lo recordar√°s por su caracter√≠stico sonido.
Volviendo al tema, como esta prueba ya la hab√≠a hecho anteriormente y conoc√≠a el funcionamiento, me pregunt√© cuando &ldquo;molar√≠a&rdquo; poder analizar la radiaci√≥n emitida desde dentro de mi cuerpo durante todo el proceso de la prueba (y las siguientes horas). Y gracias a Luis y su contador Geiger eso hice.
Llegu√© al hospital do Meixueiro (donde est√° el departamento de medicina nuclear), un poco antes de las 8 de la ma√±ana y encend√≠ el contador, la radiaci√≥n medida era muy similar (incluso m√°s baja) que la que hab√≠a medido en casa los d√≠as anteriores: 0,25¬µSv/h
Me llamaron y entr√© a la sala de espera emplomada y me inyectaron el contraste, durante unos 30 minutos esper√© tumbado en una camilla sin moverme demasiado para que el radiof√°rmaco fuese extendi√©ndose por el cuerpo. Mientras en el bolsillo de la cazadora tenia el contador Geiger que en ese momento marcaba 37¬µSv/h (llegando en algunos momentos a m√°s de 59¬µSv/h)
Finalmente por un problema t√©cnico con el detector (el PET) no pude realizar la prueba, pero si que pude continuar con el an√°lisis posterior de radiaci√≥n.
En esta gr√°fica extra√≠da del contador Geiger, pueden verse los datos durante todo el dia 26 (notad que la hora no es correcta, esta adelantada 1 hora) es decir el pico m√°ximo es sobre las 9:00, no sobre las 10:00.
Me enviaron a casa unas 3 horas y media despu√©s de la inyecci√≥n del contraste y en el coche de regreso a casa aprovech√© para activar el sonido del contador (que le da m√°s dramatismo) y grab√© este video:
Ya en casa dej√© el contador en el sal√≥n para tener una medida de la radiaci√≥n a unos metros de mi y ver como evolucionaba.
En este video (s√≠, lo siento, est√° grabado en vertical &#x1f61e;) se observa como el contador, al principio detecta poca cantidad de part√≠culas y seg√∫n me acerco a √©l, el sonido de los recuentos aumenta, para luego volver a disminuir al alejarlo.
Durante las siguientes horas fui controlando como bajaba el nivel m√°ximo de radiaci√≥n (incluso con el contador pegado al cuerpo) hasta que aproximadamente a las 6:00 de la ma√±ana la radiaci√≥n estaba en los niveles normales.
Se√±alar que los niveles de radiaci√≥n emitidos son altos (unas 200 veces la radiaci√≥n &ldquo;normal&rdquo;) pero est√°n dentro de los valores considerados seguros, si bien es cierto que la recomendaci√≥n es no estar cerca de embarazadas y/o ni√±os durante las horas siguientes a la prueba, ya que son m√°s vulnerables a la radiaci√≥n.
Para los interesados, el software de manejo del contador Geiger que he usado es Software Libre y est√° escrito en Python: GeigerLog
`,url:"https://sergiocarracedo.es/2018/12/29/Una-prueba-radiologica-PET-CT-un-friki-Yo-y-un-contador-Geiger/",image:"/2018/12/29/Una-prueba-radiologica-PET-CT-un-friki-Yo-y-un-contador-Geiger/PET_cover_hu_fe736191dde7d2ad.jpg",tags:[],readingTime:"5 minutes read",date:"Dec 29, 2018"},"https://sergiocarracedo.es/2018/12/20/Como-es-uno-de-mis-ciclos-de-quimioterapia/":{title:'Como es uno de mis "ciclos" de quimioterapia',content:` Antes de nada aclarar que lo que cuento esta basado en mi experiencia personal, no pretende ser ni una referencia, ni consejos para otras personas.
Como ya coment√© en la anterior entrada, cada vez que te &ldquo;ponen&rdquo; el tratamiento de quimioterapia se le denomina ciclo, en mi caso es cada 21 d√≠as.
Os voy a contar c√≥mo es uno de mis ciclos.
En primer paso es acudir temprano 8:00-8:40 al hospital de d√≠a, all√≠ te hacen una extracci√≥n de sangre para un an√°lisis marcado por la onc√≥loga como urgente, en este an√°lisis se &ldquo;mide&rdquo;:
Plaquetas Leucocitos Bilirrubina Urea sen sangre Na/K en sangre &hellip; Estos resultados le van a indicar a la onc√≥loga si puedes recibir el ciclo de quimio, si hay algo fuera de orden, por ejemplo, los leucocitos bajos no se puede poner y se aplazar√≠a una semana.
Sobre una hora, hora y media despu√©s, tengo la cita de consulta con la onc√≥loga (la hora es estimada por que todos tenemos cita a la misma hora y depende de si llegaron los resultados del an√°lisis del laboratorio).
Est√° cita es para revisar la anal√≠tica reci√©n hecha, ver qu√© tal est√°n los efectos secundarios del anterior ciclo y si todo est√° ok, programar el tratamiento de quimioterapia y tambi√©n me dan la cita para el siguiente ciclo.
Programar el tratamiento de ese d√≠a consiste en una hoja de medicaci√≥n, donde adem√°s de indicar los medicamentos y quimios a poner en v√≠a intravenosa, indica el caudal y el tiempo.
Con la hoja de tratamiento paso a la enfermera que me confirma las citas en el sistema inform√°tico y pide las quimios a farmacia del hospital.
Las quimios se preparan generalmente en el momento y personalizadas para cada paciente, en funci√≥n del peso, √°rea corporal, etc.
Pasas a la sala de espera de nuevo hasta que est√©n las quimios preparadas.
Una vez llegan las quimios al hospital de d√≠a me pasan y me sientan en un sill√≥n (o en mis √∫ltimos ciclos me pasan a una cama ya que me est√°n poniendo un tratamiento intratecal que requiere un pinchazo en la columna). Me canalizan la v√≠a central (reservorio).
El reservorio es una especie de c√°mara que previamente me han implantado en lado derecho la zona superior de las costillas bajo la piel y conectada a una vena &ldquo;gorda&rdquo;. En mi caso es muy parecido a esto https://www.angiodynamics.com/products/20/Smart-Port-CT-Injectable-Port/
La quimio se administra mediante una especie de caudal√≠metro. Esa maquinita como la de la imagen de la cabecera. A m√≠ es algo que siempre me hab√≠a llamado la atenci√≥n al ver a gente recibiendo quimioterapia y no sabia lo que era, hasta ahora que me ha tocado ser parte &#x1f602;.
Esta maquina lo &ldquo;√∫nico&rdquo; que hace es regular el caudal del tratamiento (que suele venir diluido en suero fisiol√≥gico), la enfermera programa el caudal y el tiempo de administraci√≥n.
Entre quimio y quimio se hace lo que se llama un &ldquo;lavado&rdquo; que consiste en poner suero fisiol√≥gico para diluir las quimios.
Algunas &ldquo;quimios&rdquo; requieren de premedicacion, es decir medicamentos para prepararte para la quimio, con por ejemplo: Paracetamol, Ondasentron (para evitar las n√°useas) y otros.
En mi caso la parte de ciclo que se pone en el hospital de d√≠a dura algo m√°s de 7 horas, sobre todo por el Rituximab que tiene que pasar con un caudal bajo (va variando cada 30 minutos inici√°ndolo en 100ml/h hasta 400ml/h subiendo en tramos de 100ml/h) para evitar reacciones adversas (subidas de tensi√≥n, etc&hellip;) que dura sobre 4 horas.
Entre quimio y quimio se hace un &ldquo;lavado&rdquo; con suero fisiol√≥gico.
Algunas de las &ldquo;quimios&rdquo; (vincristina, andrimicina y ciclofosmamida) son vesicantes, es decir provocan ampollas si entran en contacto con la piel, por ese cuando est√°s con ellas no te puedes mover de la silla/cama para evitar el riesgo de que se suelte la v√≠a y la sustancia caiga en la piel.
Sobre la vincristina un dato &ldquo;curioso&rdquo;, seg√∫n la wikipedia Su margen de seguridad es muy estrecho; la dosis de m√°xima efectividad est√° muy pr√≥xima a la t√≥xica letal. lo que te deja bastante tranquilo &#x1f602;
Parte del ciclo tambi√©n son medicamentos que tomas por otras v√≠as, en mi caso la prednisona, un corticoide (antiinflamatorio) cuya administraci√≥n es en pastillas v√≠a oral pero que ya tomo en casa en los siguientes d√≠as.
Est√°s quimios provocan una deterioro del sistema inmunol√≥gico, suele ser en la siguiente semana ya que es el periodo en el cual se renueva el sistema inmunol√≥gico. Para mejorar su recuperaci√≥n me tengo que pinchar en el abdomen (v√≠a subcut√°nea) unas inyecciones de Zarzio (Filgrastim), un medicamento que pone a funcionar la m√©dula √≥sea (donde se producen los leucocitos). Esto me provoca dolor de huesos, como cuando tienes una gripe a lo bestia pero sin los otros s√≠ntomas. Y no debo estar en contacto con ni√±os ni embarazadas (ellos tienen el sistema inmune trabajando fortalecido y pueden contagiar sin ellos tener s√≠ntomas), ni en sitios con mucha gente.
Los efectos secundarios de las quimios var√≠an seg√∫n la persona, en mi caso, no son demasiado &ldquo;graves&rdquo;, y son llevables aunque molestos, por ejemplo:
Piel muy seca Ca√≠da del pelo (parcialmente, en la cabeza no lo he perdido todo y el que queda es m√°s d√©bil y claro) Facilidad para tener heridas o que tarden m√°s en curar (debido al descenso de plaquetas) Llagas o heridas en la boca Sabor met√°lico Cansancio general Dolor de espalda / cuello Aumento desmedido del apetito (Provocado por la Prednisona) Perdida de sensibilidad en las manos (Provocado por la Vincristina) Y algunos m√°s leves&hellip; `,url:"https://sergiocarracedo.es/2018/12/20/Como-es-uno-de-mis-ciclos-de-quimioterapia/",image:"/2018/12/20/Como-es-uno-de-mis-ciclos-de-quimioterapia/bomba_quimioterapia_hu_191248f63c5cf0c3.jpg",tags:[],readingTime:"5 minutes read",date:"Dec 20, 2018"},"https://sergiocarracedo.es/2018/12/08/Cosas-que-he-aprendido-de-un-cancer/":{title:"Cosas que he aprendido de un c√°ncer",content:`Hace unos meses me han diagnosticado un c√°ncer, un Linfoma no Hodgkin difuso de c√©lulas B grandes. Ahora mismo estoy a mitad de tratamiento y me gustar√≠a compartir lo que he aprendido durante este tiempo.
Antes de nada aclarar que lo que cuento esta basado en mi experiencia personal, no pretende ser ni una referencia, ni consejos para otras personas.
La palabra c√°ncer incluye cientos de distintos tipos y subtipos, lo que implica s√≠ntomas, riesgos, complicaciones y tratamientos distintos (muy distintos unos de otros). Aunque la base de la enfermedad es la mismas (una reproducci√≥n anormal y sin control de las c√©lulas tumorales), dependiendo de qu√© cel√∫las sean y donde est√©n situadas es como si fuese una enfermedad distinta.
Yo ni lucho, ni peleo, ni nada que se le parezca contra el c√°ncer. De la misma forma que nadie lucha contra una fractura de un hueso. Es una enfermedad de tratamiento pasivo en la que te limitas a ir al m√©dico, hacer pruebas, recibir los tratamientos que te dan y soportar los efectos secundarios.
Si buscas en internet, TODO ES C√ÅNCER, da igual los s√≠ntomas que busques, acabar√°s encontrando (m√°s pronto que tarde) alguna web de &ldquo;medicina&rdquo; que relacione ese s√≠ntoma por muy inocente que sea con un c√°ncer. Mi recomendaci√≥n, NI CASO
Muchas veces pienso que para mi entorno es a veces m√°s dif√≠cil sobrellevar la situaci√≥n que para m√≠. Ves que no saben que decirte, o como decirlo. Entiendo que es normal, no es una situaci√≥n com√∫n a la que est√©s acostumbrado. Lo que yo agradezco, ya no son las palabras de √°nimo (para alguien tan pragm√°tico como yo las palabras de √°nimo son palabras), sino el saber que si realmente necesitases algo hay gente a tu lado, y s√≠, tienes mucha m√°s gente a tu lado de la que imaginas.
La actitud de uno poco (por no decir nada) va a tener que ver en el resultado del tratamiento. Pero tomarse con un poco de humor negro (o gris) la situaci√≥n de uno mismo te permite echarte unas risas. Puedo contar una peque√±a an√©cdota:
Cuando me iban a hacer la biopsia del ganglio axilar, justo antes de entrar a quir√≥fano (intervenci√≥n con anestesia general), se acerc√≥ la enfermera de quir√≥fano para comprobar los datos (no fuera a ser que me cortasen lo que no era &#x1f602;) y le pidi√≥ el tel√©fono a mi mujer para avisarla mediante SMS cuando saliese del quir√≥fano. Le pregunto la relaci√≥n familiar que tenia conmigo, ella dijo &ldquo;Soy su mujer&rdquo;, y a mi solo se me ocurri√≥ decirles, &ldquo;Bueno, mi viuda si la operaci√≥n sale mal&rdquo;. &#x1f602; &#x1f602; A√∫n me r√≠o ahora solo pensando en sus caras.
Como dec√≠a un amigo: &ldquo;Nos hemos pegado buen galleto, pero ¬øy lo que nos hemos re√≠do?&rdquo;
Si quieres aprendes mogoll√≥n de cosas: Medicamentos, s√≠ntomas, funcionamiento del cuerpo, tratamientos, pruebas diagno√≥ticas, etc. Si adem√°s tienes un grupo de onc√≥logos como los que me llevan a m√≠, ellos te indican donde buscar m√°s info, y eso mola mucho.
En mi caso, un Linfoma se considera un c√°ncer de la sangre, es un crecimiento desmedido de los linfocitos que se acumulan en los ganglios haciendo que crezcan de forma desmesuradas, comprimiendo otros √≥rganos. Como el sistema linf√°tico se considera un √∫nico √≥rgano el hecho de que se detecten c√©lulas tumorales en distintos ganglios lejos unos de otros por el cuerpo NO se considera met√°stasis.
Del punto 1 se puede extraer que el tratamiento de cada c√°ncer es tambi√©n un mundo, no hay soluciones gen√©ricas, incluso para el mismo c√°ncer. Y las quimioterapias no tiene por que ser medicamentos &ldquo;raros&rdquo;, por ejemplo una parte del &ldquo;mio&rdquo; es la prednisona que tiene unos usos m√©dicos m√°s all√° del c√°ncer.
En el caso del lifoma de c√©lulas B Grandes, el tratamiento que parece m√°s t√≠pico es el R-CHOP:
R: Rituximab Es la gran revoluci√≥n en el tratamiento del linfoma de c√©lulas B. El funcionamiento m√©dico se me escapa bastante, pero de algunas lecturas simplificadas se puede resumir en que este f√°rmaco (un anticuerpo) se une al ant√≠geno CD20 y provoca una respuesta inmune del organismo que destruye los linfocitos B. El funcionamiento parece ser tan bueno que, por ejemplo la Fundaci√≥n Josep Carreras est√° buscando financiaci√≥n para ensayos cl√≠nicos de un ant√≠geno similar para los receptores CD30 (que se expresa pr√°cticamente todos los linfomas de Hodgkin y varios subtipos de linfomas no Hodgkin.) C: Ciclofosfamida La ciclofosfamida es un medicamento de la familia de agentes alquilantes. El dato m√°s curioso que he extra√≠do es que es un derivado del Gas mostaza, si, ese que se usaba en la Primera guerra mundial como arma qu√≠mica. H: Hidroxidaunorubicina Es un potente antibi√≥tico extra√≠do de microbios del suelo. Yo a este le llamo &ldquo;BitterKas&rdquo; por que tiene un color practicamente id√©ntico. O: Oncovin o Vincristica P: Prednisona Las &ldquo;quimios&rdquo; se aplican en ciclos (en mi caso 21 d√≠as) y se mide todo en base a esos ciclos. Las quimios tienen mogoll√≥n de efectos secundarios. Ah√≠ vendr√° el gran avance en los tratamientos, en minimizar los efectos secundarios para poder aumentar los primarios. Esta historia a√∫n no ha acabado, asi que seguir√© contando curiosidades sobre mi c√°ncer.
`,url:"https://sergiocarracedo.es/2018/12/08/Cosas-que-he-aprendido-de-un-cancer/",image:"/2018/12/08/Cosas-que-he-aprendido-de-un-cancer/linfoma_1_hu_8f0e7ecbfe282fa2.jpg",tags:[],readingTime:"5 minutes read",date:"Dec 8, 2018"},"https://sergiocarracedo.es/2018/11/29/Mis-canales-favoritos-de-YouTube/":{title:"Mis canales favoritos de YouTube",content:`Hago una seleccion de canales de YouTube interesantes a los que estoy suscrito. No estan todos, bien por que no los considero relevantes o por que hace mucho tiempo que no publican nada
Ciencia y divulgaci√≥n: QuantumFracture ¬°Ciencia! ¬°y con animaciones! El lado m√°s loco (y real) del Universo. Ciencia de sof√° Divulgaci√≥n cient√≠fica para mentes distra√≠das. Jaime Altozano Todo sobre m√∫sica. C√≥mo funciona, por qu√© es como es.Canal para entender lo que oyes. Antroporama Divulgaci√≥n cient√≠fica sobre el Ser Humano, nuestra conducta, mente y nuestro secreto mejor guardado: el cerebro. La gata de Schr√∂dinger Canal de divulgaci√≥n de la ciencia desde una perspectiva esc√©ptica y con un toque de humor. Minuto de f√≠sica En pocas palabras: f√≠sica genial y ciencia interesante Deborah Ciencia Qu√≠mica, divulgadora cient√≠fica y especialista en arte contempor√°neo. Vsauce ENG Divulgaci√≥n cientifica y espirito curioso. Carolina Jim√©nez Artista de VFX especializada en Layout para cine internacional. Physics Girl ENG Ciencia aplicada Date un voltio y Date un vlog Divulgaci√≥pn f√≠sica por Javier Santaolalla. Gamming Buck Fernandez Especializado en juegos de survival. Menos Trece Juegos de todo tipo. Vicio One More Time Juegos de todo tipo, y muchos de ellos curiosos y en partidas compartidas Heikki360ES SimRacing en estado puro Formula 1 Formula 1 Canal oficial de la F1 Efeuno Canal de opini√≥n y resumenes de carreras. Programaci√≥n / T√©cnica Drupal Association Carlos Buenosvinos Zamora Igalia RootedCON General Saul Lopez Coche electrico y movilidad sostenible, desde el punto de vista de un propietario de un Tesla Model S que cuenta curiosidades de su coche. The Slow Mo Guys B√°sicamente explosiones a c√°mara super, super lenta. I_am_puma Una parejas rusa comparte su piso con un Puma llamado –ú–µ—Å—Å–∏ (Messi) Ter De este canal me quedo s√≥lo con los videos de arquitectura. David Bravo Canal del conocido abogado David Bravo. 331Erock Creador de versiones metal de temas conocidos. Humor Pantomima full La vida moderna La resistencia Nadie sabe nada Improvisaci√≥n y humor entre Buenafuente y Berto. Zorman Principamente videos musicales paridiando &ldquo;tribus urbanas&rdquo; Loulogio Humor y reviews de pelis Compilations Win Compilation Recopilaciones mensuales de videos de Wins The Pet Collective Videos de gatetes, perreros, monetes, etc. Varios LockPickingLayer Prueba detalladas de cerraduras, candados, etc. Stop a Douchebag Rusos intentando evitar que otros rusos usen las aceras para circular en coche. Vicisitud y sordidez No sabr√≠a como definir este canal. Destaco especialmente este video https://www.youtube.com/watch?v=k-ZrzH9qorw que todo buen vigu√©s deber√≠a conocer. Nuclear Vault Videos desclasificados de pruebas nucleares. Krzysztof Smejlis Videos de dos gatetes (Bobo y Nikita). Video Buck Reviews de pelis, especialmente ochenteras. `,url:"https://sergiocarracedo.es/2018/11/29/Mis-canales-favoritos-de-YouTube/",image:"/2018/11/29/Mis-canales-favoritos-de-YouTube/youtube_hu_5c77d0486680fc5f.jpg",tags:[],readingTime:"2 minutes read",date:"Nov 29, 2018"},"https://sergiocarracedo.es/2018/01/22/generacion-de-numeros-de-cancelacion-de-factura-para-drupal-commerce/":{title:"Generaci√≥n de n√∫meros de cancelaci√≥n de factura para Drupal Commerce",content:`Recientemente a uno de nuestros clientes para el cual hemos dise√±ado,¬†y programado una tienda online, empleando para ello Drupal 7¬†y Drupal commerce, le surgieron varias necesidades relativas a la facturaci√≥n.
En primer lugar necesitaba que los pedidos generasen autom√°ticamente una serie de n√∫meros de factura por cada a√±o, de la forma¬†**2018-123, 2018-124, &hellip;¬†**Hasta aqu√≠ todo normal, uno de los m√≥dulos contribuidos de¬†Drupal Commerce.
Este m√≥dulo se llama Commerce Billy, y cumple esta necesidad, adem√°s de proporcionarnos la generaci√≥n de facturas en PDF,¬†posibilidad de facturaci√≥n autom√°tica o manual, y un largo etc de peque√±as pero √∫tiles funcionalidades relativas¬†a la facturaci√≥n.
Pero nuestro cliente adem√°s de generar una serie para las facturas de venta, necesitaba que en caso de devoluci√≥n, generar una nueva serie, tambi√©n con n√∫meros consecutivos, pero independiente de la serie de facturaci√≥n.
Commerce Billy no proporcionaba esta funcionalidad, as√≠ que decidimos aprovechar la potencia de¬†hookeado¬†que proporciona Drupal, esto es, poder &ldquo;enganchar&rdquo; tu c√≥digo en distintos punto del flujo del programa, pudiendo alterar¬†o ampliar la funcionalidad del¬†_core¬†_o de un m√≥dulo, sin necesidad de modificarlo, y creamos un sencillo, pero √∫til m√≥dulo que realiza funcionalidad aprovechando parte de las funcionalidades de¬†Commerce Billy, como por ejemplo la configuraci√≥n del tipo de serie (Anual, mensual o infinita) se extrae de la especificada el¬†Commerce Billy, simplificando la configuraci√≥n por parte del usuario.
Nuestro m√≥dulo se llama¬†Commerce Billy Cancel (un nombre muy original ;) ) y hemos decidido liberarlo¬†para que cualquiera que tenga la misma necesidad que nosotros pueda resolverla f√°cilmente.
Pod√©is descargarla aqu√≠
`,url:"https://sergiocarracedo.es/2018/01/22/generacion-de-numeros-de-cancelacion-de-factura-para-drupal-commerce/",image:"/2018/01/22/generacion-de-numeros-de-cancelacion-de-factura-para-drupal-commerce/pexels-photo-811103_1_hu_f399ac8635ce3360.jpeg",tags:[],readingTime:"2 minutes read",date:"Jan 22, 2018"},"https://sergiocarracedo.es/2018/01/05/he-dejado-facebook/":{title:"He dejado de usar Facebook. ¬øY ahora qu√©?",content:`Respuesta larga (TL;DR):
Entre en Facebook el 31 de diciembre de 2008 (casi 10 a√±os ya) pero¬†hace tiempo que¬†ya no me proporcionaba lo que hacia en su momento, una forma de conectar y de compartir momentos interesantes con mis conocidos, amigos y allegados, sino que se hab√≠a convertido en un multitudinario mon√≥logo m√∫ltiple.
He de decir que todos los a√±os por estas fechas tenia la costumbre de revisar mis contactos y eliminar sin pudor aquellas personas con las que no hab√≠a tenido contacto en la propia plataforma (ni un comentario, ni un like, ni un chat en varios meses) ya que no tiene mucho sentido estar &ldquo;ligado&rdquo; a una persona con la que no tienes ni un m√≠nimo de interacci√≥n.
Esto me hab√≠a permitido mantener bastante ordenado mi timeline, evitando en la medida de lo posible, las cadenas de mensaje, los &ldquo;retos&rdquo;, las noticias falsas (hoax) y est√∫pidas que se repet√≠an cada cierto tiempo: el de la leche repasteurizada, el de la propiedad intelectual del contenido y fotos que subes a Facebook, as√≠ un largo etc. Pero √∫ltimamente eran ya un porcentaje alto de publicaciones.
Pero lo que m√°s me ha hecho tomar la decisi√≥n es algo que para muchos puede parecer nimio: El orden¬†del timelime.
Si, puede ser una chorrada¬†para muchos pero para m√≠ el hecho de que Facebook ordenara las publicaciones seg√∫n su criterio me desesperaba, por qu√© no ve√≠a la misma publicaci√≥n repetidas veces, y no sabia si lo que hab√≠a &ldquo;debajo&rdquo; ya lo hab√≠a visto o no. esta m√°s que claro que esto hace que est√©s m√°s tiempo en la plataforma, lo que es el objetivo principal de Facebook o al menos uno de los principales. Y esto no es una paranoia o una teor√≠a de la conspiraci√≥n, el inter√©s de ordenar las noticias seg√∫n el criterio de Facebook es importarte, por ejemplo, en la versi√≥n web puedes marcar que te muestre las &ldquo;M√°s recientes&rdquo; pero cuando haces click en el logo de Facebook para volver la timeline¬†&ldquo;olvida&rdquo; esta opci√≥n. Instal√© una extensi√≥n que a√±ad√≠a un par√°metro en la url de la home del¬†timeline¬†para forzar el orden por fecha, pero al poco tiempo Facebook lo empez√≥ a ignorar.
En la app m√≥vil, que es una devoradora de recursos (Espacio ocupado, memoria, bater√≠a) era imposible siquiera esta opci√≥n de orden de una forma sencilla (sin tener que pasar por 5 men√∫s).
As√≠ que poco a poco y de forma bastante natural y nada traum√°tica, me di cuenta de que lo usaba menos, hasta un d√≠a, en el que decid√≠ desinstalar la app m√≥vil y¬†cerrar la sesi√≥n web.¬†He de decir que me plante√© eliminar la cuenta, pero al ser desarrollador¬†y tener apps¬†que usan la api de Facebook iba a ser un engorro.¬†Tambi√©n me he descargado todos los datos que en teor√≠a Facebook tiene de m√≠, ah√≠ puedes consultar como hacerlo¬†https://es-es.facebook.com/help/131112897028467 y podr√°s comprobar por ti mismo como Facebook no olvida nada, cualquier &ldquo;amigo&rdquo; que tuvieses y que hubieses eliminado, seguir√° ah√≠ en un listado de tus relaciones.
¬øTe estoy animando a que dejes tu tambi√©n Facebook? Para nada, simplemente quer√≠a exponer mi punto de vista y mi experiencia respecto a esto.
¬øNo lo echas de menos? No, soy un¬†hard-user¬†de Twitter y para conectar con la familia, amigos y colegas uso otros m√©todos.
Resumiendo, ¬øqu√© ha pasado despu√©s de dejar Facebook? Nada malo, no he perdido amigos (de los de verdad) ni nada parecido, ni la gente me ha dejado de hablar, ni estoy fuera del mundo sin enterarme de las &ldquo;√∫ltimas novedades&rdquo;.
`,url:"https://sergiocarracedo.es/2018/01/05/he-dejado-facebook/",image:"/2018/01/05/he-dejado-facebook/facebook_hu_b7adef167379b708.jpeg",tags:[],readingTime:"3 minutes read",date:"Jan 5, 2018"},"https://sergiocarracedo.es/blog/2017/abstraer-los-datos-de-su-representacion-en-un-cms-win/":{title:"Abstraer los datos de su representaci√≥n en un CMS = WIN",content:`Tanto en desarrollo web¬†como en otras muchas √°reas la¬†abstracci√≥n¬†es fundamental para poder avanzar sin necesidad de estar pensando en cada uno de los niveles de complejidad.¬†La¬†abstracci√≥n¬†b√°sicamente consiste en aislar un elemento de su contexto.
Si has le√≠do el t√≠tulo del¬†post, te habr√° dado cuenta de que¬†vamos a hablar de abstracci√≥n de contenido respecto a la **representaci√≥n (**que tendr√° para el visitante de la web), por ejemplo esto es¬†que¬†el campo fecha¬†(de un art√≠culo), no lo tenga que insertar el autor junto con el resto del texto.¬†Lo mismo que el t√≠tulo, las im√°genes o cualquier otro elemento.
¬øY por qu√©¬†no hacerlo as√≠, insertando todo en un campo de texto? Se me ocurren muchos motivos para no hacerlo as√≠:¬†por ejemplo, si necesitas¬†ordenar por fecha y la fecha esta en medio del texto, la tarea de ordenaci√≥n ser√° mucho m√°s complicada que si la fecha estuviese en un campo separado.
A√∫n m√°s, si la fecha adem√°s de estar guardada en un campo separado, est√° guardada de forma estructurada (por ejemplo un timestamp + timezone) podremos mostrar la fecha y hora en las distintas zonas horarias y en los distintos formatos de fecha:¬†DD/MM/YYY, MM/DD/YYY, etc
¬øC√≥mo puedo llevar la abstracci√≥n a cabo? Existen varias formas de hacerlo:
Tokenizado: El tokenizado no es la soluci√≥n optima, pero si es una mejora respecto a insertar todo &ldquo;junto&rdquo; en un solo campo.¬†En este caso, lo que hacemos en lugar de insertar la fecha en el campo principal, podemos insertar un token se substituir√° por la fecha, por ejemplo [fecha_creacion].
Esta soluci√≥n solo se puede aplicar a algunos campos, ya que al final dependemos de datos ya separados por lo cual es mejor usar el siguiente m√©todo.
Separaci√≥n en¬†campos:¬†Consiste en realizar un an√°lisis previo del contenido para separarlo en campos independientes.
Vamos a ver el m√©todo de¬†Separaci√≥n en campos¬†con un ejemplo: Supongamos esta misma entrada del blog, en esta web, ¬øqu√©¬†elementos la componen?, A primera vista, un¬†t√≠tulo, un¬†cuerpo de texto,¬†la¬†imagen¬†de cabecera, la¬†fecha, las etiquetas y los¬†relacionados.
Adem√°s de estos elementos, en algunas entradas necesito a√±adir varias im√°genes¬†o videos.
Pues ya tenemos los elementos, ahora en nuestro backend¬†tenemos un campo para cada uno de estos elementos como puede verse en la imagen inferior:
Quiero llamar la atenci√≥n sobre el campo¬†Cuerpo de texto¬†que es un habitual campo de texto WYSIWYG. Este tipo de campos es &ldquo;peligroso&rdquo; por que puede tender a ser un caj√≥n desastre donde terminar insertando todo tipo de contenido. En mi opini√≥n para evitar esto, y conseguir la m√°xima abstracci√≥n posible este campo solo deber√≠a permitir insertar:
Negritas (strong), cursiva (em) Enlaces (a) T√≠tulos (h2, h3&hellip;.) pero solo de modo sem√°ntico, es decir, que como se muestren corre a cargo del frontend Listas (ul, ol) En general otras etiquetas que aporten valor sem√°ntico al contenido, nada que tenga que ver con la visualizaci√≥n:
Colores Tama√±os de texto Tipograficas Im√°genes (como etiqueta,, como token es correcto) ¬øQu√© ventajas me da abstraer contenido y separarlo de la visualizaci√≥n?
Como ya mencionamos antes, una de las ventajas es poder manipular y transformar el contenido antes su visualizaci√≥n, por ejemplo, la fecha en esta entrada de blog, aunque fue introducida como una fecha DD/MM/YYYY HH:MM, se muestra en un formato relativo (Hace x d√≠as)
Las im√°genes, son recortadas y reescaladas seg√∫n necesidades, pudiendo tener distintos tama√±o en funci√≥n del tama√±o de la pantalla para evitar la descarga innecesaria de datos. Todo ello manteniendo la imagen original inalterada.
Pero una ventaja que se suele ver a largo plazo, y que para nosotros es muy importante, es que podr√≠amos cambiar el dise√±o de la web sin necesidad de tocar el contenido en el backend.
Un cambio de tipograf√≠as y/o colores por que el cliente ha cambiado su imagen corporativa, una restructuraci√≥n profunda del dise√±o, por ejemplo moviendo elementos de sitio.
Esto no es solo teor√≠a, tenemos casos reales de clientes que han modificado hasta 4 veces el dise√±o de la web a o largo de 8 o 9 a√±os, manteniendo la misma estructura interna de contenido, pero modificando la visualizaci√≥n de la web.
Para finalizar, solo puedo alabar la simplicidad que nos aporta¬†Drupal¬†a la hora de estructurar el contenido de formas correcta, y mantenerlo separado de la visualizaci√≥n.
`,url:"https://sergiocarracedo.es/blog/2017/abstraer-los-datos-de-su-representacion-en-un-cms-win/",image:"/blog/2017/abstraer-los-datos-de-su-representacion-en-un-cms-win/architecture-construction-build-building-162557_hu_2a584d0bafb8e640.jpeg",tags:[],readingTime:"4 minutes read",date:"Nov 27, 2017"},"https://sergiocarracedo.es/2017/10/23/crear-plantillas-de-email-con-nuxtjs/":{title:"Crear plantillas de email con nuxt.js",content:`Si eres maquetador Web y quieres viajar al pasado, a un pasado en el que se maquetaban las webs usando tablas anidas hasta el infinito, im√°genes de 1x1px¬†transparentes, la etiqueta¬†, en lugar de , y un largo ec√©teras de cosas que me ponen los pelos de punta.
Como, probablemente has podido extraer de mis palabras, para mi es una pesadilla. maquetar una plantailla de email, por que es como si de golpe te quitasen todas las herramientas que tienes para hacerlo y con un martillo y cincel, por eso trato siemple de simplificar lo m√°ximo posible el trabajo buscnado m√©todos que me permitan que la tarea de maquetar un email tengoa un poco de dignidad¬†;).
Revisando maquetaciones pasadas me he dado cuenta que muchos elementos se repiten, y que es posible modularizarlo,¬†aprovechando que estoy de lleno con Vue.js y Nuxt,js,¬†he visto que encaja muy bien, ya que debido al modelos de componentes de Vue.js podemos &ldquo;trocear&rdquo; la maqueta en esos componentes que por supuesto podemos reutilizar, por ejemplo una noticia, podria ser un componente, con su foto, texto y titulo y esta se repetir√° varias veces en la maqueta, por lo que un v-for nos resuelve la repetici√≥n de forma sencilla, he incluso podriamos pasarles valores para generar distintas noticias.
Al tener los componentes una sola vez, si necesitamos hacer un ajuste de maquetaci√≥n en una noticia, este ya se aplica a todas.
Una vez maquetado, entra la parte¬†nuxt.js¬†que con su funci√≥n de gneraci√≥n de p√°gina est√°ticas al estilo Jekyillrb o Hugo, nos permite tener un html puro de cada plantailla y esta es la que pasaremos al programa de envio de emails / newsletters o a nuestro cliente,
Si nos llegan cambios de cliente, hacer una modificaci√≥n es sencillo, se modifican los componentes necesarios y se vuelve a genear.
No es que este uso de nuxt.js sea el invento del siglo, pero si es una forma un poco distinta de sacarle partido.
`,url:"https://sergiocarracedo.es/2017/10/23/crear-plantillas-de-email-con-nuxtjs/",image:"/2017/10/23/crear-plantillas-de-email-con-nuxtjs/pexels-photo-261706_hu_3eec1c5caea80e4f.jpeg",tags:[],readingTime:"2 minutes read",date:"Oct 23, 2017"},"https://sergiocarracedo.es/2017/06/13/por-que-el-directorio-home-en-linux-se-representa-con-tilde/":{title:"¬øPor qu√© el directorio home en linux se representa con ~ (tilde)?",content:`Cuando comenc√© a dar mis primeros pasos en Linux me llam√≥ la atenci√≥n que si quer√≠a cambiar a mi directorio home, lo pod√≠a hacer de &ldquo;la forma larga&rdquo;
$ cd /home/usuario O usando¬†~ (lo que los anglosajones llaman¬†tilde)
$ cd ~/ lo que era un ahorro de escritura y la mayor ventaja que un script funcionaria para cualquier usuario.
Alguna vez me pregunte cual era el origen del uso de este car√°cter, pero nunca investigu√© mucho m√°s, hasta que hace unos d√≠as encontr√© esta explicaci√≥n en¬†https://unix.stackexchange.com/questions/34196/why-was-chosen-to-represent-the-home-directory/34198#34198 que no deja de ser curiosa y traduzco de forma libre &ldquo;En los sistemas operativos tipo Unix, la tilde significa el directorio home, una practica derivada de la Terminal¬†Lear-Siegler ADM-3A¬†muy com√∫n en los 70s que en la misma tecla aparec√≠a la¬†tilde¬†y la inscripci√≥n¬†_HOME¬†_que servia para ir a la parte superior de la pantalla.
Como tambi√©n se comenta esta terminal tambi√©n es el origen de las teclas de movimiento de Vi (solo tienes que ver la foto para darte cuenta de porqu√©. ;)
`,url:"https://sergiocarracedo.es/2017/06/13/por-que-el-directorio-home-en-linux-se-representa-con-tilde/",image:"/2017/06/13/por-que-el-directorio-home-en-linux-se-representa-con-tilde/pexels-photo-698808_hu_f09123cb0d583f11.jpeg",tags:[],readingTime:"1 minute read",date:"Jun 13, 2017"},"https://sergiocarracedo.es/2017/06/08/usar-un-field-widget-en-un-formulario-convencional-en-drupal-7/":{title:"Usar un 'field widget' en un formulario convencional en Drupal 7",content:`El Form API de Drupal es muy potente, para m√≠ es algo, que cuando programo sobre otros CMS o frameworks echo mucho en falta, por que Drupal ha conseguido que la laboriosa tarea de crear, validar, y securizar formularios, quede reducida a un &ldquo;simple&rdquo; array.
Este Form API dispone de unos tipos, que yo denominar√≠a primitivos, y no porque&rsquo;est√©n anticuados, si no por que a partir de ellos se pueden crear construcciones m√°s complejas, como hace el m√≥dulo Field, m√°s concretamente su subm√≥dulo Field UI, que como ya sabr√©is es el que nos facilita la creaci√≥n de los distintos tipos de campos, por ejemplo en la creaci√≥n de contenido.¬†Doy por hecho que conoces Drupal y con ello lo potente que es a la hora de tener distintos tipos de campos para crear contenido, no los campos convencionales de una base de datos, si no campos como, un mapa de localizaci√≥n, im√°genes que pueden ser recortadas, un campo de direcci√≥n postal¬†que cambia seg√∫n el pa√≠s seleccionado, etc&hellip;.
Estos campos (fields) disponen de¬†widgets¬†para introducir los valores, por ejemplo el campo localizaci√≥n en lugar de pedir, las coordenadas de forma num√©rica (algo generalmente complicado para un usuario), lo hace mostrando un mapa sobre el que podemos hacer b√∫squedas y pinchar en la localizaci√≥n que deseemos, esto internamente sigue siendo un par de n√∫meros, pero la forma de introducirlos es lo que marca la diferencia.
Estos widgets¬†que implementan los m√≥dulos (ya sea el propio¬†Fields¬†o los de terceros) lo que hacen en realidad es usar como base la potencia del¬†**Form API. ¬ø**Y si pudi√©semos nosotros reutilizar los widgets que se est√°n usando, por ejemplo en un formulario de creaci√≥n de un tipo de contenido (nodo)?. Pues podemos y es relativamente sencillo.
function mymodule_register_form($form, &amp;$form_state) { $form[&#39;organization_name&#39;] = array( &#39;#type&#39; =&gt; &#39;textfield&#39;, &#39;#title&#39; =&gt; &#39;Name of the organization&#39;, &#39;#required&#39; =&gt; true ); $tmpnode = new stdClass(); $tmpnode-&gt;type = &#39;organization&#39;; field_attach_form(&#39;node&#39;, $tmpnode, $form, $form_state, LANGUAGE_NONE, array( &#39;field_name&#39; =&gt; &#39;field_full_address&#39;, )); return $form; } En el ejemplo, tenemos un formulario est√°ndar con un campo¬†organization_name¬†que es un simple textfield¬†del Form API, pero adem√°s, necesitamos recuperar una direcci√≥n, para lo cual podr√≠amos crear varios campos y programar la intereactividad, buscar la lista de pa√≠ses, con sus provincias y como se estructura una direcci√≥n en cada uno, etc. Podr√≠amos hacer, si, pero ya est√° hecho por el m√≥dulo¬†addressfield, que ademas estamos usando en un tipo de contenido llamado &ldquo;Organizaci√≥n&rdquo; con el nombre de campo¬†_field_full_address¬†_y lo √∫nico que tenemos que hacer es reusar el widget de dicho campo.
$tmpnode = new stdClass(); $tmpnode-&gt;type = &#39;organization&#39;; field_attach_form(&#39;node&#39;, $tmpnode, $form, $form_state, LANGUAGE_NONE, array( &#39;field_name&#39; =&gt; &#39;field_full_address&#39;, )); Con las lineas encima de este p√°rrafo, lo que estamos haciendo es crear un objeto est√°ndar con la √∫nica propiedad¬†type¬†que indica el tipo de nodo donde esta el campo del que vamos a &ldquo;copiar&rdquo; el widget. E invocando a field_attach_form¬†lo que hacemos es &ldquo;inyectar&rdquo; en un formulario todos los campos de otro (en este caso todos los campos del formulario que se mostrar√≠a al crear o editar un nodo de tipo¬†organization, pero como solo queremos usar un campo concreto, el √∫ltimo par√°metro es un array de opciones donde especificamos que solo queremos usar el campo llamado¬†_field_full_address ¬°_y listo!
Ya podemos usar el widget en nuestro¬†formulario.
El caso que ha motivado esta entrada de blog, es algo que puede ser relativamente¬†habitual, en mi caso, en el formulario de registro de usuario de Drupal, necesitaba pedir una serie de campos adicionales (como la direcci√≥n) que no iban a ser almacenados en el usuario, sino que crear√≠an un nodo en nombre del usuario con los datos adicionales pedidos en el formulario de registro, de esta forma el usuario solo tiene que completar un formulario mejorando su experiencia en el uso de la web.
`,url:"https://sergiocarracedo.es/2017/06/08/usar-un-field-widget-en-un-formulario-convencional-en-drupal-7/",image:"/2017/06/08/usar-un-field-widget-en-un-formulario-convencional-en-drupal-7/pexels-photo-268466_0_hu_605297ea5b919ae8.jpeg",tags:[],readingTime:"4 minutes read",date:"Jun 8, 2017"},"https://sergiocarracedo.es/2017/05/18/recibir-notificaciones-de-acceso-ssh-desde-una-ip-desconocida/":{title:"Recibir notificaciones de acceso SSH desde una IP desconocida",content:`Es habitual que administremos o accedamos de una u otra forma a m√°quina remotas mediante SSH, ya sea mediante contrase√±a o¬†key¬†(muy recomendado), pero el acceso remoto es una vulnerabilidad en si misma, o al menos es un posible punto de entrada de un atacante o individuo malicioso.¬†Lo habitual es disponer de¬†fail2ban¬†sobre el servicio sshd¬†que evite los ataques de fuerza bruta.
Pero adem√°s, puede ser muy interesante recibir una notificaci√≥n (SMS, email, Push, etc&hellip;) cada vez que se produzca un login correcto en el sistema mediante¬†ssh.
PAM es tu amigo Linux Pluggable Authentication Modules (PAM), sin entrar en mucho detalle, es un sistema de m√≥dulos de autentificaci√≥n para Linux. Simplificando PAM nos permite indicar que se ejecute una aplicaci√≥n o un script¬†_bash_bajo ciertas condiciones, como puede ser en el caso que comentamos, cuando un usuario se loguee exitosamente mediante SSH en nuestro sistema.
En la mayor√≠a de los sistemas el fichero relativo el login ssh se encuentra en /etc/pam.d/sshd, as√≠ que editamos dicho fichero
y a√±adimos el final del mismo lo siguente
session optional pam_exec.so /usr/local/bin/notify-on-ssh-login.sh
Con esta modificaci√≥n estamos indicando que se ejecute un script¬†on login.
El contenido del fichero que se ejecutar√° es el siguiente:
!/bin/bash if [ &#34;$PAM_TYPE&#34; != &#34;open_session&#34; ] then exit 0 fi MY_IP=_XXX.XXX.XXX.XXX_ REMOTE_IP=$(getent hosts &#34;$PAM_RHOST&#34; | awk &#39;{ print $1 }&#39;) if [ &#34;$OFFICE_IP&#34; != &#34;$REMOTE_IP&#34; ] then echo &#34;Login &#39;$PAM_USER&#39; from &#39;$REMOTE_IP&#39;&#34; | mailx -s &#34;ALERT!!!!&#34; _recipient@somewhere.com_ fi exit 0 Este script lo que hace es comprobar que se ha abierto una sesi√≥n (desde SSH), si es asi, comprueba que la IP remota (desde la que se han conectado) no coincide con la que nosotros especifiquemos (para evitar recibir notificaciones cada vez que nosotros mismos nos logueemos) y si no coincide, emplea el¬†mailx¬†para enviar un correo avis√°ndonos de esto.
En mi caso, como m√©todo de notificaci√≥n uso Pushover¬†que permite lanzar notificaciones push a tu m√≥vil, y como ventaja aporta niveles de prioridad, permitiendo, por ejemplo, en un caso &ldquo;grave&rdquo; como este que se salte el modo &ldquo;no molestar&rdquo; del tel√©fono.
Obviamente este script es mejorable y es posible que realice las acciones que nosotros indiquemos.
Usando PAM¬†tambi√©n podr√≠amos tener notificaciones de conexiones¬†ppp, o cuando un usuario haga un¬†sudo, hay mucho donde¬†&ldquo;jugar&rdquo;.
`,url:"https://sergiocarracedo.es/2017/05/18/recibir-notificaciones-de-acceso-ssh-desde-una-ip-desconocida/",image:"/2017/05/18/recibir-notificaciones-de-acceso-ssh-desde-una-ip-desconocida/pexels-photo-397225_hu_a52a5084f2447a5b.jpeg",tags:[],readingTime:"2 minutes read",date:"May 18, 2017"},"https://sergiocarracedo.es/2017/05/04/vigo-hace-pina/":{title:"Vigo hace pi√±a",content:`Hace un par de meses, comentaba que¬†&ldquo;algo se mov√≠a&rdquo; en Vigo , que las comunidades de desarrolladores, y en general t√©cnicas, comenzaban aparecer y crecer.
Pues bien, esto pasa al siguiente nivel: de la mundana necesidad de coordinar los espacios y las fechas de los meetups de los distintos grupos, ha surgido algo m√°s, algo que personalmente me ilusiona mucho, por que siempre es bonito ver crecer algo, una idea, pero m√°s si uno es participe aunque sea en una peque√±a proporci√≥n.
Ha nacido¬†VigoTech Alliance, un grupo¬†de grupos, o un¬†meta grupo, cuyo principal prop√≥sito es sumar la fuerza de cada uno de los grupos que lo forman: Agile Vigo, Codesign Project, GDG Vigo, JoomlaVigo, JavascriptVigo, PHPVigo, PythonVigo, SysAdminGalicia, VigoJUG¬†y por √∫ltimo, pero no menos importante VigoLabs.
Esa suma de fuerza de conocimiento, de cada grupo se concreta, por ejemplo, en la difusi√≥n de las fechas del resto de grupos y cada grupo, aumentando la visibilidad de todos, y aportando a los asistentes de cada reuni√≥n la posibilidad de seguir aprendiendo tambi√©n en otro de los grupos.
Como ya comentaba en el anterior post se trata de ense√±ar al mundo (si, ya s√© que suena pretencioso, pero d√©jame so√±ar un poco ;) ) lo que se hace en Vigo y en su entorno, por que se hacen cosas y cosas muy chulas, y muy potentes que poco tienen que envidiar a lo realizado en otras partes del mundo. (y no miro a nadie, coff, coff,¬†Silicon Valley).
Creo que alg√∫n d√≠a har√© un¬†post¬†listando las &ldquo;cosas&rdquo; tecnol√≥gicas que se hacen en Vigo y alrededores y que, por desgracia, tiene mucha menos difusi√≥n que si se hiciesen desde el otro lado del Atl√°ntico, pero ese tema para otro d√≠a&hellip;.
Como dec√≠a al principio, es ilusionante aportar un peque√±o granito de arena a un proyecto o una idea en la que crees, por eso de las mano de Pedro Figueras hemos aportado la identidad de¬†VigoTech Alliance, la cual pod√©is consultar en¬†https://github.com/VigoTech/Design-elements, os recomiendo leer la argumentaci√≥n de como se construye la idea¬†https://github.com/VigoTech/Design-elements/blob/master/README.md
Por √∫ltimo dar las gracias a todas las personas que forman cada uno de los grupos, que aportan su tiempo sin m√°s inter√©s que el de hacer crecer la comunidad en Vigo.
`,url:"https://sergiocarracedo.es/2017/05/04/vigo-hace-pina/",image:"/2017/05/04/vigo-hace-pina/pexels-photo-244538_hu_6fa21b127e1a8567.jpeg",tags:[],readingTime:"2 minutes read",date:"May 4, 2017"},"https://sergiocarracedo.es/2017/04/18/usando-vuejs-electronjs-para-crear-aplicaciones-de-escritorio/":{title:"Usando Vue.js + electron.js para crear aplicaciones de escritorio",content:`**Electron.js¬†**es framework¬†pensado para crear aplicaciones &ldquo;nativas&rdquo; de escritorio usando JavaScript, HTML y CSS. B√°sicamente ejecuta tu c√≥digo en node.js y realiza el render de la app en Chromium.
Este detalle lo debemos tener muy en cuenta: por un lado tenemos un proceso (main process) que es como su nombre indica el proceso principal de la aplicaci√≥n, y que mientras este ejecut√°ndose, nuestra aplicaci√≥n esta funcionando.¬†El¬†Main process, no tiene visualizaci√≥n, es decir, no es la ventana principal, pero si que una de sis funciones es crear la ventana o ventanas de nuestra aplicaci√≥n, y esto nos lleva a la segunda pieza; el/los procesos de render (Renderer process), cada ventana que cree (no tiene por que visualizarse en ese momento) crea una instancia de¬†Chormium¬†que corre en su propio proceso.y de forma independiente a las dem√°s. Cada una de estos procesos (renderer process) unicamente puede comunicarse con el main, no hay posibilidad de comunicaci√≥n¬†directa¬†entre¬†renderers.
El objeto de este post, no es entrar en los entresijos de¬†_electron.js,¬†_sino¬†que, aprovechando la experiencia que tenemos en¬†vue.js, comentar las posibilidades que nos ofrece el hecho de unir los dos frameworks¬†en un mismo proyecto.
Vue.js¬†es JavaScript, as√≠ que por lo tanto y de forma directa funciona sin ning√∫n tipo de problema en cualquiera de los procesos de render de electron, ya que no son m√°s que una instancia de¬†Chromium, como hemos dicho. As√≠ que por lo tanto emplear ambos frameworks es sencillamente¬†trivial.¬†Ahora bien, podemos simplificarnos m√°s la vida usando un¬†boilerplate¬†que nos cree el¬†scaffolding¬†de nuestra futura app para que solo tengamos que ponernos a escribir nuestro c√≥digo.
Este boilerplate¬†lo tenemos en https://github.com/SimulatedGREG/electron-vue y su instalaci√≥n es tan sencilla como:
# Instalar vue-cli y la plantilla de scaffold npm install -g vue-cli vue init simulatedgreg/electron-vue my-project # Instalar las dependencias y ejecutar tu app cd my-project npm install npm run dev Y¬†voil√†¬†ya podemos empezar a crear una app de escritorio.
Este¬†boilerplate¬†adem√°s ya nos instala¬†vuex.
Como prueba de concepto he creado una aplicaci√≥n que permite tener¬†una ventana para usar como fondo del OBS en las grabaciones de las reuniones las comunidades de usuarios, permitiendo configurar de forma sencilla la lista de charla y ponente, escoger para que comunidad se configura el fondo, podemos tener incluso un cronometro para las charlas¬†lightning.
https://github.com/sergiocarracedo/ug-obs-background
Os recomiendo echarle un vistazo, y si cre√©is que se puede mejorar los¬†pull requests¬†son bienvenidos.
`,url:"https://sergiocarracedo.es/2017/04/18/usando-vuejs-electronjs-para-crear-aplicaciones-de-escritorio/",image:"/2017/04/18/usando-vuejs-electronjs-para-crear-aplicaciones-de-escritorio/screenshot_from_2017-04-11_20-38-13_hu_5920918fd55787c.png",tags:[],readingTime:"2 minutes read",date:"Apr 18, 2017"},"https://sergiocarracedo.es/2017/04/06/vuex-el-redux-de-vuejs-ii/":{title:"Vuex el Redux de Vue.js (II)",content:`En la primera parte , expliqu√©¬†algunos de los conceptos b√°sicos de¬†**vuex:¬†**El¬†**state,¬†**¬†los¬†getters¬†y las¬†mutations. Si recuerdas, el paradigma de una¬†librer√≠a de control del patr√≥n de estado¬†como¬†vuex¬†es mantener el state como √∫nica fuente de verdad, y para ello los valores del estado solo pueden ser modificados mediante¬†mutations, que deben ser adem√°s¬†s√≠ncronas. Si necesitamos hacer &ldquo;cambios&rdquo; de forma as√≠ncrona podemos hacer uso de las acciones / actions.
Hago aqu√≠¬†un peque√±o inciso, aunque parezca repetitivo, escribir√© al menos una vez todos los conceptos en castellano y en ingles; en castellano para que se entiendan mejor los conceptos y la lectura no sea extra√±a y en ingl√©s para mantener la misma terminolog√≠a que¬†vuex.
Acciones / Actions Las acciones son similares a las mutaciones, con dos diferencias principales:
Las acciones no modifican el estado, sino que las acciones¬†emiten mutaciones (commit mutations) Las acciones pueden contener operaciones as√≠ncronas. Las acciones reciben un objeto¬†contexto (context)¬†que expone los mismos m√©todos y propiedades que el Store¬†(el store no es m√°s que la instancia de vuex que contiene el estado, las mutaciones, las acciones y los getters.
Una acci√≥n se ver√≠a as√≠:
actions: { getAllPosts: (context) =&gt; { Api.getAllPosts().then(response =&gt; { let posts = response.data; context.commit(types.RECEIVE_POSTS, { posts }) }) } } Esta acci√≥n, como ves es as√≠ncrona, ya que descarga una lista de posts¬†haciendo uso de un objeto API, y una vez resuelta la promesa, emite un¬†_mutation¬†_pas√°ndole como payload¬†la lista de posts.
Las acciones tampoco se llaman de forma directa, si no que lo hacemos mediante un¬†dispatch.
store.dispatch(&#39;getAllPosts&#39;, {payload}) Como ves, la acci√≥n puede tambi√©n recibir un¬†payload, de la misma forma que una mutaci√≥n.
Como hemos visto las acciones pueden ser as√≠ncronas y si necesitamos saber cuando se ha ejecutado la podemos devolver una¬†promesa¬†que el¬†dispacher¬†se encargar√° de devolver. Modificando el ejemplo anterior:
actions: { getAllPosts: (context) =&gt; { return new Promise((resolve, reject) =&gt; { Api.getAllPosts().then(response =&gt; { let posts = response.data; context.commit(types.RECEIVE_POSTS, { posts }) }) } } } // y de esta forma llamamos a la acci√≥n y obtenemos su promesa store.dispatch(&#39;getAllPosts&#39;).then(() =&gt; { //...... }) Y ahora, ¬øcomo usamos todo esto en la vida real?. La respuesta es sencilla, (doy por hecho que tienes vuex instalado como dependencia de tu proyecto). Por un lado debemos crear un store de esta forma:
// Asegurate de llamar a Vue.use(Vuex) primero const store = new Vuex.Store({ state: { count: 0 }, mutations: { increment: (state) =&gt; { state.count++ } }, actions: { }, getters: { } }) //Ahora puedes modificar el estado y acceder a el de esta forma store.commit(&#39;increment&#39;); console.log(store.status.count); // retorna 1 Usando el estado y las mutaciones en componentes Ahora, lo l√≥gico es que queramos acceder al estado o emitir una mutaci√≥n desde un componente. Lo primero es inyectar el¬†store¬†reci√©n creado en Vue, para que este se inyecte en todos los componentes (por el momento no entraremos en m√°s detalles, pero es posible usar un¬†store¬†de forma local en un componente)
const app = new Vue({ el: &#39;#app&#39;, store, ...... }) Para acceder al store desde un componente lo hacemos de la sigueinte manera:
Lo normal es que queramos usar la reactividad, as√≠ que solo tenemos que hacer uso de las¬†**propiedades computadas¬†**(computed properties)
const Counter = { //.... computed: { count() { return this.$store.state.count } } } De un modo similar, podemos llamar a las acciones o a las mutaciones desde un componente:
this.$store.dispatch(&#39;accion&#39;) this.$store.commit(&#39;mutacion&#39;) Todo lo que aqu√≠ he contado, no es m√°s que la punta del iceberg de¬†vuex, hay mucho m√°s detr√°s (por ejemplo los mapping helpers, los m√≥dulos, el¬†scaffolding). Yo os recomiendo leer en detalle la documentaci√≥n¬†https://vuex.vuejs.org/en/
Tambi√©n he dejado un github https://github.com/sergiocarracedo/vuex-demo un proyecto de ejemplo, algo sencillo, simplemente descarga una lista de posts y los pagina.
Espero que estos dos posts sobre¬†vuex¬†te gustasen y espero hacer en el futuro alg√∫n otro entrando m√°s en detalle.
`,url:"https://sergiocarracedo.es/2017/04/06/vuex-el-redux-de-vuejs-ii/",image:"/2017/04/06/vuex-el-redux-de-vuejs-ii/pexels-photo-99541_1_hu_e226ff4c6e3faf8c.jpeg",tags:[],readingTime:"4 minutes read",date:"Apr 6, 2017"},"https://sergiocarracedo.es/2017/04/03/vuex-el-redux-de-vuejs-i/":{title:"Vuex el Redux de Vue.js (I)",content:`Vuex se define a si mismo como una¬†librer√≠a de control del patr√≥n de estado. Veremos lo que implica este patr√≥n, pero antes veamos los conceptos b√°sicos:
Estado (state) El estado es un objeto plano, este contiene los valores que definen tu aplicaci√≥n en un momento dado. Es la¬†√∫nica fuente de¬†verdad, es decir, es el √∫nico lugar donde consultar estos valores. Aclarar que el estado es el conjunto de valores que usa la aplicaci√≥n para reaccionar como lo hace en un momento determinado, por ejemplo, si estamos mostrando una lista de¬†posts de blog, esta lista es parte del estado, la p√°gina que estamos viendo es otra parte del estado.¬†const state = { counter: 0, blog: { posts: [] currentPage: 0 } } El paradigma subyacente de este patr√≥n es que, en una SPA (Single page application) los componentes acceden y manipulan datos que son usados en otros componentes y que cualquier cambio en un valor debe¬†trasladarse de forma sencilla a todos los componentes que lo necesitan, por ello el estado debe ser la √∫nica fuente de verdad.
Adem√°s, hay una estricta regla que debe cumplirse: los componentes no pueden modificar el valor de estado directamente. Para garantizar que un cambio de estado se traslada a todos los componentes, cualquier cambio se debe hacer mediante¬†mutations, que explico un poco m√°s abajo en este mismo post.
Getters En ocasiones necesitamos manipular el resultado de un estado, por ejemplo, paginar los resultados o filtrar por t√≠tulo, normalmente esto lo realizar√≠amos en el componente, pero si queremos reusar esa funci√≥n podemos usar los getters, que no son m√°s que datos derivados el estado actual.
getters: { postsActive: state =&gt; { return state.blog.posts.filter(post =&gt; post.active) } } Mutaciones / Mutations Como deciamos, la √∫nica forma de alterar el estado es usando mutaciones, que son algo parecido a lanzar un evento. Cada mutaci√≥n tiene un tipo (type)¬†y un¬†manejador (handler).
El_handler_ es una funci√≥n que recibe como primer argumento el estado actual, y puede recibir opcionalmente un¬†payload¬†como segundo parametro.
mutations: { nextPage (state) { // mutate state state.blog.currentPage++ }, prevPage (state, payload) { state.blog.currentPage = state.blog.currentPage - payload.pages } } Un¬†mutation¬†nunca debe ser llamado directamente, la forma de hacerlo es usar¬†store.commit(&lsquo;mutation type&rsquo;), por ejemplo:
store.commit(&#39;nextPage&#39;) Las mutaciones siguen las reglas de reactividad de¬†vue.js, esto es muy importante, por que cuando el valor del estado sea alterado mediante mutaciones, cualquier componente que est√©¬†observando el estado, recibir√° el nuevo valor.
Algo que debemos tener en cuenta y que es MUY IMPORTANTE, **las mutaciones deben ser s√≠ncronas.¬†**Supongamos que tenemos un¬†mutation¬†que al ser lanzado hace una llamada as√≠ncrona para actualizar los post del blog y otro que simplemente elimina uno de la lista. Al ser el primero as√≠ncrono no podemos garantizar el orden real en el que se alterar√° el estado.
Esto lo veremos en detalle en el siguente post y la forma en la que resolver la necesidad de hacer llamadas as√≠ncronas.
`,url:"https://sergiocarracedo.es/2017/04/03/vuex-el-redux-de-vuejs-i/",image:"/2017/04/03/vuex-el-redux-de-vuejs-i/pexels-photo-99541_0_hu_570d567e86a126d0.jpeg",tags:[],readingTime:"3 minutes read",date:"Apr 3, 2017"},"https://sergiocarracedo.es/2017/03/15/introduccion-vuejs/":{title:"Introducci√≥n a Vue.js",content:`Desde hace unos a√±os han aparecido multitud de librer√≠as y¬†frameworks javascript para el desarrollo de aplicaciones frontend, facilitado por el aumento de rendimiento y velocidad de javascript en los navegadores modernos.
Existe una variedad, que yo definiria como excesiva, de frameworks JS.¬†Entre los m√°s famosos o extendidos estan Angular.js, Angular2, React.js, Ember.js, y tambi√©n el objeto de este post:¬†Vue.js.
Vue.js¬†se define a si mismo como &ldquo;Intuitivo, r√°pido y por componentes MVVM para construir interfaces interactivas&rdquo; (pod√©is saber m√°s sobre el patr√≥n Model-View-Viewmodel aqu√≠:¬†https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93viewmodel)
Bajo mi punto de vista las ventajas de¬†Vue.js¬†son,
Es muy¬†sencillo de aprender. El c√≥digo es sencillo de entender y de mantener. El sistema de compomentes es muy sencillo de entender y crear componentes es casi trivial. El sistema de data binding. La velocidad de ejecuci√≥n (prometen ser m√°s r√°pidos que React.js y Angular) Si has trabajado con React.js,¬†Vue.js¬†te parecera conceptualmente similar (por ejemplo tambi√©n usa Virtual DOM), ellos mismos lo explican aqu√≠¬†https://vuejs.org/v2/guide/comparison.html
Vue.js¬†esta enfocado en el core. Por defecto no aporta¬†_routing¬†_ni otras funcionalidades, pero estas pueden realizarse mediante plugins, de hecho¬†**Vue.js¬†**tiene dos plugins oficiales¬†vue-router, y¬†vuex¬†un port de¬†Redux para¬†Vue.js.
Vamos a ver un ejemplo muy simple de Vue.js
&lt;!DOCTYPE html&gt; &lt;html lang=&#34;es&#34;&gt; &lt;head&gt; &lt;meta charset=&#34;UTF-8&#34;&gt; &lt;title&gt;Introduci√≥n a Vue.js&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&#34;app&#34;&gt;¬†&lt;input type=&#34;text&#34; placeholder=&#34;Escribe tu nombre&#34; v-model=&#34;name&#34; /&gt; &lt;span&gt;Has escrito {{ name }}&lt;/span&gt; &lt;/div&gt; &lt;script src=&#34;https://cdnjs.cloudflare.com/ajax/libs/vue/2.0.3/vue.js&#34;&gt;&lt;/script&gt; &lt;script&gt; vm = new Vue({ el: &#39;#app&#39;, data() { return { name: &#39;&#39;, } }, }); &lt;/script&gt; &lt;/body&gt; &lt;/html&gt;¬†Puedes¬†ver el ejemplo funcionando en¬†https://jsfiddle.net/sergio_carracedo/0d2ymgft/
Lo que vemos aqu√≠, es un ejemplo b√°sico del data binding, al crear la instancia principal de¬†Vue¬†le pasamos un objeto plano, con dos elementos:
el: es el punto de montaje de¬†Vue, normalmente es un div con el id¬†#app data: debe ser una funci√≥n que devuelva las variables que se van a¬†_bindear**,¬†**_en nuestro ejemplo¬†name En el¬†input¬†vemos el¬†atributo¬†v-model¬†que indica a¬†Vue¬†el nombre de la variable que se¬†bindear√°, funcionando en ambos sentidos, si modificamos en cualquier lugar el valor del¬†name¬†se modificar√° en el input (y en todos los lugares donde se referencie) y si modificamos el valor del input el valor de¬†name¬†se modificar√° en todos los lugares, por ejemplo en el {{ name }} Como ves en unas pocas y sencillas l√≠neas hemos puesto a funcionar¬†Vue.js, obviamente este es un ejemplo casi trivial, y¬†Vue.js¬†es mucho m√°s que esto,¬†se pueden crear aplicaciones complejas con facilidad y en poco tiempo.
Estos dos videos que dejo a continuaci√≥n explican mejor que como funciona de Vue.js¬†en 60 minutos. En el segundo explica como crear un¬†CRUD¬†usando¬†Vue.js¬†a partir de una API (creada en ese caso en¬†Slim)
`,url:"https://sergiocarracedo.es/2017/03/15/introduccion-vuejs/",image:"/2017/03/15/introduccion-vuejs/pexels-photo-196160_hu_968505edb4d3fa96.jpeg",tags:[],readingTime:"3 minutes read",date:"Mar 15, 2017"},"https://sergiocarracedo.es/2017/03/07/entidad-con-propietario-en-laravel-54/":{title:"Entidad con propietario en Laravel 5.4",content:`Recientemente he iniciado un proyecto con Laravel 5.4, en el que algunos de mis modelos tienen un propietario, es decir, los crea un usuario y s√≥lo el tiene acceso para ver o editar los datos de su entidad.
La forma m√°s simple que he¬†encontrado es hacer uso de las posiblilidades que Laravel nos ofrece mediante los¬†middleware.
En primer lugar extendemos el Model que nos provee Elocuent
&lt;?php namespace App; use Illuminate\\Database\\Eloquent\\Model; class OwnableModel extends Model { public function getOwnerId() { return isset($this-&gt;user_id) ? $this-&gt;user_id : null; } public function checkIsOwner($user_id) { return $this-&gt;getOwnerId() == $user_id; } } Esta clase tiene dos m√©todos:¬†getOwnerID()¬†que se encarga de devolver el id del propietario de la entidad (por defecto usa el campo user_id)
checkIsOwner($user_id) devuelve TRUE si el user_id el el propietario de la entidad
Ahora el modelo concreto que queramos controlar debe extender de¬†OwnableModel¬†en lugar de¬†Model.
Los dos m√©todos mencionados podremos sobreescribirlos para adaptarlos a nuestras necesidades.
Ahora tenemos que crear el¬†middleware, en la carpeta app/Http/Middleware creamos el archivo¬†AbortIfNotOwner.php¬†con el siguente contenido.
&lt;?php namespace App\\Http\\Middleware; use Closure; use Illuminate\\Support\\Facades\\Auth; class AbortIfNotOwner { /** * Handle an incoming request. * * @param \\Illuminate\\Http\\Request $request * @param \\Closure $next * @return mixed */ public function handle($request, Closure $next) { $isOwner = TRUE; foreach ($request-&gt;route()-&gt;parameters() as $model) { if ($model instanceof \\App\\OwnableModel &amp;&amp; !$model-&gt;checkIsOwner(Auth::id())) { $isOwner = FALSE; } } if (!$isOwner) { return response(&#39;Unauthorized.&#39;, 401); } return $next($request); } } Este middleware debemos a√±adirlo a las rutas que deseemos comprobar que el due√±o de la entidad es el que accede a ella.
Route::get(&#39;/ruta/{entity}/&#39;, [ &#39;uses&#39; =&gt; &#39;RutaController@index&#39;, ])-&gt;middleware(&#39;owner&#39;); Con estos sencillos pasos tenemos protegida la entidad de forma que si un usuario intenta acceder a una entidadad de las que no es propietario recibir√° un error HTTP 401.
`,url:"https://sergiocarracedo.es/2017/03/07/entidad-con-propietario-en-laravel-54/",image:"/2017/03/07/entidad-con-propietario-en-laravel-54/pexels-photo-132907_0_hu_845205c7592172f0.jpeg",tags:[],readingTime:"2 minutes read",date:"Mar 7, 2017"},"https://sergiocarracedo.es/2017/03/03/algo-se-mueve-en-vigo/":{title:"Algo se mueve en Vigo",content:`Como algunos sabreis hace poco m√°s de un a√±o F√©lix G√≥mez y yo creamos la comunidad de desarrolladores¬†PHPVigo.¬†El objectivo, mediante eventos mensuales, es conocer a otros desarrolladores, compartir conocimiento mediante charlas y principalmente crear comunidad, una comunidad que unida pueda demostrar que desde Vigo, desde Galicia, hay personas y empresas¬†desarrollando proyectos y t√©cnolog√≠as que nada tiene que envidiar a otras partes del mundo, como ha sucedido recientemente con Rub√©n Gonz√°lez que a ra√≠z de preparar una charla para PHPVigo, descubri√≥ como reducir hasta un 30% el uso de memoria de composer.
Desde luego¬†**PHPVigo¬†**no es la primera comunidad en Vigo, mucho antes llegaron Python Vigo,¬†GDG Vigo,¬†Vigo Labs¬†y alguna que seguro me dejo en el tintero.
Lo mejor de todo, es que aunque puedan parecer comunidades desconectadas, que tratan sobre lenguajes de programaci√≥n distintos o tecnolog√≠as diferentes, es todo lo contrario, ha surgido una gran simbiosis entre estas comunidades y en los √∫ltimos 6¬†meses han aparecido multitud de nuevas comunidades: Javascript Vigo, Agile Vigo, VigoJUG, SysAdminGalicia, etc (perdonadme si me dejo alguna, que seguro que lo hago) lo que, bajo mi punto de vista, no es m√°s que la constataci√≥n de que Vigo se mueve, tiene ganas de hacer cosas y adem√°s cosas muy chulas.
Para mi personalmente es un orgullo y un placer impulsar una de estas comunidades y tambien para mi compa√±ero dise√±ador **Pedro Figueras¬†** colaborar en todo lo posible, como por ejemplo aportando la identidad visual de PHPVigo
Espero y deseo que la tracci√≥n que las comunidades estan adquiriendo continue y de m√°s frutos, que los dar√°.
`,url:"https://sergiocarracedo.es/2017/03/03/algo-se-mueve-en-vigo/",image:"/2017/03/03/algo-se-mueve-en-vigo/pexels-photo-209728_hu_8f0772e6dd07e008.jpeg",tags:[],readingTime:"2 minutes read",date:"Mar 3, 2017"},"https://sergiocarracedo.es/2016/12/28/backup-de-archivos-que-no-estan-en-un-repo-git/":{title:"Backup de archivos que no estan en un repo GIT",content:`Recientemente me surgi√≥ la necesidad de optimizar el espacio requerido para los backups¬†de mi equipo de trabajo. Principalmente son sitios web y actualmente volcaba todas las carperas de proyectos a otro disco y a Dropbox, pero Dropbox tiene un grave problema de rendimiento indexando gran cantidad de archivos.
Analizando el proceso de backup y hablando con otros programadores sobre su forma de hacer backups, me di cuenta que una parte del problema ya la tenia resuelta de antemano: GIT.¬†Ya estoy usando git en los proyectos, y esto quiere decir que todo el contenido del repo ya esta salvaguardado en el servidor (Bitbucket o Github en mi caso) y en mis otros equipos de trabajo, por lo que s√≥lo me quedaba resolver la otra parte del problema: los archivos que no estan en el repo, por ejemplo, en el caso de los¬†Drupales¬†la carpeta files, en la¬†que estan los archivos subidos desde el backend y que no tiene sentido que esten en el repo.
Pues, despues de haber presenciado la charla de¬†F√©lix G√≥mez¬†sobre creaci√≥n de apps de linea de comandos en PHP (https://www.youtube.com/watch?v=mGNgT6y_8NY), me lanc√©¬†a crear la mia para hacer esta tarea.
La app por el momento es muy sencilla. Dado un directorio de partida y un directorio de destino, busca en el directorio de partida todas las carpetas que contengan un repo git y consulta todos los archivos que estan en el repo, esa lista de archivos se almacena en un archivo de texto temporal que posteriormente se pasa como parametro de un¬†rsync¬†en la opcion &ndash;exclude-list¬†de forma que rsync no realize la copia de esos archivos.
Tambien he a√±adido la opci√≥n¬†&ndash;cvs-exclude para que rsync ignore los archivos propios de git, como por ejemplo la carpeta .git
Con esta sencilla herramienta que he subido a un repo publico¬†https://github.com/sergiocarracedo/backup-tools¬†por si alguien quiere probar o aportar sus ideas, tengo resuelto el problema de los backups.
Como¬†to-dos de esta herramienta, esta el optimizar ese exclude list ya que puede generarse un archivo de varios megas, el crear un segundo comando para consultar que repos tienen cambios sin comitear, etc&hellip;.
`,url:"https://sergiocarracedo.es/2016/12/28/backup-de-archivos-que-no-estan-en-un-repo-git/",image:"/2016/12/28/backup-de-archivos-que-no-estan-en-un-repo-git/pexels-photo-117729_hu_6e322eeae90c058.jpeg",tags:[],readingTime:"2 minutes read",date:"Dec 28, 2016"},"https://sergiocarracedo.es/2016/09/12/modulo-drupal-7-cuenta-atras-lanzamiento-website/":{title:"M√≥dulo Drupal 7: Cuenta atras lanzamiento website",content:`En m√°s de una ocasi√≥n nos ha sucedido que una vez finalizado el trabajo en un sitio web, el cliente solicita que este no se publique de forma instantanea, si no que aparezca una p√°gina en la que se muestre una cuenta atr√°s y que una vez esta cuenta llege a 0 el site este online.¬†Tradicionalmente esto lo realizabamos de forma manual, pero tiene bastantes desventajas, como tener que estar pendiente de hacer la publicaci√≥n del sitio. Por ello decidimos crear un m√≥dulo para Drupal 7 que permitiese hacer esta tarea de forma autom√°tica y que adem√°s fuese facil de gestionar por el cliente¬†llegado el caso de que necesitase ajustar la fecha l√≠mite de la cuenta atr√°s.
Llamamos a este m√≥dulo¬†Site publish countdown¬†y actualmente esta en el sandbox de m√≥dulos de Drupal¬†https://www.drupal.org/sandbox/sergiocarracedo/2718591
Este m√≥dulo, una vez instalado y activado, permite al administrador del sitio web establecer la fecha de lanzamiento antes de la cual, redirigir√° todas las visitas an√≥nimas (a cualquier url) a la p√°gina de cuentra atr√°s.¬†El adminsitrador o administadores el sito podr√°n loguearse de forma habitual, y estos podran ver el sitio web con normalidad para poder realizar tareas antes del lanzamiento de la web.
Una vez agotada la cuenta a atr√°s la propia p√°gina se recargar√° mostrando ya la versi√≥n¬†normal¬†del sitio.
Este m√≥dulo permite a los desarrolladores maquetar y personalizar la p√°gina de cuenta atr√°s, solo es necesario a√±adir el fichero¬†sitepublishcountdown.tpl.php¬†en la carpeta del tema usado por la web (y limpiar el theme registy). El √∫nico contenido obligatorio de este fichero es la siguente linea:
&lt;time datetime=&#34;&lt;?php print $datetime; ?&gt;&#34;&gt;&lt;?php print $datetime; ?&gt;&lt;/time&gt; que es la que se encarga de mostrar la cuenta atr√°s. El resto queda a libre disposici√≥n del maquetador del site para que pueda personalizarla.
`,url:"https://sergiocarracedo.es/2016/09/12/modulo-drupal-7-cuenta-atras-lanzamiento-website/",image:"/2016/09/12/modulo-drupal-7-cuenta-atras-lanzamiento-website/screenshot_from_2016-12-28_12-18-18_hu_a48fc972effc7b13.png",tags:[],readingTime:"2 minutes read",date:"Sep 12, 2016"},"https://sergiocarracedo.es/2016/03/01/circuitos-de-formula-1-2016-en-google-maps/":{title:"Circuitos de Formula 1 (2016) en Google Maps",content:`Como fan de¬†la Formula 1 que soy, simpre me ha gustado conocer y ver los circuitos m√°s all√° de lo que las televisiones nos ofrecen, que es una visi√≥n muy centrada en el circuito.
Gracias a Google Maps podemos llegar a cada uno de los circuitos del Mundial,¬†ver su entorno y en algunos casos el Street View dentro de los propios ciruitos.
Me llaman la atenci√≥n algunos circuitos, como por ejemplo Albert Park, un circuito semi-urbano que se disputa sobre las propias carreteras que atraviesan¬†el parque y que es completamente desmontado una vez finalizado¬†el evento de F1¬†y otras pruebas que se disputan en el. Si accedemos al¬†Street View¬†del circuito, cuesta en algunos tramos reconocer la pista¬†sin las vallas de seguridad.
En la mayoria de los circuitos podemos ver las modificaciones que han sufrido y las variantes que se han introducido. Como por ejemplo en circuito de Interlagos, en el¬†que podemos ver el circuito¬†completo que fue modificado al actual trazado en 1990, pero a√∫n se conserva el trazado original, aquel por el que Kimi Raikkonen decidi√≥¬†pasearse en 2012
Dejo aqui la lista completa de los circuitos de 2016:
Albert Park¬†(Australia) Sakhir¬†(Barhr√©in) Shanghai¬†(China) Sochi Circuit¬†(Rusia) Montmel√≥¬†(Espa√±a) M√≥naco¬†(Montecarlo) Gilles Villenuve (Montreal)¬†(Canad√°) Baku City Circuit (Azerbaiy√°n) Red Bull Ring¬†(Austria) Silverstone¬†(Gran Breta√±a) Hungaroring¬†(Hungria) Hockenheim (Alemania) Spa-Francorchamps¬†(B√©lgica) Monza¬†(Italia) Marina Bay¬†(Singapur) Sepang¬†(Malasia) Suzuka¬†(Jap√≥n) Circuit of the Americas¬†(USA) Hermanos Rodr√≠guez¬†(M√©xico) Interlagos¬†(Brasil) Yas Marina¬†(Abu Dhabi) `,url:"https://sergiocarracedo.es/2016/03/01/circuitos-de-formula-1-2016-en-google-maps/",image:"/2016/03/01/circuitos-de-formula-1-2016-en-google-maps/formula1_hu_d418643f23e0d673.jpeg",tags:[],readingTime:"2 minutes read",date:"Mar 1, 2016"},"https://sergiocarracedo.es/2015/10/29/despliegue-usando-deployer/":{title:"Despliegue usando Deployer",content:`Desde el momento que ponemos un sistema en producci√≥n y necesitamos mantener el c√≥digo o continuar el desarrollo surge una tediosa tarea, subir los cambios al servidor de producci√≥n. Esto normalmente lo hacemos empleando un FTP mediante el cual subimos los ficheros modificados. Pero ahi esta el &ldquo;problema&rdquo;, en muchos casos conocer los ficheros que se han modificado (aunque lo podemos hacer empleando git y comparando commits para obtener una lista de archivos cambiados) y una vez que los conocemos debemos subir a mano los ficheros modificados.¬†Esto es un proceso tedioso, adem√°s tenemos un riesgo alto de cometer un error sin una posiblidad sencilla¬†de volver atr√°s.¬†que en muchos proyectos no es aceptable.
Para resolver esto existen multiples automatizadores de despliegue, pero sobre el que vamos a hablar hoy es¬†Deployer
Este tipo de herramientas (y Deployer no es una excepci√≥n), emplea un repositiorio¬†git como fuente del c√≥digo (podemos usar github, bitbucket, etc), por lo que en primer lugar debemos garantizar que el servidor destino tiene acceso a esa repo, lo que normamente haremos usando un sistema de clave p√∫blica / privada que nos garantice acceso al repo.
Una vez que tenemos acceso al repo desde el servidor remoto, instalamos Deployer en nuestro equipo local y configuramos los parametros de conexi√≥n al servidor en el fichero deploy.php
Es imporante indicar que es posible establecer varios entornos, por si por ejemplo tenemos un enterono de desarrollo, o de stage antes de enviar a produci√≥n
Una vez configurado¬†Deployer simplemente ejecutariamos¬†dep deploy prod y con esto deployer se encargar√° de poner en producci√≥n las modificaciones que hubiesemos efectuado en el repo en unos instantes, y por defecto nos almacenar√° 3 displiegues por lo que si hay cualquier problema al desplejar, por ejemplo un error en el c√≥digo, que se nos ha pasado por alto o algo que no funciona en el servidor con un simple
dep rollback prod Volveremos al estado anterior, todo ello en unos segundos
Deployer funciona basado en un sistema de recetas, de hehco el propio¬†deploy¬†es una receta, esto nos permite, por ejemplo crear una receta que cree en el servidor de producci√±on el archivo de configuracion de nuestro CMS, con las credenciasles de BBDD y otras configuraciones especificas del servidor, ya que doy por hehco que ese archivo no va a estar en tu repo git por razones obvias.
Tambien podemos indicar diractorios¬†shared, que ser√°n directorios, donde normalmente se almacena imagenes y recursos del usaurio, que no deberian estar en el repo, por ejemplo el diretior wp-uploads de¬†Wordpress¬†o¬†files¬†en Drupal de forma que se linkeen en cada despliegue sin necesidad de copiarlo.¬†En otra entrada de post explicar√© como crear una receta, por ejemplo para Drupal 7, que nos permita configurar el¬†settings.php¬†o descargar a local el directorio¬†files.
Si quereis m√°s informaci√≥n sobre Deployer y como ponerlo en marcha podeis consultar su documentaci√≥n
`,url:"https://sergiocarracedo.es/2015/10/29/despliegue-usando-deployer/",image:"/2015/10/29/despliegue-usando-deployer/deployer_hu_40b8a9ce8be4c6e5.png",tags:[],readingTime:"3 minutes read",date:"Oct 29, 2015"},"https://sergiocarracedo.es/2015/02/05/sirve-tu-pagina-404-como-estatica-en-drupal/":{title:"Sirve tu p√°gina 404 como est√°tica en Drupal",content:`En la configuraci√≥n¬†est√°ndar¬†de Drupal este trata de¬†absorber¬†todo el tr√°fico que se genera contra el directorio (y subdirectorios) donde esta instalado en el servidor, esto incluye¬†tambi√©n¬†a los errores 404.¬†Cualquier¬†solicitud¬†de un fichero que no exista Drupal la gestionar√° en un nuevo dominio esto no es un problema ya que, en teor√≠a, la¬†mayor√≠a¬†de los errores 404 se deber√°n a errores de escritura en las rutas por parte de los usuarios, pero en un sitio con alto tr√°fico que acabamos de¬†remodelar¬†y en el que no¬†est√°n¬†correctamente creadas las redirecciones desde la antiguas rutas a la nuevas podemos encontrarnos con que¬†estamos¬†sirviendo muchas p√°ginas 404.
Por desgracia,si estamos usando Boost¬†este no cachea las p√°ginas 404, por lo tanto Drupal tiene que ejecutar un proceso &lsquo;completo para devolver&rsquo; la p√°gina (recuerda que si estamos usando Boost no podemos hacer uso del cacheado interno de Drupal) y por poco que esto pueda parecer esto supone un uso¬†est√©ril¬†de recursos del servidor.
El m√≥dulo Static 404, pone fin a este problema. Una vez instalado navegamos hasta Configuraci√≥n &gt; Sistema &gt; Informaci√≥n del sitio¬†y en la secci√≥n P√°ginas de error, pulsamos el bot√≥n Generate static 404 Page y a partir de ese momento Drupal devolver√° una p√°gina est√°tica para cualquier error 404. Aunque parezca una nimiedad, hemos realizado pruebas de stress en nuestro servidor local y hemos reducido el tiempo de respuesta un 80%, lo que hace que en websites con alto tr√°fico marque una diferencia y nos permita dedicar esos recursos liberados a lo que realmente importa.
`,url:"https://sergiocarracedo.es/2015/02/05/sirve-tu-pagina-404-como-estatica-en-drupal/",image:"/2015/02/05/sirve-tu-pagina-404-como-estatica-en-drupal/pexels-photo-1579062_hu_bdc508507ef7e4f6.jpeg",tags:[],readingTime:"2 minutes read",date:"Feb 5, 2015"},"https://sergiocarracedo.es/2013/01/21/el-uso-de-como-comodin-para-http-y-https/":{title:"El uso de &#039;//&#039; como comod√≠n para &#039;http://&#039; y &#039;https://&#039;",content:`Crear una web que pueda ser navegable mediante http y https normalmente supone tener alg√∫n tipo de variable o modificador en el lenguaje que genere la salida HTML del website, por ejemplo PHP para que las peticiones de los recursos, como CSS, JS, im√°genes, etc se realicen mediante el protocolo correcto.
Por ejemplo si estamos navegando por una web en http:// (modo no encriptado) y pedimos una imagen mediante https://www.misitio.com/imagen.png¬†en principio puede parecer que no¬†deber√≠amos¬†tener ning√∫n problema, pero si la red del visitante filtra el puerto 443 (https) cosa que sucede en algunas empresas (cada vez menos) no podr√≠a acceder al recurso, si el recurso que solicitamos (que no coincide en protocolo) necesita hacer uso de alguna cookie, dependiendo de como este configurada esta podr√≠a no poder¬†acceder¬†a ella.
Pero el problema m√°s evidente es cuando el¬†usuario¬†esta navegando por nuestra web usando https¬†y la p√°gina solicita un recurso mediante http, da igual lo buena, caro y bonito que sea nuestro certificado, el navegador mostrar√° una advertencia que pueda hacer que los¬†usuarios¬†desconf√≠en¬†de nuestro website.
Para evitar estos problemas, podemos o bien hacer uso de una variable o similar que modifique el protocolo en nuestra programaci√≥n o podemos optar por una soluci√≥n mucho m√°s sencilla que nos cubre en los dos casos: hacer uso de &lsquo;//&rsquo; en lugar de http:// o https://, si has¬†le√≠do¬†bien, solo usar // Por ejemplo, http://www.misitio.com/miimagen.jpg se¬†convertir√≠a¬†en //www.misitio.com/miimagen.jpg Ahora es el navegador el que se encarga de decidir el protocolo que usa en funci√≥n de como hemos pedido el documento HTML que va a solicitar esos recursos.
Sobra decir que si por el motivo que sea¬†necesitamos¬†forzar uno de los protocolos, simplemente especificamos el protocolo como hemos hecho hasta el momento. Puedes encontrar m√°s informaci√≥n sobre el uso de SSL en la web en¬†http://support.google.com/adwords/answer/2580401/?hl=es
`,url:"https://sergiocarracedo.es/2013/01/21/el-uso-de-como-comodin-para-http-y-https/",image:"/2013/01/21/el-uso-de-como-comodin-para-http-y-https/el_uso_de_039039_como_comodin_para_039http039_y_039https039_hu_2d0b394a4351c0c7.jpg",tags:[],readingTime:"2 minutes read",date:"Jan 21, 2013"},"https://sergiocarracedo.es/2013/01/07/visualiza-graficamente-tus-expresiones-regulares-regexpercom/":{title:"Visualiza gr√°ficamente tus expresiones regulares: regexper.com",content:`Las expresiones regulares¬†en cualquier lenguaje de programaci√≥n son muy √∫tiles y potentes,¬†ahorr√°ndonos¬†escribir muchas lineas de c√≥digo. Si la expresi√≥n es sencilla, nos va a ser¬†f√°cil¬†como humanos comprenderla de un vistazo, pero si la expresi√≥n se complica, si no la hemos creado nosotros, o si ya ha pasado tiempo desde que lo hemos hecho, puede ser un poco m√°s complicado entender lo que hace o para que nos sirve.
Hace unos¬†d√≠as¬†descubr√≠ en twitter a trav√©s de un retweet¬†a un¬†usuario¬†que merece la pena seguir @patxangas¬†y en uno de sus tweets nos descubira la herramienta¬†www.regexper.com¬†que a partir de una¬†expresi√≥n¬†regular genera un grafo como el que veis en la imagen que acompa√±a a esta entrada. El ejemplo empleado es
/(ftp|http|https):\\/\\/(\\w :{0,1}\\w*@)?(\\S )(:[0-9] )?(\\/|\\/([\\w#!:.? =&amp;amp;%@!\\-\\/]))?/ Como ves en la imagen, es muy f√°cil entender lo que hace esta¬†expresi√≥n¬†regular. Ahora puedes probar las tuyas!
`,url:"https://sergiocarracedo.es/2013/01/07/visualiza-graficamente-tus-expresiones-regulares-regexpercom/",image:"/2013/01/07/visualiza-graficamente-tus-expresiones-regulares-regexpercom/visualiza_graficamente_tus_expresiones_regulares_regexper.com_hu_728301c979f0445d.jpg",tags:[],readingTime:"1 minute read",date:"Jan 7, 2013"},"https://sergiocarracedo.es/2012/11/29/bigdump-importa-grandes-ficheros-mysql-sin-problemas/":{title:"Bigdump: Importa grandes ficheros mysql sin problemas",content:`Una de las pesadillas de los desarrolladores web (a menos es una de las m√≠as) es encontrase con la necesidad de subir un volcado SQL muy grande a un servidor y no tener acceso remoto al servidor mysql y, tampoco, tener acceso via shell, es decir la &ldquo;√∫nica&rdquo;¬†v√≠a¬†es usar PhpMyAdmin.
He puesto √∫_nica_ entre comillas por que desde luego hay muchas m√°s alternativas, algunas muy manuales, o algunas m√°s c√≥modas como la que os propongo: **Bigdump¬†**
Bigdump es un peque√±o script php que una vez configurado con los datos de nuestra base de datos se encarga de trocear los archivos SQL que subamos para insertarlos &ldquo;poco a poco&rdquo; en la base de datos, incluso en servidores con limitaciones como safe_mode.
Su uso es muy sencillo:
En el fichero descargado¬†bigdump.php¬†configuramos los datos de conexi√≥n a la base de datos, es importante no olvidar modificar¬†tambi√©n¬†la codificaci√≥n para evitarnos problemas con los caracteres ya que por defecto usar√°¬†latin1
Subimos bigdump.php al servidor donde necesitamos hacer la importaci√≥n &ldquo;problematica&rdquo;
Subimos al mismo directorio el fichero .sql a importar (podemos subirlo comprimido con gz). Si el directorio tiene permisos de escritura nos ofrecer√° la posibilidad de subir el fichero directamente desde un formulario, aunque no recomiendo esta opci√≥n ya que estamos hablando siempre de ficheros muy grandes
Ejecutamos el fichero bigdump.php desde el navegador, por ejemplo http://www.tuweb.com/bigdump.php
Escogemos el fichero a importar, y comienza la importaci√≥n del mismo sin tener que preocuparnos de nada m√°s, eso si no podremos cerrar la ventana del navegador mientras se esta ejecutando la impotaci√≥n
IMPORTANT√çSIMO!!! Por razones de seguridad obvias eliminar el fichero bigdump.php¬†y los .sql¬†o .gz¬†que hemos subido al terminar.
Descargar¬†BigDump ver. 0.34b (beta)¬†directamente de la web de su creador
`,url:"https://sergiocarracedo.es/2012/11/29/bigdump-importa-grandes-ficheros-mysql-sin-problemas/",image:"/2012/11/29/bigdump-importa-grandes-ficheros-mysql-sin-problemas/pexels-photo-129544_hu_4d88d36ca58b724d.jpeg",tags:[],readingTime:"2 minutes read",date:"Nov 29, 2012"},"https://sergiocarracedo.es/2012/05/24/cuentas-tuteladas-para-menores-en-las-redes-sociales/":{title:"¬øCuentas tuteladas para menores en las redes sociales?",content:`Las redes sociales cada d√≠a¬†est√°n¬†m√°s presentes y ya forman parte de la vida de muchos de nosotros. Sin llegar a ser una dependencia, son una herramienta que nos permite estar en contacto con nuestros amigos y conocidos, como antes lo fueron las postales, las cartas, y el tel√©fono fijo por ejemplo.
La edad m√≠nima para acceder a una red social son los 14 a√±os (al menos para Facebook y Tuenti en Espa√±a), pero es sabido por todos que los menores intentan acceder a estas redes mucho antes, obligando a los administradores de las redes a eliminar muchas cuentas creadas por menores de 14 a√±os.
Todos conocemos alg√∫n ejemplo de primo, sobrino, etc que alguna vez a intentado agregarnos como amigo en Facebook y desde luego es una¬†conducta¬†&lsquo;reprobable&rsquo; (y denunciable ante las propias redes) que recomendar√≠amos poner en conocimiento de sus padres.
Pero ¬øy si existiese un tipo de cuenta tutelada en las redes sociales?
Esta cuenta la¬†crear√≠an¬†los padres y¬†tendr√≠an¬†el control de toda actividad y acciones importantes de la cuenta, por ejemplo. Si alguien trata de agregar como amigo a tu hijo, lo razonable parece que tu decidas si le permites aceptar esa petici√≥n, como sucede en la vida real, en la que los padres valoran las amistades y no¬†permitir√≠an, por ejemplo que un ni√±o sea &ldquo;amigo&rdquo; de un adulto desconocido.
Tambi√©n¬†tendr√≠an¬†los padres que autorizar la im√°genes que se suben a estas cuentas tuteladas, etc. Estas cuentas¬†servir√≠an¬†para que los ni√±os puedan estar en contacto con sus¬†t√≠os, primos, abuelos, etc. pero siempre con la supervisi√≥n de un adulto, tal y como sucede en la vida real.
Las redes sociales, por lo general, no tratan m√°s que modelar la vida real, crear una replica digital de las relaciones reales.
Este tipo de cuentas¬†permitir√≠an¬†a los menores ir aprendiendo a emplear las redes sociales y a conocer sus peligros ayudados y guiados siempre por sus padres.
Un buen¬†s√≠mil¬†ser√≠a, el ense√±ar a un ni√±o a cruzar la calle. primero le acompa√±a su padre/madre y le va explicando, que tiene que cruzar cuando &lsquo;el mu√±equito esta en verde&rsquo;, que tiene que mirar a los lados, no correr, etc&hellip; y¬†seg√∫n¬†el ni√±o lo va asimilando podemos ir¬†cedi√©ndole la confianza necesaria para que lo haga solo y con la seguridad que da conocer los riesgos.
`,url:"https://sergiocarracedo.es/2012/05/24/cuentas-tuteladas-para-menores-en-las-redes-sociales/",image:"/2012/05/24/cuentas-tuteladas-para-menores-en-las-redes-sociales/pexels-photo-2577939_hu_dae947ab2d5d73e1.jpeg",tags:[],readingTime:"2 minutes read",date:"May 24, 2012"},"https://sergiocarracedo.es/2012/05/09/que-estabas-haciendo-hace-un-ano-las-redes-sociales-te-lo-recuerdan/":{title:"¬øQu√© estabas haciendo hace un a√±o? Las redes sociales te lo recuerdan",content:`Las redes sociales que usamos habitualmente: Facebook, Twitter, Foursquare, instagram, etc. son, al fin y al cabo, una especie de diario de nosotros mismos, de lo que hacemos, de donde hemos estado, las fotos que hemos sacado, etc&hellip; Timehop¬†es una herramienta, que una vez logueados en las distintas redes, nos enviar√° un email diariamente (o podemos acceder a su web para verlo) con un resumen de las entradas que hemos publicado en las redes sociales justo hace un a√±o. Muy interesante para rememorar hecho pasados.
Por otra parte tenemos¬†Memolane, mucho m√°s potente que Timehop, en ella podremos a√±adir muchas m√°s redes, adem√°s de las citadas anteriormente, Picassa, Paginas de Facebook, fickr, google+, youtube, etc&hellip;¬†El contenido proveniente de esas redes se mostrar√° en una linea temporal, como se muestra en la imagen, y por la cual podremos viajar hasta una fecha espec√≠fica y recordar lo que &lsquo;hicimos&rsquo; ese dia.
`,url:"https://sergiocarracedo.es/2012/05/09/que-estabas-haciendo-hace-un-ano-las-redes-sociales-te-lo-recuerdan/",image:"/2012/05/09/que-estabas-haciendo-hace-un-ano-las-redes-sociales-te-lo-recuerdan/pexels-photo-273011_hu_4a4721e7e73d16c0.jpeg",tags:[],readingTime:"1 minute read",date:"May 9, 2012"},"https://sergiocarracedo.es/2012/04/17/como-descargar-un-sitio-web-completo-tu-equipo-con-wget/":{title:"Como descargar un sitio web completo a tu equipo con wget",content:`Hace unos d√≠as me surgi√≥ la necesidad de descargar un sitio web completo a mi disco duro como copia y referencia. Conoc√≠a la existencia de algunas herramientas como Httrack, una aplicaci√≥n multiplataforma (Windows, OSX, Linux) que nos permite descargar un sitio web completo al disco duro modificando incluso los enlaces para que sea perfectamente navegable sin conexi√≥n. Pero los usuarios de linux tenemos una alternativa m√°s simple pero igual de potente wget wget es un comando que nos permite descargar cualquier contenido en la red a nuestro equipo, normalmente se emplea para descargar ficheros sueltos: html, im√°genes, v√≠deos, etc&hellip; pero con los par√°metros adecuados puede descargar un sitio web completo
wget --mirror -p --convert-links -P ./DIRECTORIO-LOCAL URL-WEB-A-DESCARGAR Con este simple c√≥digo descargaremos el sito web completo a nuestro disco duro y los enlaces ser√°n reescritos para permitir la navegaci√≥n local M√°s informaci√≥n:
http://www.thegeekstuff.com/2009/09/the-ultimate-wget-download-guide-with-15-awesome-examples/ http://jamsubuntu.blogspot.com.es/2009/02/using-wget-to-download-entire-websites.html `,url:"https://sergiocarracedo.es/2012/04/17/como-descargar-un-sitio-web-completo-tu-equipo-con-wget/",image:"/2012/04/17/como-descargar-un-sitio-web-completo-tu-equipo-con-wget/pexels-photo-270456_hu_e329e335461e94f6.jpeg",tags:[],readingTime:"1 minute read",date:"Apr 17, 2012"},"https://sergiocarracedo.es/2012/04/10/disconectme-navega-de-forma-privada/":{title:"Disconect.me: Navega de forma privada",content:`Con la proliferaci√≥n de los plugins &lsquo;sociales&rsquo;, que est√°n cada d√≠a presentes en m√°s paginas, la navegaci√≥n privada comenzaba a ser una¬†utop√≠a. Por ejemplo, por el simple hecho de tener una sesi√≥n abierta (o no) en Facebook, cada vez que entramos en una web que dispone de un inocente¬†bot√≥n¬†&ldquo;Me gusta&rdquo;, podremos ser identificados.
Esto no quiere decir que se haga, pero la posibilidad esta ah√≠. La web que visitamos no sabe (o no tiene que saber quienes somos), pero el plugin social si que sabe quienes somos y que web estamos visitando, ya que tiene nuestra sesi√≥n en la red social y la p√°gina que pide mostrar el plugin.
Si usamos **Chrome, Firefox o Safari¬†**para navegar y nos gustar√≠a evitar que esto suceda estamos de suerte:
Creada por ex-trabajadores de Google, https://disconnect.me/¬†es un extensi√≥n para Chrome, Firefox y Safari¬†que seg√∫n sus propios creadores bloquea las peticiones a los servidores de Google, Facebook, Twitter, Yahoo, etc¬†evitando que nos puedan &lsquo;seguir&rsquo; en nuestra navegaci√≥n fuera de estos sitios. Tenemos la posibilidad de desactivar el bloqueo para la p√°gina que estamos visitando en ese momento, por si queremos hacer un &lsquo;me gusta&rsquo;, cosa que recordar√° en futuras visitas.
Como pronto lo que si hemos notado es una mayor velocidad de respuesta inicial de las p√°ginas con el plugin en modo bloqueo, ya que se limitan las peticiones, carga y ejecuci√≥n de javascript en estas p√°ginas.
`,url:"https://sergiocarracedo.es/2012/04/10/disconectme-navega-de-forma-privada/",image:"/2012/04/10/disconectme-navega-de-forma-privada/pexels-photo-906018_hu_37dcd60f07a2fc37.jpeg",tags:[],readingTime:"2 minutes read",date:"Apr 10, 2012"},"https://sergiocarracedo.es/2012/02/22/cookies-en-un-iframe-en-internet-explorer-p3p-policy/":{title:"Cookies en un IFRAME en Internet Explorer: p3p policy",content:`Si desarrollas una aplicaci√≥n PHP web que trabaja dentro de un IFRAME lo m√°s probable es que necesite usar una sesi√≥n y para almacenar el id de la sesi√≥n necesitar√°s una cookie¬†(de esto se encarga el propio PHP) pero deber√°s tener en cuenta algunas consideraciones para evitarte problemas de funcionamiento en Internet Explorer 6+
IE considera que el contenido del IFRAME es &ldquo;contenido de terceras partes&rdquo; y siguiendo¬†la configuraci√≥n por defecto de seguridad del navegador bloquear√° las cookies del IFRAME y por lo tanto perder√°s la sesi√≥n. Siempre existe la posibilidad de emplear el m√©todo de paso de sesi√≥n por URL, pero solo resolver√° el problema respecto a PHP si usas otro tipo de herramientas que necesiten sesiones (Facebook, Twitter, etc) estas no tendr√°n acceso a su cookie.
Como resolverlo Para resolver este problema, necesitamos que el IFRAME defina la pol√≠tica p3p, segun la wikipedia
La¬†Plataforma de Preferencias de Privacidad¬†(Platform for Privacy Preferences) o¬†P3P¬†es un¬†protocolo¬†que permite a los¬†sitios Web¬†declarar las intenciones de uso de la informaci√≥n recopilada sobre los usuarios que lo visitan y dar de esta forma un mayor control a √©stos sobre su informaci√≥n personal cuando navegan. P3P fue desarrollado por el¬†World Wide Web Consortium¬†(W3C) y se recomend√≥ oficialmente el¬†16 de abril¬†de¬†2002. La plataforma establece un formato estandar para declarar la identidad y las pr√°cticas sobre la informaci√≥n de los usuarios. Esta informaci√≥n puede ser interpretada por usuarios o por software dedicado a este prop√≥sito. Por tanto se pueden construir herramientas (agentes de usuario) que permiten al usuario especificar sus preferencias y √©ste software se encarga de comprobar autom√°ticamente si lo espec√≠ficado por el usuario se verifica en un website concreto. Dependiendo de las preferencias especificadas el agente puede por ejemplo mostrar un mensaje de alerta, generar una ventana para pedir instrucciones, permitir el acceso, rechazar el acceso&hellip; El proceso de comprobaci√≥n de las preferencias se debe llevar a cabo en una zona segura en la cual el servidor web debe recoger s√≥lo la m√≠nima informaci√≥n posible del cliente.
B√°sicamente nos obliga a indicar nuestras intenciones¬†para que el navegador se f√≠e de nosotros. La forma m√°s sencilla de implementarlo en PHP es mediante el¬†env√≠o¬†de cabeceras
header(&#39;P3P: CP=&#34;NOI ADM DEV COM NAV OUR STP&#34;&#39;); Tienes un listado completo del significado de estas directivas y de otras muchas en¬†http://www.p3pwriter.com/LRN_111.asp Esta cabecera m√°gica har√° que nuestras cookies funcionen correctamente. Este art√≠culo se ha basado en informaci√≥n¬†extra√≠da¬†de: http://stackoverflow.com/questions/389456/cookie-blocked-not-saved-in-iframe-in-internet-explorer http://blog.sweetxml.org/2007/10/minimal-p3p-compact-policy-suggestion.html
`,url:"https://sergiocarracedo.es/2012/02/22/cookies-en-un-iframe-en-internet-explorer-p3p-policy/",image:"/2012/02/22/cookies-en-un-iframe-en-internet-explorer-p3p-policy/cookies_en_un_iframe_en_internet_explorer_p3p_policy_hu_d9f0ef807ddb11f5.jpg",tags:[],readingTime:"2 minutes read",date:"Feb 22, 2012"},"https://sergiocarracedo.es/2012/02/15/cck-blocks-separa-los-campos-de-un-nodo/":{title:"CCK Blocks: Separa los campos de un nodo",content:`Hoy me gustar√≠a presentarte, si a√∫n no lo conoces, el m√≥dulo CCK Blocks. Un sencillo y √∫til m√≥dulo que nos permite mostrar los campos de un nodo en bloques, lo que nos da una mayor flexibilidad a la hora de maquetar y de mostrar la informaci√≥n.¬†Esta disponible para descargar en¬†http://drupal.org/project/cck_blocks, una vez instalado, accedemos a la¬†configuraci√≥n¬†el tipo de contenido que ya tengamos creado y editamos los campos que deseemos mostrar por separado.¬†En la p√°gina de edici√≥n tenemos una nueva opci√≥n:¬†Provide block for this field¬†que una vez activada y volviendo a la configuraci√≥n del tipo de contenido, esta vez en la pesta√±a Gestionar Presentaci√≥n¬†dispondremos de una nueva subpesta√±a llamada CCK Blocks¬†que nos permitir√° arrastrar los campos que queramos mostrar en bloque independiente. Una vez realizado estos pasos en la Estructura &gt; bloques¬†tendremos un nuevo bloque por cada campo disponible para mostrar en la regi√≥n que deseemos.
`,url:"https://sergiocarracedo.es/2012/02/15/cck-blocks-separa-los-campos-de-un-nodo/",image:"/2012/02/15/cck-blocks-separa-los-campos-de-un-nodo/cck_blocks_separa_los_campos_de_un_nodo_hu_fa8b14bef8257d77.jpg",tags:[],readingTime:"1 minute read",date:"Feb 15, 2012"},"https://sergiocarracedo.es/2011/12/13/backup-incremental-sobre-ftp-con-rsync-curlftpfs/":{title:"Backup incremental sobre FTP con rsync + curlftpfs",content:`Seguro que en alguna ocasi√≥n has necesitado realizar un backup de un sitio web al que solo tienes acceso mediante ftp. Lo habitual es emplear un cliente ftp tipo filezilla¬†o directamente trabajando en bash. El problema se plantea cuando, por ejemplo necesitamos realizar una copia incremental del sitio web y no tenemos acceso ssh sobre el que pueda trabajar **rsync¬†**o unison. La soluci√≥n nos la da curlftpfs¬†que nos permite montar un directorio ftp en nuestro sistema para tratarlo como si fuese una carpeta local y realizar un rsyn o unison de forma normal.
En primer lugar instalamos curlftpfs
$ sudo apt-get install curlftpfs A continuaci√≥n debemos editar nuestro /etc/fstab¬†con nuestro editor favorito
$ sudo nano /etc/fstab y le a√±adimos la siguiente linea curlftpfs#_user_:_pass_@_host.com_/_ruta-carpeta-remota_ /_punto-de-montaje_/ fuse ro,user,uid=_userid_,auto 0 0
Donde:
user: es el nombre de usuario ftp pass: el la contrase√±a de acceso al ftp host: es el host ftp userid: es el id del usuario no-root al que le permitiremos montar la carpeta, podemos obtener el nuestro mediante el comando $ id En este caso he montado la carpeta ftp¬†en modo solo lectura por razones de seguridad (nadie quiere borrar por accidente su sitio web ;) ) La carpeta se montara en el pr√≥ximo reinicio del sistema o podemos forzarlo mediante
$ sudo mount -a Lo siguiente realizar un rsync entre la carpeta montada y la carpeta donde almacenaremos el backup, por ejemplo:
$ rsync -arzvl /punto-de-montaje/* /home/usuario/backups/mi-ftp&lt;/span&gt; Solo nos quedaria crear un script y a√±adirlo a nuestro¬†crontab¬†para que se realice el backup automaticamente. Nota: La velocidad respecto a una sincronizaci√≥n de rsync sobre SSH es considerablemente inferior, por ello esta t√©cnica de considerarse una alternativa cuando no tenemos posibilididad de acceso ssh
`,url:"https://sergiocarracedo.es/2011/12/13/backup-incremental-sobre-ftp-con-rsync-curlftpfs/",image:"/2011/12/13/backup-incremental-sobre-ftp-con-rsync-curlftpfs/back-to-school-paper-colored-paper-stationery-159519_hu_927ec971e2caa0f4.jpeg",tags:[],readingTime:"2 minutes read",date:"Dec 13, 2011"},"https://sergiocarracedo.es/2011/09/22/reuniones-presenciales-vs-reuniones-online/":{title:"Reuniones presenciales vs reuniones online",content:`En los¬†√∫ltimos¬†a√±os se han puesto a nuestra disposici√≥n multitud de herramientas de trabajo para gestionar y realizar reuniones con clientes y colaboradores, y el crecimiento de la banda ancha y los dispositivos m√≥viles con conexi√≥n a internet hace que¬†todav√≠a¬†sea m√°s sencillo realizar una reuni√≥n en la que sus integrantes se encuentran en sitios dispares.
Como cualquier cambio social y cultural, lo nuevo no elimina a lo antiguo, lo desplaza y le deja los reductos donde lo nuevo no es tan efectivo, hay multitud de ejemplos de esto:
Cuando apareci√≥ de CD se¬†pens√≥¬†que iba a eliminar por completo a los vinilos 20 a√±os¬†despu√©s¬†se¬†siguen¬†convercializando¬†vinilos¬†y¬†dispositivos¬†reproductores. Es un mercado peque√±o pero existe.
Se¬†supon√≠a¬†que la televisi√≥n¬†matar√≠a¬†a la radio, pero a√∫n tiene una alta cuota de uso, sobre todo en lugares o actividades en las que la televisi√≥n no tiene cabida como ir en coche, caminar, trabajar, etc&hellip;
Reuniones presenciales Tienen la ventaja de la presencia personal, con lo que eso conlleva:¬†Cercan√≠a, poder leer el lenguaje corporal, poder establecer relaciones m√°s all√° de la propia reuni√≥n. La desventaja es que los participantes tiene que estar en un mismo lugar al mismo tiempo, algo que muchas veces es dif√≠cil de conseguir. Si tenemos que desplazarnos para una reuni√≥n de estas¬†caracter√≠sticas¬†tambi√©n¬†debemos considerar el tiempo de desplazamiento que dejamos de dedicar a otras tareas Es el formato de reuni√≥n m√°s recomendado para los comerciales sobre todo en la primera visita al cliente.
Reuniones online Su principal ventaja es que eliminamos de la ecuaci√≥n el par√°metro &ldquo;lugar&rdquo;, solo tenemos que centrarnos en encontrar un &ldquo;momento&rdquo; (Si los participantes se encuentran en partes del mundo muy diversas esto se puede complicar por la diferencia horaria) Este tipo de reuniones se pueden organizar de forma muy r√°pida, y no lo vamos a negar, nos permiten muchas veces seguir trabajando mientras asistimos a la reuni√≥n. Su desventaja principal es que no hay cultura de uso de las herramientas online en muchas organizaciones y nos podemos encontrar que no¬†est√©n¬†dispuestos a &ldquo;verse por Skype&rdquo; Es el formato de reuni√≥n m√°s adecuado para equipos de trabajo dentro de la misma empresa, para comunicarse con los proveedores, y para gestionar un proyecto con cliente una vez que ya ha arrancado (cambios, ajustes, intercambio de impresiones, etc)
`,url:"https://sergiocarracedo.es/2011/09/22/reuniones-presenciales-vs-reuniones-online/",image:"/2011/09/22/reuniones-presenciales-vs-reuniones-online/reuniones_presenciales_vs_reuniones_online_hu_5784c4b21743a14d.jpg",tags:[],readingTime:"2 minutes read",date:"Sep 22, 2011"},"https://sergiocarracedo.es/2011/08/29/convertir-tablas-innodb-en-myisam/":{title:"Convertir tablas InnoDB en MyISAM",content:`Recientemente me¬†surgi√≥¬†la necesidad de convertir motor¬†de las tablas de una base de datos que usaban InnoDB¬†a¬†MyISAM, la soluci√≥n es trivial en apariencia, es decir, convertir una tabla es tan sencillo como
ALTER TABLE [NOMBRE_BBDD].[NOMBRE_TABLA] engine=MyISAM; Pero si son muchas las tablas a convertir esta tarea se puede convertir en insufrible¬†haci√©ndonos¬†perder un valioso tiempo, e aqu√≠ la soluci√≥n:
SELECT CONCAT(&#39;ALTER TABLE &#39;,table_schema,&#39;.&#39;,table_name,&#39; engine=MyISAM;&#39;) FROM information_schema.tables WHERE ENGINE = &#39;InnoDB&#39; AND table_schema = &#39;[NOMBRE_BASE_DE_DATOS]&#39; Con esta consulta conseguimos como resultado una cantidad de tuplas igual al numero de tablas a convertir como √∫nico contenido de cada uno de ellos es la consulta SQL necesaria para convertir la tabla. Ya solo nos queda ejecutar las consultas obtenidas y voil√°¬†tendremos nuestras tablas convetidas a MyISAM
`,url:"https://sergiocarracedo.es/2011/08/29/convertir-tablas-innodb-en-myisam/",image:"/2011/08/29/convertir-tablas-innodb-en-myisam/technology-servers-server-159282_hu_e3683651274e1fd8.jpeg",tags:[],readingTime:"1 minute read",date:"Aug 29, 2011"},"https://sergiocarracedo.es/2011/08/12/moviendo-tu-blog-wordpress-de-dominio/":{title:"Moviendo tu blog wordpress de dominio",content:`En ocasiones necesitamos cambiar una p√°gina realizada en Wordpress de ruta o de dominio, esto puede deberse simplemente a que estabamos trabajando en una copia local y una vez finalizada la llevamos al servidor definitivo, o por que queremos hacer funcionar nuestro blog, que tenemos en un servidor , en local.
Wordpress guarda en la base de datos informaci√≥n referenciada a rutas absolutas, por lo que una vez movido el blog dejar√° de funcionar o no lo har√° de una forma correcta.
Con estos sencillos pasos podr√°s resolver este inconveniente.
En primer lugar es muy importante realizar una copia de seguridad de la base de datos, por lo que pudiera pasar.
Una vez que tenemos copiado todo el contenido del √°rbol de directorios y la base de datos de _Wordpress_en la nueva ubicaci√≥n cambiamos si es necesario los datos de acceso a la base de datos (si fuese necesario), estos datos se encuentan en wp-config.php
Ahora accedemos a la base de datos con phpmyadmin, o cualquier otro cliente que nor permita ejecutar las siguientes consultas SQL.
UPDATE wp_options SET option_value = replace(option_value, &#39;http://DOMINIOANTERIOR.com&#39;, &#39;http://NUEVODOMINIO.com&#39;); UPDATE wp_options SET option_value = replace(option_value, &#39;feed://DOMINIOANTERIOR.com&#39;, &#39;feed://NUEVODOMINIO.com&#39;); UPDATE wp_posts SET guid = replace(guid, &#39;http://DOMINIOANTERIOR.com&#39;,&#39;http://NUEVODOMINIO.com&#39;); UPDATE wp_posts SET post_content = replace(post_content, &#39;http://DOMINIOANTERIOR.com&#39;, &#39;http://NUEVODOMINIO.com&#39;); UPDATE wp_postmeta SET meta_value = replace(meta_value, &#39;http://DOMINIOANTERIOR.com&#39;, &#39;http://NUEVODOMINIO.com&#39;); Con esto tenemos hecha la mayor parte del trabajo, pero si hemos cambiado la ruta (independientemente de que cambie el dominio) y utilizamos URLs amigables mediante mod_rewrite, necesitamos cambiar el .htaccess
RewriteEngine On RewriteBase / #Modificar con la ruta adecuada RewriteRule ^index\\.php$ - [L] RewriteCond %{REQUEST_FILENAME} !-f RewriteCond %{REQUEST_FILENAME} !-d RewriteRule . /index.php [L] #Modificar con la ruta adecuada `,url:"https://sergiocarracedo.es/2011/08/12/moviendo-tu-blog-wordpress-de-dominio/",image:"/2011/08/12/moviendo-tu-blog-wordpress-de-dominio/pexels-photo-310983_hu_4cb517db49539a0f.jpeg",tags:[],readingTime:"2 minutes read",date:"Aug 12, 2011"},"https://sergiocarracedo.es/2011/08/08/nueva-rama-de-desarrollo-en-nodejs-y-nueva-imagen-de-marca/":{title:"Nueva rama de desarrollo en Node.js y nueva imagen de marca",content:`Node.js del que ya hemos hablado en posts anteriores, acaba de publicar una nueva rama (branch) de desarrollo, la 0.5.x, sobre la que seguir creciendo.¬†Hay numerosos cambios y bugfixes que podeis conocer aqui.¬†La rama estable continua siendo la 0.4.x, Este es un momento importante para el proyecto que continua su desarrollo. Muestra de esto es al apoyo de Microsoft para crear un port¬†sobre la plataforma Windows Tambi√©n¬†ha aprovechado para evolucionar la marca y crear una nueva imagen corporativa m√°s consistente e identificable.
`,url:"https://sergiocarracedo.es/2011/08/08/nueva-rama-de-desarrollo-en-nodejs-y-nueva-imagen-de-marca/",image:"/2011/08/08/nueva-rama-de-desarrollo-en-nodejs-y-nueva-imagen-de-marca/nueva_rama_de_desarrollo_en_node.js_y_nueva_imagen_de_marca_hu_2f18950b8cfeead9.png",tags:[],readingTime:"1 minute read",date:"Aug 8, 2011"},"https://sergiocarracedo.es/2011/08/03/ocultar-codigo-comentado-en-plantillas-php/":{title:"Ocultar c√≥digo comentado en plantillas PHP",content:`Un post fresquito para el verano ;) Cuando trabajamos con plantillas php, como las que usan Drupal¬†o **Wordpress,¬†**en las que introducimos nuestras variables directamente el html mediante _php,¬†_en ocasiones, sobre todo cuando estamos realizando pruebas o cambios solicitados por el cliente necesitamos comentar parte del c√≥digo, tanto¬†php¬†como¬†_html,¬†_para poder volver¬†f√°cilmente¬†a el si algo va mal. Podemos caer en la tentaci√≥n de usar las etiquetas de comentarios de html ,¬†esto ocultar√° lo que no queremos ver, pero el php dentro del comentario continuar√° ejecutandose. De esta forma es m√°s probable que aparezcan errores en el nuevo c√≥digo adem√°s de perjudicar el tiempo de ejecuci√≥n y carga de la p√°gina ya que estamos procesando y enviado al navegador elementos que no van a ser mostrados. La soluci√≥n es tan sencilla como elegante, solo tenemos que introducir todo el c√≥digo (html y php) a comentar dentro de comentarios de php Veamos un ejemplo:
&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;!--?php echo $variable ?--&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; La forma de comentarlo seria:
&lt;?php /* &lt;ul&gt; &lt;li&gt; &lt;!--?php echo $variable ?--&gt; &lt;/li&gt; &lt;/ul&gt; */ ?&gt; De esta forma no quedar√° rastro del c√≥digo en el contenido enviado al navegador ni se consumira tiempo de ejecuci√≥n.
`,url:"https://sergiocarracedo.es/2011/08/03/ocultar-codigo-comentado-en-plantillas-php/",image:"/2011/08/03/ocultar-codigo-comentado-en-plantillas-php/pexels-photo-270360_hu_591fced7e8991dc5.jpeg",tags:[],readingTime:"1 minute read",date:"Aug 3, 2011"},"https://sergiocarracedo.es/2011/07/14/google-plus-la-nueva-red-social/":{title:"Google Plus, ¬øla nueva red social?",content:`Ya han pasado unas semanas desde la presentaci√≥n de Google+, y ha superado todas las expectativas de la propia Google, se habla de 30 millones de usuarios antes de final de mes. Estas cifras son sorprendentes y analizadas sin tener en cuenta el contexto actual son muy sorprendentes y favorables, pero comparativamente con el gigante de las redes sociales Facebook son irrisorias. Actualmente Facebook cuenta con 550 millones de usuarios en todo el mundo y aunque empieza a mostrar estancamiento, o incluso perdida de usuarios, en algunos paises (USA y UK, por ejemplo) a√∫n tiene potencial para seguir creciendo.
Facebook tiene a su favor lo m√°s importante, los usuarios, a los que mantiene semisecuestrados en su red, complicando la salida de estos hacia otras redes. En este sentido es notable el caso de la extensi√≥n del navegador Chrome, Facebook Friend Exporter, que permitia exportar tu lista de amigos de Facebook a CVS o a Google Contacts (lo que facilita mucho el paso a Google+) que he podido usar sin problemas, pero que desde hace unos¬†d√≠as, ha dejado de funcionar, como indica su desarrollador, por que Facebook esta poniendo todos los impedimentos posibles al funcionamiento de la misma. No voy a entrar en muchos detalles sobre el funcionamiento de Google+, por que sobre ello se han escrito multitud de entradas de blogs, noticias, etc, pero si me gustar√≠a opinar sobre lo que le falta, en este momento, a Google+ para seguir avanzando.
API Aun no hay API disponible, aunque si ya hay un prerregistro para solicitar que nos avisen en cuanto este disponible. La API es una pieza muy importante, que¬†permitir√°¬†entre otras cosas, que que podamos acceder a nuestra red G+ desde aplicaciones como Tweetdeck, Hootsuite, Echofon, twicca, etc&hellip;
Perfiles o p√°ginas de empresas En Facebook, se ha generado un ecosistema de marketing alrededor de las p√°ginas de empresa, que son¬†tambi√©n¬†una forma de estar en contacto con las empresas de nuestro interes. En G+ tampoco¬†deber√≠an¬†faltar.
Juegos Probablemente los geeks no estemos de acuerdo con esto, no los necesitamos, y son muchas veces una fuente de spam, pero para el usuario medio si que son importantes y pueden ser un atractivo que les invite a dar el salto a esta red. La compra de varias empresas de juegos online por parte de Google hace unos meses y el hallazgo en el c√≥digo js de G+ de cadenas relacionadas con invitaciones a juegos, nos hace pensar que esto ser√° una realidad muy pronto. A continuaci√≥n dejo unos interesantes links con informaci√≥n relativa a G+ http://www.genbeta.com/redes-sociales/los-puntos-fuertes-y-debiles-de-google-tras-varios-dias-de-uso http://vicarlone.com/trucos-google-plus/ http://www.enriquedans.com/2011/07/google-impresiones-tras-dos-semanas-de-uso.html http://starblogger.net/9-must-have-chrome-extensions-enhance-your-google-plus-experience.html
`,url:"https://sergiocarracedo.es/2011/07/14/google-plus-la-nueva-red-social/",image:"/2011/07/14/google-plus-la-nueva-red-social/logo-google-plus_hu_f3b8cc982a9d50e4.jpg",tags:[],readingTime:"3 minutes read",date:"Jul 14, 2011"},"https://sergiocarracedo.es/2011/07/12/chat-en-tu-navegador-con-nodejs-y-faye/":{title:"Chat en tu navegador con node.js y Faye",content:`Siguiendo con los posts sobre faye, vamos a realizar un ejemplo pr√°ctico y muy vistoso, vamos a realizar un sencillo pero funcional chat web en este post.
Lo primero es crear el servidor node: Solo tenemos que incluir la extensi√≥n de faye para node
var Faye = require(&#39;./faye/faye-node.js&#39;); var fayeServer = new Faye.NodeAdapter({&amp;nbsp; mount: &amp;nbsp; &amp;nbsp;&#39;/&#39;}); fayeServer.listen(8888); A este fichero lo llamaremos node.js, ahora desde bash lanzamos el servidor que acabamos de crear: $ node node.js Y ya tenemos nuestro &ldquo;servidor de chat&rdquo; corriendo, preparado para funcionar En el lado cliente creamos un sencillo html y un script para gestionar el chat:
&lt;script type=&#34;text/javascript&#34; src=&#34;&lt;a href=&#34;https://ajax.googleapis.com/ajax/libs/jquery/1.6.2/jquery.min.js&#34;&gt;&lt;/script&gt;&lt;script&#34;&gt;https://ajax.googleapis.com/ajax/libs/jquery/1.6.2/jquery.min.js&#34;&gt;&lt;/scri...&lt;/a&gt; type=&#34;text/javascript&#34; src=&#34;faye/faye-browser-min.js&#34;&gt;&lt;/script&gt; &lt;script type=&#34;text/javascript&#34;&gt;// &lt;![CDATA[ $(function() { var client = new Faye.Client(&#39;&lt;a href=&#34;http://localhost:8888/&#39;&#34;&gt;http://localhost:8888/&#39;&lt;/a&gt;); var nick = &#34;&#34;; client.subscribe(&#39;/chat&#39;, function(message) { $(&#34;#chat&#34;).append(&#34;&lt;div&gt;&lt;strong&gt;&#34;+message.nick+&#34;&lt;/strong&gt;: &#34;+message.text+&#34;&lt;/div&gt;&#34;) }); $(&#34;.nick form&#34;).submit(function() { nick=$(&#34;.nick input[name=nick]&#34;).val(); $(&#34;.nick&#34;).hide(); $(&#34;.chatMessage&#34;).show(); return false; }) $(&#34;.chatMessage form&#34;).submit(function() { text = $(&#34;.chatMessage input[name=message]&#34;).val(); client.publish(&#39;/chat&#39;, { nick : nick, text: text }); return false; }) }); &gt;&lt;/script&gt; &lt;div class=&#34;nick&#34;&gt; &lt;form action=&#34;#&#34; enctype=&#34;multipart/form-data&#34; method=&#34;POST&#34;&gt; &lt;label&gt;Tu nick:&lt;/label&gt;&amp;nbsp; &lt;input name=&#34;nick&#34; type=&#34;text&#34; /&gt; &lt;input type=&#34;submit&#34; value=&#34;Entrar al chat!!&#34; /&gt; &lt;/form&gt; &lt;/div&gt; &lt;div class=&#34;chatMessage&#34; style=&#34;display: none;&#34;&gt; &lt;form action=&#34;#&#34; enctype=&#34;multipart/form-data&#34; method=&#34;POST&#34;&gt; &lt;label&gt;Tu mensaje:&lt;/label&gt;&amp;nbsp; &lt;input name=&#34;message&#34; type=&#34;text&#34; /&gt; &lt;input type=&#34;submit&#34; value=&#34;Enviar&#34; /&gt; &lt;/form&gt; &lt;/div&gt; `,url:"https://sergiocarracedo.es/2011/07/12/chat-en-tu-navegador-con-nodejs-y-faye/",image:"/2011/07/12/chat-en-tu-navegador-con-nodejs-y-faye/chat_en_tu_navegador_con_node.js_y_faye_hu_ab26e8997ccec8.jpg",tags:[],readingTime:"1 minute read",date:"Jul 12, 2011"},"https://sergiocarracedo.es/2011/07/11/mejorando-nodejs-faye/":{title:"Mejorando node.js: Faye",content:`Como ya comentamos en una entrada anterior node.js es una herramienta muy potente, que nos permite crear servidores empleando javascript como lenguaje de programaci√≥n en el servidor, pero puede ser costoso crear ciertas herramientas y protocolos desde 0. Faye es una capa (extensi√≥n) sobre node.js, l√≥gicamente¬†escrita en javascript,¬†que crea un sistema de publicaci√≥n y¬†suscripci√≥n¬†a canales de mensajes basados en el protocolo Bayeux.
En otras palabras, faye se encarga de gestionar los canales, los envios de mensajes y la recepci√≥n por parte de los¬†subscriptores¬†a esos canales. Faye consta de dos partes, una en el lado servidor y otra en el lado cliente (tambi√©n¬†escrita en javascript) Para explicar el funcionamiento de faye m√°s f√°cilmente pongamos un ejemplo: Imaginemos que estamos creando una aplicaci√≥n que necesita enviar notificaciones en tiempo real a todos los usuarios conectados a una p√°gina. En nuestro script para node.js instroducimos lo siguiente:
var Faye = require(&#39;faye&#39;), server = new Faye.NodeAdapter({mount: &#39;/&#39;}); server.listen(8000); y lanzamos node.js
$ node faye.js Ahora en el cliente, cargamos un fichero javascript con este contenido:
var client = new Faye.Client(&#39;&lt;a href=&#34;http://localhost:8000/&#34;&gt;http://localhost:8000/&lt;/a&gt;&#39;); client.subscribe(&#39;/messages&#39;, function(message) { alert(&#39;Got a message: &#39; + message.text); }); Con estas dos sencillas operaciones, todos los usuarios que est√©n conectados a nuestra web estar√°n subscritos al canal messages y estar√°n en disposici√≥n de recibir informaci√≥n. Para enviar un mensaje, desde uno de los clientes, que puede ser nuestro panel de control de la web ejecutamos el siguiente javascript:
client.publish(&#39;/messages&#39;,{ text: &#39;Hello world&#39; }); Y inmediatamente en todos los navegadores de los visitantes de la web aparecer√° un alert con el mensaje &ldquo;Hello world&rdquo; Este es un ejemplo¬†extra√≠do¬†de la web de faye, pero¬†quer√≠a¬†explicarlo con un ejemplo pr√°ctico. Como veis la potencialidad y simpleza de este sistema es grande, ya que podemos enviar m√°s variables que message si lo necesitasemos (por ejemplo, n√∫mero de mensajes sin leer, fecha, etc), tambien podemos tener varios canales distintos a por los que enviar distintos tipos de informaci√≥n o solo subscribir a algunos usuarios a esos canales. En un pr√≥ximo post explicar√© como manipular los mensajes en el lado servidor, para por ejemplo, almacenarlos en una base de datos, enviar mensajes desde el propio servidor, etc.
`,url:"https://sergiocarracedo.es/2011/07/11/mejorando-nodejs-faye/",image:"/2011/07/11/mejorando-nodejs-faye/mejorando_node.js_faye_hu_f0ac1e66b0a80874.jpg",tags:[],readingTime:"2 minutes read",date:"Jul 11, 2011"},"https://sergiocarracedo.es/2011/06/27/3d-sin-plugins-en-tu-navegador-con-javascript-threejs/":{title:"3D sin plugins en tu navegador con javascript: three.js",content:`Hasta hace muy poco¬†necesit√°bamos¬†plugins, para realizar tareas que ahora nos parecen triviales, reproducir sonido, ver videos, hasta la m√°s simple de las animaci√≥n de un bot√≥n. Pero esto esta cambiando, la evoluci√≥n de los navegadores es fren√©tica y la pr√≥xima liberaci√≥n de la especificaci√≥n final de HTML5 y CSS3 y otros estandares (actualmente son borradores), nos hace pensar que podremos tener pronto una nueva revoluci√≥n en la web. Permitiendo a los creativos dar rienda suelta a sui imaginaci√≥n sin chocar con la usabilidad, la accesibilidad y el posicionamiento como sucede hasta la fecha.
En una nueva demostraci√≥n de las posibilidades de los¬†est√°ndares¬†mrdoob, pone a nuestra disposici√≥n un framework escrito completamente en javascript para manipulaci√≥n de objetos 3D. Con lo que nos libera de la carga de realizar muchos de los¬†c√°lculos¬†necesarios para algo tan simple generar la¬†perspectiva¬†segun el posicionamiento de la c√°mara. Simplemente nos tendremos que preocupar de colocar los objetos y la¬†c√°mara¬†en nuestro¬†lienzo para empezar a jugar con el 3d en nuestro navegador. three.js es capaz de trabajar sobre canvas (modo soportado por IE9, safari 4, FF4, Chrome) y empleando webGL, una especificaci√≥n que permite al navegador usar la tarjeta gr√°fica (GPU) para hacer el render de la escena, con la consiguiente mejora de velocidad y de calidad de los gr√°ficos (soportado por Chrome y FF4) Es casi obvio decir que modo canvas tiene¬†limitaciones en cuanto al uso de texturas, luces y sistemas de¬†part√≠culas.¬†Pero es perfectamente funcional para muchas aplicaciones.
`,url:"https://sergiocarracedo.es/2011/06/27/3d-sin-plugins-en-tu-navegador-con-javascript-threejs/",image:"/2011/06/27/3d-sin-plugins-en-tu-navegador-con-javascript-threejs/deer-1333814_1920_hu_1a2b7f4063f82f79.jpg",tags:[],readingTime:"2 minutes read",date:"Jun 27, 2011"},"https://sergiocarracedo.es/2011/04/29/websites-para-moviles/":{title:"Websites para m√≥viles",content:`Existe un gran debate acerca de que soluciones usar cuando adaptemos un website a m√≥viles. Por lo normal, usar CSS (style sheets) como soluci√≥n, nos ayuda a crear distintas visualizaciones sobre diferentes dispositivos m√≥viles, pero no todos los fabricantes siguen los est√°ndares. El resultado sobre algunos en ellos puede ser impredecible.
Hay diversidad de tipos de m√≥viles. En algunos solo se puede visualizar los elementos gr√°ficos m√°s esenciales, como las webs de principios de la d√©cada. En otros, como los smartphones, pueden mostrar mejor el contenido web e incluso tener m√°s funciones, ejecutar jQuery Mobile, etc. Aunque alguno de ellos, por ejemplo iPhone, no funciona, ni funcionar√° tecnolog√≠a Flash. Para sacar el m√°ximo partido a un website, proponemos dos opciones: A) Crear un website que funcione en la mayor√≠a de los m√≥viles Podemos hacer que nuestro site funcione sobre casi todos los dispositivos m√≥viles recortando los servicios y opciones, haciendo una maquetaci√≥n m√°s suelta, sin m√°rgenes, etc. De esta manera, nos aseguraremos de que nuestro website se ajustar√° al tama√±o de nuestro iPhone, Android o iPad, etc. Por otro lado, al navegar desde un ordenador de sobremesa, nuestro website puede parecer extremadamente minimalista, sin opciones, simple. Aun estando bien dise√±ado gr√°ficamente, puede parecer mal organizado. Al usuario le costar√° navegar y entender que est√° viendo, frustr√°ndose y creando una imagen negativa del sitio. Adem√°s, a la hora de desarrollar un sitie tan compatible, suma unos costes, que muchas veces compensa crear otro sitio independiente para los m√≥viles. Con este problema nos hemos encontrado muchas veces para hacer funcionar bien en el navegadores de sobremesa como Microsoft Explorer. Mejor crear una css exclusiva para √©l que limitar las opciones de otras como Safari, Firefox o Chrome. b) Crear un sitio espec√≠fico para m√≥viles, paralelo a nuestro website De esta manera tendremos la seguridad de que nuestro sitio va a funcionar perfectamente, tanto en el escritorio como el dispositivo m√≥vil. Podemos simplificar la navegaci√≥n, cambiar la estructura y organizar mejor el contenido que se muestre en el m√≥vil. Podremos tambi√©n desarrollar c√≥digo m√°s simple. En definitiva, el resultado ser√° m√°s predecible, tendremos m√°s controlados los bugs. Como en todo en la vida, tiene una parte buena y otra mala. En este caso, la mala, es que obligar√° a mantener dos sitios en vez de uno. A priori no parece gran cosa, pero seguro que cambiaremos de idea con proyectos de dimensiones considerables y varios idiomas. Si no se tiene bien controlado podemos encontrarnos con problemas de actualizaci√≥n, entre los contenidos de los dos sitios. Tambi√©n puede obligarnos a sacrificar ciertas caracter√≠sticas que aporten los dispositivos de gama, alta por miedo a que no funcionen en los de gama baja. No se puede tener todo. En fin, la decisi√≥n del m√©todo a seguir, depender√° de los usuarios a los que ir√° dirigido el sitio. Si nuestro p√∫blico tiene poder adquisitivo, tendr√°n m√≥viles de gama alta y podremos ofrecerles lo m√°ximo en prestaciones web. Por el contrario, si el poder adquisitivo es bajo, evidentemente los m√≥viles ser√°n de baja gama y tendremos que pensar en recortar posibilidades para que la aplicaci√≥n corra perfectamente en todos ellos y as√≠ llegar al m√°ximo de p√∫blico.
`,url:"https://sergiocarracedo.es/2011/04/29/websites-para-moviles/",image:"/2011/04/29/websites-para-moviles/table-1100252_1920_hu_2a5efb8e21b45371.jpg",tags:[],readingTime:"3 minutes read",date:"Apr 29, 2011"},"https://sergiocarracedo.es/2011/04/07/spam-social/":{title:"Spam Social",content:`El spam es una de las actividades m√°s molesta de internet, nos hace consumir nuestro tiempo en filtrar la informaci√≥n que recibimos.
Desgraciadamente estamos ya acostumbrados a la forma original de spam, la que nos llega por correo electr√≥nico, pero poco a poco los filtros antispam han mejorado mucho y es posible reducir la cantidad de spam recibida.
Con el nacimiento y posterior expansi√≥n de las redes sociales los spammers han modificado la forma en la que nos hacian llegar spam para acosarnos en Facebook, Twitter, MySpace, etc&hellip; Pero la novedad de esta spam es que ¬°nos lo envian nuestros amigos!
Es lo que podemos denominar Spam Social, mensajes que aparentemente nos dejan nuestros amigos en nuestro muro para que probemos una aplicaci√≥n, veamos un video o vayamos a una p√°gina.
En muchos casos el Spam Social se usa para difundir virus, al llevarte a una p√°gina maliciosa.
Pero yo no le envio ese tipo de mensajes a mis amigos, ¬øMe han pirateado la cuenta? ¬øHan robado mi contrase√±a? No, no han pirateado tu cuenta, ni nadie sabe tu contrase√±a, no es necesario, has permidio que ocurra.
Quien no a empleado una aplicacion, por ejemplo, de Facebook donde para acceder nos solicita unos permisos que muchas veces no leemos y que puede incluir desde, acceder a nuestros datos, o a nuestras fotos, a ver nuestros post, o escribir en el muro por ti.
Algunas de estas aplicaciones que hacen¬†Spam Social, no tienen que ser necesariamente aplicaciones maliciosas, me ha pasado estos dias con BranchOut una aplicaci√≥n de tintes similares a LinkedIn, pero dentro de Facebook. BranchOut emplea la mala pr√°ctica de publicar en el muro de tus amigos cualquier actividad que realices, por ejemplo contestar ¬øCon quien prefieres trabajar? de entre dos de tus amigos (aunque en cada pregunta aparece un diminuto checkbox para evitarlo). Y eso es spam en todo regla sobre todo cuando tu no sabes que al contestar esa inocente pregunta va a ser publicada en el muro del susodicho.
Hay otras muchas aplicaciones que realizan estas molestas practicas.
Entiendo que deseen darse a conocer y llegar r√°pidamente a muchos usuarios, probablemente lo consigan, pero en lo que a mi respecta las aplicaciones que practican esta forma tan agresiva de autopromocionarse, pasan a engrosar la ya larga lista de aplicaciones bloqueadas en mi cuenta.
Para terminar, os dejo unos cuantos enlaces sobre debates y opiniones similares
http://www.quora.com/Why-are-several-people-complaining-about-BranchOuts-viral-features
http://www.eriontheinterweb.com/2011/02/facebook-faux-paux-face-palm-i-hate-you-branchout/
http://www.liquida.com/article/15581986/facebook-linkedin-twitter/
http://www.allfacebook.com/branchout-lets-users-spam-others-explodes-in-popularity-2011-01
`,url:"https://sergiocarracedo.es/2011/04/07/spam-social/",image:"/2011/04/07/spam-social/5473611430_867ebc8ba3_hu_cbe1aacb143571a3.jpg",tags:[],readingTime:"2 minutes read",date:"Apr 7, 2011"},"https://sergiocarracedo.es/2010/11/26/musica-invisible/":{title:"M√∫sica invisible",content:`Soy un gran consumidor de m√∫sica a trav√©s de servicios de streaming como spotify, last.fm y otros. Creo que es el futuro de la m√∫sica, ese futuro que avanza como una tsunami y al que algunos tratan de poner freno por los m√©dios que sea.
La m√∫sica estan en su mejor momento (m√∫sica‚â†discogr√°ficas), pero los h√°bitos de consumo estan cambiando, por lo menos en lo que a mi ata√±e. Ya no quiero tener la m√∫sica almacenada en CDs por que es un formato inc√≥mo, si quiero escuchar un tema en concreto tengo que buscar el CD donde esta, y metelo en el ordenador.
Con los mp3 me pasa algo parecido, aunque tengo una gran biblioteca de m√∫sica, pero normalmente trabajo en varios ordenadores y a veces escucho m√∫sica en el m√≥vil. ¬øTengo que tener una replica de mi biblioteca musical en cada equipo que uso? ¬øY si quiero ense√±arle un tema concreto a un amigo en su ordenador?
Por eso cada d√≠a consumo m√°s m√∫sica de servicios de streaming de los que adem√°s soy subscriptor.
El problema que me plantean estos servicios es que hay ciertos grupos que han decidio no estar ahi, que no desean que la gente escuche su m√∫sica en estos servicios y que prefieren que pase por caja y se compre un CD o un mp3. Mi experiencia personal respecto a esto es que que poco a poco he dejado de escuchar esos grupos, o que el deseo de escucharlos tiene que ser muy poderoso para salir de mi programa de streaming y abrir los MP3 de sus canciones. Es por esto que creo que poco a poco ciertos grupos grandes que no estan en streaming se volver√°n invisibles para las nuevas generaciones de consumidores de m√∫sica que la consumiran preferiblemente en streaming.
Grupos como Pink Floyd, Oasis, Metallica, The Beatles,etc&hellip; sean completamente invisibles y perder√°n el valor de marca, de grupo de masas, que tuvieron en su momento.
Esta reflexi√≥n viene a colaci√≥n de la recientemente anunciada incorporaci√≥n de The Beatles al cat√°logo de iTunes, y estoy convencido que lo han hecho por que han percibido que las ventas de discos han caido en picado y que estaban perdiendo la ola, pero llega una ola m√°s grande que la de la venta digital, la ola del streaming
`,url:"https://sergiocarracedo.es/2010/11/26/musica-invisible/",image:"/2010/11/26/musica-invisible/musica-invisible_hu_de1afcf6186eae7a.jpeg",tags:["lifestyle"],readingTime:"2 minutes read",date:"Nov 26, 2010"},"https://sergiocarracedo.es/2010/10/25/nodejs-javascript-power2/":{title:"Node.js: Javascript power!!",content:`Node.js es un entorno de programaci√≥n de entrada/salida orientada a eventos, sobre el motor de Javascript V8 (el mismo que usa Google Chrome)
Pero ¬øQue quiere decir todo esto? ¬øY para que me puede servir?
Lo m√°s b√°sico que podemos realizar con node.js es un servidor web (HTTP).
var http = require(&#39;http&#39;); http.createServer(function (req, res) { res.writeHead(200, {&#39;Content-Type&#39;: &#39;text/plain&#39;}); res.end(&#39;Hello World\\n&#39;); }).listen(8124, &#34;127.0.0.1&#34;); Con estas pocas lineas tenemos un servidor web escuchando el puerto 8124 funcionando en nuestro equipo y respondiendonos siempre &ldquo;Hello World&rdquo;
Podemos crear tambien servidores TCP.
Para programar un servidor node.js empleamos javascript como lenguaje, con los que nos evitamos muchas de las complicaciones de otros lenguajes de programaci√≥n como C, C++, python, etc.
Como comentaba, esta completamente orientado a eventos por lo que no tendremos que preocuparnos de muchas tareas que el servidor realiza por nosotros, por ejemplo. Si queremos responder a una petici√≥n, programamos la respuesta al evento &ldquo;on data&rdquo;.
Es muy ligero y r√°pido, y su API es simplemente fantastica, tenemos acceso de recursos del equipo (ficheros) y muchas utilidades que nos facilitar√°n el trabajo de programaci√≥n.
Muchos os preguntareis: ¬øPara que necesito programar un servidor HTTP si hay opciones muy potentes disponibles (apache, ngix, etc)?
La respuesta es bien sencilla, no son tan ligeros y especificos como node.js, con el podremos hacer uso de la tecnolog√≠a push que nos permitira hacer virguerias como estas:
Chat en el navegador: http://chat.nodejs.org/
Podemos mostrar el cursor de los otros visitantes de la web: http://jeffkreeftmeijer.com/2010/experimenting-with-node-js/
Servidor de archivos est√°ticos http://net.tutsplus.com/tutorials/javascript-ajax/learning-serverside-javascript-with-node-js/
En proximos post seguiremos adentrandonos en este maravilloso proyecto
`,url:"https://sergiocarracedo.es/2010/10/25/nodejs-javascript-power2/",image:"/2010/10/25/nodejs-javascript-power2/node_hu_557d149cc2e30266.jpg",tags:["node","js"],readingTime:"2 minutes read",date:"Oct 25, 2010"},"https://sergiocarracedo.es/2010/10/19/servidor-de-maquinas-virtuales-remotas-con-virtualbox/":{title:"Servidor de M√°quinas virtuales remotas con Virtualbox",content:`Para un desarrollador de web, es muy importante tener un entorno de desarrollo y de prueba completamente optimizado.
En el apartado de pruebas lo recomendable es que probemos nuestros desarrollos web con la mayor cantidad de navegadores posibles.
Si como es nuestro caso, empleas Linux como sistema operativo de desarrollo, podras probar si problema la mayoria de los navegadores: Firefox, Chrome, Opera, Konqueror, etc, (incluso Safari usando wine), pero hay un navegador que se nos va a escapar y que por desgracia para la web es ampliamente utilizado: Internet Explorer.
Una posible soluci√≥n es disponer de un equipo con Windows (XP,Vista, 7) con este navegador y acceder a el para realizar las pruebas. Esto supone disponer de un equipo que √∫nicamente funcionar√° en Windows, con el coste que ello supone.
La soluci√≥n m√°s econ√≥mica es instalar windows en una m√°quina virtual y realizar las pruebas desde ella.¬†En nuestro caso usamos VirtualBox.
El problema de arrancar la maquina virutal en el mismo equipo es el consumo de recursos, sobre todo de memoria, lo que va a reducir el rendimiento del equipo y nuestra productividad.
Por ello y aprovechandonos del protoloco VRCP que implementa VirtualBox, podemos instalar la m√°quia virtual con windows en otro equipo de la red y acceder a la citada m√°quina mediante algun cliente de Terminal Server. De esta manera transladaremos la carga de recursos (procesador, memoria, etc) a otro equipo, permitiendonos adem√°s que varios usuarios utilicen la m√°quina virtual desde sus puestos de trabajo.
Voy a dar por hecho que ya tenemos instalada y configurada la m√°quina virtual en el equipo remoto (la documentaci√≥n de instalaci√≥n se puede consultar aqui).
Ahora accedemos por ssh al equipo en el que tenemos la m√°quina virtual.
ssh equipo_vm
Y arrancamos la m√°quina virtual pero sin interface g≈ïafica
VBoxHeadless -startvm &quot;Windows 7&quot;
Ahora ya podemos conectarnos via cliente de terminal a la m√°quina y realizar nuestras pruebas
`,url:"https://sergiocarracedo.es/2010/10/19/servidor-de-maquinas-virtuales-remotas-con-virtualbox/",image:"/2010/10/19/servidor-de-maquinas-virtuales-remotas-con-virtualbox/virtualbox_hu_f14720e025098ce0.png",tags:["devops"],readingTime:"2 minutes read",date:"Oct 19, 2010"},"https://sergiocarracedo.es/2010/08/25/usabilidad-notificaciones-en-el-titulo-de-la-pagina/":{title:"Usabilidad: Notificaciones en el t√≠tulo de la p√°gina",content:`Hoy en dia la mayoria de los usuarios cuando estamos navegando tenemos abiertas simultaneamente varias pesta√±as en el navegador. A veces nuestra aplicaci√≥n web necesita llamar la atenci√≥n del usuario para requerir de el una acci√≥n o simplemente hacerle sabes que ha recibido una notificaci√≥n.
Una buena manera de hacerlo es cambiar alternativamente el t√≠tulo de la p√°gina por un aviso, de la misma forma que hace facebook cuando recibimos una notificaci√≥n de chat; el t√≠tulo de la p√°gina cambia 3 o 4 veces entre &ldquo;Facebook&hellip;.&rdquo; y &ldquo;Fulanito te ha enviado un mensaje&rdquo;.
Para facilitar esta, a simple vista tarea, hemos crado un plugin de jQuery muy sencillo de utilizar.
Solo tenemos que pasarle como par√°metro, el mensaje de aviso que se va a alternar con el t√≠tulo original de la p√°gina.
Podemos pasarle como opciones el n√∫mero de repeticiones del cambio y el tiempo (en milisegundos) entre cambios.
Os dejo el c√≥digo fuente del pluign
/** * Title blink for web pages that allow to change title page blinks like facebook chat notificacion * * This file is part of jquery.titleBlink * * jquery.titleBlink is free software: you can redistribute it and/or modify it under * the terms of the GNU Lesser General Public License as published by the Free * Software Foundation, either version 3 of the License, or (at your option) * any later version. * * jquery.titleBlink is distributed in the hope that it will be useful, but WITHOUT ANY * WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS * FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for * more details. * * You should have received a copy of the GNU Lesser General Public License * along with jquery.titleBlink. If not, see . * * @author Sergio Carracedo Martinez * @copyright 2010 Sergio Carracedo Martinez * @license &lt;a href=&#34;http://www.gnu.org/licenses/lgpl-3.0.txt&#34;&gt;http://www.gnu.org/licenses/lgpl-3.0.txt&lt;/a&gt; GNU LGPL 3.0 * @version SVN: $Id: jquery.titleBlink.js 1 2010-08-25 17:44:00Z gasman406f $ */ jQuery.extend({ titleBlink : function(title,options) { var defaults = { repeat : 5, delay : 800 }; var options = $.extend(defaults, options); var repeatCount = 0; var currentTitle=$(document).attr(&#34;title&#34;); var blinkInterval = setInterval(function() { if($(document).attr(&#34;title&#34;)==currentTitle) { $(document).attr(&#34;title&#34;,title); } else { $(document).attr(&#34;title&#34;,currentTitle); repeatCount++; if (repeatCount==options.repeat) { clearInterval(blinkInterval); } } }, options.delay); } }) `,url:"https://sergiocarracedo.es/2010/08/25/usabilidad-notificaciones-en-el-titulo-de-la-pagina/",image:"/2010/08/25/usabilidad-notificaciones-en-el-titulo-de-la-pagina/pexels-photo-1851415_hu_11381a65d90f2838.jpeg",tags:["js","frontend"],readingTime:"2 minutes read",date:"Aug 25, 2010"},"https://sergiocarracedo.es/about/":{title:"About me",content:`I discovered computers when I was child in a moment the home computers started to be more common. My first computer was an Amstrad CPC 464 with a green-screen, in that moment I understood and wanted to learn more about computers.
As child I started to program in BASIC, then QBASIC and later Turbo Pascal (I keep an example rescued from an old floppy disk: https://bitbucket.org/sergiocarracedo/liga-fantastica-marca/src/master/)
`,url:"https://sergiocarracedo.es/about/",image:"",tags:[],readingTime:"1 minute read",date:"Jan 1, 0001"},"https://sergiocarracedo.es/blog/by-year/":{title:"Blog posts by year",content:"",url:"https://sergiocarracedo.es/blog/by-year/",image:"",tags:[],readingTime:"0 minutes read",date:"Jan 1, 0001"},"https://sergiocarracedo.es/cookies/":{title:"Cookies",content:`Esta web, como otras muchas, utiliza cookies que permitir√°n mejorar su experiencia como usuario y en cumplimiento de la LSSI (LEY 34/2002, de 11 de julio, de servicios de la sociedad de la informaci√≥n y de comercio electr√≥nico), vamos a informarle sobre que son, cuales utilizamos en nuestra web, como eliminarlas y le pediremos el consentimiento para su uso.
INFORMACI√ìN ¬øQu√© son las cookies? Son peque√±os archivos de texto que se descargan e instalan en su ordenador o en su m√≥vil a trav√©s de su navegador ( Internet Explorer, Firefox, Chrome, Safari‚Ä¶) o a trav√©s de una aplicaci√≥n Flash. Las cookies permiten que las webs le ‚Äúrecuerden‚Äù, ya sea durante una visita (mediante las llamadas ‚Äúcookies de sesi√≥n‚Äù) o durante varias visitas (mediante las llamadas ‚Äúcookies persistentes‚Äù) y almacenen informaci√≥n sobre sus preferencias para ofrecerle una experiencia personalizada, m√°s f√°cil y m√°s r√°pida. Solamente el servidor que la instal√≥ podr√° leer su contenido, que por otra parte, ser√° an√≥nimo. Aunque se instalen en su disco duro, al ser solo texto, no son capaces por s√≠ mismas de acceder a informaci√≥n personal de su ordenador, ni a trasmitir virus, tan solo rastrean la navegaci√≥n sobre un sitio web. A continuaci√≥n le explicamos cuales son las cookies seg√∫n su funci√≥n:
COOKIES SEG√öN EL PLAZO DE TIEMPO COOKIES DE SESI√ìN Son cookies dise√±adas para recabar y almacenar datos mientras el usuario accede a una p√°gina web. Se suelen emplear para almacenar informaci√≥n que solo interesa conservar para la prestaci√≥n del servicio solicitado por el usuario en una sola ocasi√≥n.
COOKIES PERSISTENTES Son un tipo de cookies en el que los datos siguen almacenados en el terminal y a los que se puede acceder y tratar durante un periodo definido por el responsable de la cookie, y que puede ir de unos minutos a varios a√±os.
COOKIES SEG√öN LA ENTIDAD QUE LAS GESTIONE COOKIES PROPIAS Son las creadas por el responsable de la p√°gina web.
COOKIES DE TERCEROS Son las creadas y gestionas por otras entidades, como prestadores de servicios publicitarios, de personalizaci√≥n o an√°lisis, estos terceros pueden reportar datos an√≥nimos.
COOKIES SEG√öN FINALIDAD COOKIES T√âCNICAS Son aqu√©llas que permiten al usuario la navegaci√≥n a trav√©s de una p√°gina web,plataforma o aplicaci√≥n y la utilizaci√≥n de las diferentes opciones o servicios que en ella existan como, por ejemplo, controlar el tr√°fico y la comunicaci√≥n de datos, identificar la sesi√≥n, acceder a partes de acceso restringido, recordar los elementos que integran un pedido, realizar el proceso de compra de un pedido, realizar la solicitud de inscripci√≥n o participaci√≥n en un evento, utilizar elementos de seguridad durante la navegaci√≥n, almacenar contenidos para la difusi√≥n de videos o sonido o compartir contenidos a trav√©s de redes sociales
COOKIES DE PERSONALIZACI√ìN Son aqu√©llas que permiten al usuario acceder al servicio con algunas caracter√≠sticas de car√°cter general predefinidas en funci√≥n de una serie de criterios en el terminal del usuario como por ejemplo serian el idioma, el tipo de navegador a trav√©s del cual accede al servicio, la configuraci√≥n regional desde donde accede al servicio, etc.
COOKIES DE AN√ÅLISIS Son aqu√©llas que permiten al responsable de las mismas, el seguimiento y an√°lisis del comportamiento de los usuarios de los sitios web a los que est√°n vinculadas. La informaci√≥n recogida mediante este tipo de cookies se utiliza en la medici√≥n de la actividad de los sitios web, aplicaci√≥n o plataforma y para la elaboraci√≥n de perfiles de navegaci√≥n de los usuarios de dichos sitios, aplicaciones y plataformas, con el fin de introducir mejoras en funci√≥n del an√°lisis de los datos de uso que hacen los usuarios del servicio.
COOKIES PUBLICITARIAS Son aqu√©llas que permiten la gesti√≥n, de la forma m√°s eficaz posible, de los espacios publicitarios que, en su caso, el editor haya incluido en una p√°gina web, aplicaci√≥n o plataforma desde la que presta el servicio solicitado en base a criterios como el contenido editado o la frecuencia en la que se muestran los anuncios.
COOKIES DE PUBLICIDAD COMPORTAMENTAL Son aqu√©llas que permiten la gesti√≥n, de la forma m√°s eficaz posible, de los espacios publicitarios que, en su caso, el editor haya incluido en una p√°gina web, aplicaci√≥n o plataforma desde la que presta el servicio solicitado. Estas cookies almacenan informaci√≥n del comportamiento de los usuarios obtenida a trav√©s de la observaci√≥n continuada de sus h√°bitos de navegaci√≥n, lo que permite desarrollar un perfil espec√≠fico para mostrar publicidad en funci√≥n del mismo.
¬øQu√© tipo de cookies utiliza esta web ? A continuaci√≥n te definimos las funciones y el tipo de cookies que utilizamos en esta web y su finalidad:
COOKIES PROPIAS cookieconsent_status Almacena la respuesta al consentimiento de uso de cookies durante 1 a√±o.
COOKIES DE TERCEROS Google Analitycs: Tipo: Cookie anal√≠tica de terceros utilizadas por nosotros y por Google M√°s Informaci√≥n: Definici√≥n: Las Cookies anal√≠ticas son aqu√©llas que permiten al responsable de las mismas, el seguimiento y an√°lisis del comportamiento de los usuarios de los sitios web a los que est√°n vinculadas. Finalidad: La informaci√≥n recogida mediante este tipo de cookies se utiliza en la medici√≥n de la actividad de los sitios web, aplicaci√≥n o plataforma y para la elaboraci√≥n de perfiles de navegaci√≥n de los usuarios de dichos sitios, aplicaciones y plataformas, con el fin de introducir mejoras en funci√≥n del an√°lisis de los datos de uso que hacen los usuarios del servicio.
Cookies: _utma: Finalidad: Registra la fecha de la primera y √∫ltima vez que el usuario visito el sitio web. Expira: 2 a√±os
_utmb: Finalidad: Calcula el tiempo que dura la visita de un usuario almacenando el momento que entro en la p√°gina. ** Expira**: 30 minutos
_utmc: Finalidad: Calcula el tiempo que dura la visita de un usuario almacenando el momento que sale de la p√°gina. ** Expira**: Al finalizar la sesi√≥n
_utmz: Finalidad: Mantiene un seguimiento de donde proviene el visitante, que motor de b√∫squeda se utiliz√≥, en que enlace hizo click, que palabras claves utilizo y desde en qu√© lugar del mundo se accedi√≥ a la p√°gina. Expira: 6 meses
_ga Finalidad: Se utiliza para distinguir a los usuarios Expira: 2 a√±os
_gid Finalidad: Se usa para distinguir a los usuarios. Expira: 24 horas
_gat Finalidad: Se usa para limitar el porcentaje de solicitudes. Expira: 1 minuto
AMP_TOKEN Finalidad: Incluye un token que se puede utilizar para recuperar un ID de cliente del servicio de ID de cliente de AMP. Otros posibles valores indican inhabilitaciones, solicitudes en curso o errores obtenidos al recuperar un ID del servicio de ID de cliente de AMP. Expira: 30 segundos a 1 a√±o
gac Finalidad: Incluye informaci√≥n de la campa√±a relativa al usuario. Si has vinculado tus cuentas de Google Analytics y AdWords, las etiquetas de conversi√≥n de sitios web leer√°n esta cookie, a menos que la inhabilites. M√°s informaci√≥n Expira: 90 d√≠as
Disqus: Tipo: Cookies necesarias por el sistema de comentarios Disqus M√°s Informaci√≥n: Definici√≥n:Estas cookies son aqu√©llas que permiten al responsable de las mismas, la gesti√≥n del sistema de comentarios. Finalidad: La informaci√≥n recogida mediante este tipo de cookies se utiliza en la medici√≥n de la actividad de los sitios web, aplicaci√≥n o plataforma y para la elaboraci√≥n de perfiles de navegaci√≥n de los usuarios de dichos sitios, aplicaciones y plataformas, con el fin de introducir mejoras en funci√≥n del an√°lisis de los datos de uso que hacen los usuarios del servicio.
Cookies: __qca: Finalidad: Registra la fecha de la primera y √∫ltima vez que el usuario visito el sitio web. Expira: 2 a√±os
__jid Finalidad: Identificar al usuario logueado en Disqus. Expira: 30 minutos
_utma: Finalidad: Registra la fecha de la primera y √∫ltima vez que el usuario visito el sitio web. Expira: 2 a√±os
_utmb: Finalidad: Calcula el tiempo que dura la visita de un usuario almacenando el momento que entro en la p√°gina. ** Expira**: 30 minutos
_utmc: Finalidad: Calcula el tiempo que dura la visita de un usuario almacenando el momento que sale de la p√°gina. ** Expira**: Al finalizar la sesi√≥n
_utmz: Finalidad: Mantiene un seguimiento de donde proviene el visitante, que motor de b√∫squeda se utiliz√≥, en que enlace hizo click, que palabras claves utilizo y desde en qu√© lugar del mundo se accedi√≥ a la p√°gina. Expira: 6 meses
mc Finalidad: Mantiene un seguimiento de donde proviene el visitante, que motor de b√∫squeda se utiliz√≥, en que enlace hizo click, que palabras claves utilizo y desde en qu√© lugar del mundo se accedi√≥ a la p√°gina. Expira: 6 meses
_ga.js: Finalidad: cookies de origen para: determinar el dominio que se medir√°, distinguir a los usuarios √∫nicos, recordar el n√∫mero y la duraci√≥n de las visitas anteriores, recordar la informaci√≥n de las fuentes de tr√°fico, determinar el inicio y el fin de una sesi√≥n, recordar el valor de las variables personalizadas de visitante. Expira: 2 a√±os
_ga: Finalidad: Se utiliza para distinguir a los usuarios. Expira: 2 a√±os
disqus_unique: Finalidad: Estadisticas internas de Disqus usadas para visitantes an√≥nimos. Expira:: 1 a√±o
testCookie: Finalidad: Usada por Disqus para comprobar si el navegador permite cookies de terceros. Expira: Sesi√≥n
Revocaci√≥n del consentimiento: Si m√°s adelante decides revocar el consentimiento configurar tu navegador para que rechace las cookies y tambi√©n deber√°s borrar elhistorial para eliminar las ya instaladas.
Si por cualquier motivo decides que no quieres que se instalen las cookies en tu ordenador tenemos obligaci√≥n de informarte sobre c√≥mo hacerlo y para ello deber√°s configurar tu navegador. A continuaci√≥n te facilitamos los enlaces para la configuraci√≥n de las cookies en los principales navegadores.
Desactivaci√≥n de las cookies. A continuaci√≥n te facilitamos los enlaces con la informaci√≥n necesaria para desactivar las cookies de los navegadores m√°s utilizados:
Internet Explorer M√°s informaci√≥n Firefox M√°s Informaci√≥n Chrome M√°s informaci√≥n Opera M√°s informaci√≥n Safari M√°s informaci√≥n Safari para IOS M√°s informaci√≥n Chrome para Android M√°s Informaci√≥n Enlaces: Cuando enlaza con noticias o v√≠deos externos a nuestra web, la p√°gina enlazada puede contener otras cookies y deber√° visitar su propia pol√≠tica de cookies para ajustar la configuraci√≥n, ya que son completamente ajenas a nosotros.
Si quiere eliminar la publicidad originada por las cookies de tercera parte, tambi√©n tiene la opci√≥n de instalar en su navegador un plugin como Adblock, que bloquee los anuncios.
Tambi√©n puedes aplicar la extensi√≥n ‚ÄúGhostery‚Äù a su navegador y bloquear individualmente aquellas cookies, plugins y widgest que no le interesen.
En cualquier caso recomendamos hacer una limpieza del historial de navegaci√≥n y de las cookies de su ordenador de manera sistem√°tica.
Tambien puedes obtener m√°s informaci√≥n en:
http://www.youronlinechoices.com/es/ https://cookiesandyou.com/ Por √∫ltimo, puede gestionar las cookies instaladas por Adobe Flash Player desde la web de Adobe. Las cookies de terceros podr√°n ser modificadas, implementadas o anuladas por ellos mismos sin que nos lo notifiquen, por lo que no podemos hacernos responsables de que la informaci√≥n est√© perfectamente actualizada en todo momento seg√∫n la ley establece. No obstante para ofrecerle un mejor servicio de informaci√≥n hemos enlazado las pol√≠ticas de cookies y privacidad de estos terceros con la finalidad de que se puedan consultar en cualquier momento en su versi√≥n m√°s actualizada y por nuestra parte iremos revis√°ndola peri√≥dicamente para actualizar nuestros textos.
Actualizaci√≥n: 1/12/2019
`,url:"https://sergiocarracedo.es/cookies/",image:"",tags:[],readingTime:"9 minutes read",date:"Jan 1, 0001"}}</script></div><footer class="web-footer background-paper-secondary"><div class=wrapper-1200><div class=web-footer__left><div class=me><div class=me__avatar><img alt="Sergio Carracedo" height=180 src=/i/sergiocarracedo_hu_29e02a349ba171bb.jpg width=180></div><div class=me__titles><h1>Sergio Carracedo</h1><h2>Senior Fullstack developer.</h2><h3>Always learning, cats, good conversations and small details lover.</h3></div></div></div><div class=web-footer__right><nav class=menu><ul role=menu><li class=menu__item role=presentation><a role=menuitem href=https://sergiocarracedo.es/>Home</a></li><li class=menu__item role=presentation><a role=menuitem href=https://sergiocarracedo.es/blog>Blog</a></li><li class=menu__item role=presentation><a role=menuitem href=https://sergiocarracedo.es/index.xml>RSS Feed</a></li></ul></nav><div class=social><a class=social__link href=http://www.linkedin.com/in/sergiocarracedo target=_blank title=LinkedIn style=animation-delay:2s><i class="fab fa-linkedin"></i>
</a><a class=social__link href=mailto:hi+blog@sergiocarracedo.es target=_blank title=Email style=animation-delay:1.9333333333333333s><i class="fas fa-envelope"></i>
</a><a class=social__link href=https://github.com/sergiocarracedo target=_blank title=GitHub style=animation-delay:1.8666666666666667s><i class="fab fa-github"></i>
</a><a class=social__link href=https://bsky.app/profile/sergiocarracedo.es target=_blank title=Bluesky style=animation-delay:1.8s><svg fill="none" viewBox="0 0 64 57"><path fill="currentcolor" d="M13.873 3.805C21.21 9.332 29.103 20.537 32 26.55v15.882c0-.338-.13.044-.41.867-1.512 4.456-7.418 21.847-20.923 7.944-7.111-7.32-3.819-14.64 9.125-16.85-7.405 1.264-15.73-.825-18.014-9.015C1.12 23.022.0 8.51.0 6.55.0-3.268 8.579-.182 13.873 3.805zm36.254.0C42.79 9.332 34.897 20.537 32 26.55v15.882c0-.338.13.044.41.867 1.512 4.456 7.418 21.847 20.923 7.944 7.111-7.32 3.819-14.64-9.125-16.85 7.405 1.264 15.73-.825 18.014-9.015C62.88 23.022 64 8.51 64 6.55c0-9.818-8.578-6.732-13.873-2.745z"/></svg>
</a><a class=social__link href=https://www.drupal.org/u/sergiocarracedo target=_blank title=Drupal style=animation-delay:1.7333333333333334s><i class="fab fa-drupal"></i>
</a><a class=social__link href=https://www.npmjs.com/~sergiocarracedo target=_blank title=Npm style=animation-delay:1.6666666666666667s><svg viewBox="0 0 780 250" aria-hidden="true"><path fill="currentcolor" d="M240 250h1e2v-50h1e2V0H240V250zM340 50h50v1e2h-50V50zM480 0v2e2h1e2V50h50v150h50V50h50v150h50V0H480zM0 2e2h1e2V50h50v150h50V0H0V2e2z" stroke-width="5" stroke="currentcolor"/></svg>
</a><a class=social__link href=https://www.twitter.com/sergiocarracedo target=_blank title=Twitter style=animation-delay:1.6s><i class="fab fa-twitter"></i>
</a><a class=social__link href=http://www.last.fm/user/gasman40 target=_blank title=Last.fm style=animation-delay:1.5333333333333332s><i class="fab fa-lastfm"></i>
</a><a class=social__link href=http://sergiocarracedo.tumblr.com/ target=_blank title=Tumblr style=animation-delay:1.4666666666666668s><i class="fab fa-tumblr"></i></a></div></div></div></footer><script async src="https://www.googletagmanager.com/gtag/js?id=G-87X78TXEGE"></script><script>function runGA(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-87X78TXEGE")}</script><script>function runDisqus(){(function(){var e=document,t=e.createElement("script");t.src="https://sergiocarracedo.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()}</script><script src=//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js></script><script>window.addEventListener("load",function(){window.cookieconsent.initialise({palette:{popup:{background:"#004262",text:"#fff"},button:{background:"#8ec760",text:"#ffffff"}},type:"opt-out",content:{message:"Este sitio usa cookies propias y de terceros para mejorar la experiencia de usuario y esas cosas.",allow:"Ok, continua",deny:"Nada de cookies",link:"Saber m√°s",href:"https://sergiocarracedo.es/cookies"},onStatusChange:function(e){e=="allow"&&runCookiesAllowed()},onInitialise:function(e){e=="allow"&&runCookiesAllowed()}})})</script><script>addEventListener("scroll",e=>{let t=document.getElementsByTagName("body")[0];t&&window.scrollY>10?t.classList.add("not-in-top"):t.classList.remove("not-in-top")});function runCookiesAllowed(){runGA(),runDisqus()}</script><script integrity="sha256-hyPros3hoBf0jJM1+zayQLFFkchXwy4d/k2QMvk0sHs=" src=https://sergiocarracedo.es/js/theme.min.8723eba2cde1a017f48c9335fb36b240b14591c857c32e1dfe4d9032f934b07b.js></script><script src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js type=text/javascript></script><script integrity="sha256-9aiKKMxF0IhB2gmJbfgVOjk8GCtKW7zP2MwD74dPfDo=" src=https://sergiocarracedo.es/js/search.min.f5a88a28cc45d08841da09896df8153a393c182b4a5bbccfd8cc03ef874f7c3a.js></script></body></html>